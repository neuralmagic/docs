{"componentChunkName":"component---src-root-jsx","path":"/products/deepsparse/enterprise","result":{"data":{"site":{"siteMetadata":{"title":null,"docsLocation":"https://docs.neuralmagic.com"}},"mdx":{"fields":{"id":"d79d91ee-f596-5f3a-aafd-02c85a389909","title":"DeepSparse Enterprise","slug":"/products/deepsparse/enterprise","githubURL":"https://github.com/neuralmagic/docs/blob/main/src/content/products/deepsparse/enterprise.mdx"},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"DeepSparse Enterprise\",\n  \"metaTitle\": \"DeepSparse Enterprise\",\n  \"metaDescription\": \"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application\",\n  \"index\": 2000\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"div\", {\n    \"style\": {\n      \"display\": \"flex\",\n      \"flexDirection\": \"column\"\n    }\n  }, \"\\n  \", mdx(\"h1\", {\n    parentName: \"div\"\n  }, \"\\n    \", mdx(\"img\", {\n    parentName: \"h1\",\n    \"alt\": \"tool icon\",\n    \"src\": \"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/source/icon-deepsparse.png\"\n  }), \"\\n    \\xA0\\xA0DeepSparse Enterprise\\n  \"), \"\\n  \", mdx(\"h3\", {\n    parentName: \"div\"\n  }, \"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application\"), \"\\n  \", mdx(\"div\", {\n    parentName: \"div\",\n    \"style\": {\n      \"display\": \"flex\",\n      \"flexWrap\": \"wrap\"\n    }\n  }, \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://docs.neuralmagic.com/products/deepsparse/\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Documentation\",\n    \"src\": \"https://img.shields.io/badge/documentation-darkred?&style=for-the-badge&logo=read-the-docs\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ/\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Slack\",\n    \"src\": \"https://img.shields.io/badge/slack-purple?style=for-the-badge&logo=slack\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/issues/\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Support\",\n    \"src\": \"https://img.shields.io/badge/support%20forums-navy?style=for-the-badge&logo=github\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/actions/workflows/quality-check.yaml\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Main\",\n    \"src\": \"https://img.shields.io/github/workflow/status/neuralmagic/deepsparse/Quality%20Checks/main?label=build&style=for-the-badge\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/releases\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"GitHub release\",\n    \"src\": \"https://img.shields.io/github/release/neuralmagic/deepsparse.svg?style=for-the-badge\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/blob/main/CODE_OF_CONDUCT.md\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Contributor Covenant\",\n    \"src\": \"https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?color=yellow&style=for-the-badge\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://www.youtube.com/channel/UCo8dO_WMGYbWCRnj_Dxr4EA\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"YouTube\",\n    \"src\": \"https://img.shields.io/badge/-YouTube-red?&style=for-the-badge&logo=youtube&logoColor=white\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://medium.com/limitlessai\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Medium\",\n    \"src\": \"https://img.shields.io/badge/medium-%2312100E.svg?&style=for-the-badge&logo=medium&logoColor=white\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n    \", mdx(\"a\", {\n    parentName: \"div\",\n    \"href\": \"https://twitter.com/neuralmagic\"\n  }, \"\\n      \", mdx(\"img\", {\n    parentName: \"a\",\n    \"alt\": \"Twitter\",\n    \"src\": \"https://img.shields.io/twitter/follow/neuralmagic?color=darkgreen&label=Follow&style=social\",\n    \"height\": 25\n  }), \"\\n    \"), \"\\n  \")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse\"\n  }, \"DeepSparse\"), \" is a CPU inference runtime that takes advantage of sparsity within neural networks to execute inference quickly. Coupled with \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/sparseml\"\n  }, \"SparseML\"), \", an open-source optimization library, DeepSparse enables you to achieve GPU-class performance on commodity hardware.\"), mdx(\"h2\", null, \"Installation\"), mdx(\"p\", null, \"DeepSparse is available in two editions: \"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"DeepSparse Community is free for evaluation, research, and non-production use with our \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://neuralmagic.com/legal/engine-license-agreement/\"\n  }, \"DeepSparse Community License\"), \".\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"DeepSparse Enterprise requires a \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://neuralmagic.com/deepsparse-free-trial/\"\n  }, \"trial license\"), \" or \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://neuralmagic.com/legal/master-software-license-and-service-agreement/\"\n  }, \"can be fully licensed\"), \" for production, commercial applications.\")), mdx(\"h4\", null, \"Install via PyPI\"), mdx(\"p\", null, \"Install DeepSparse Enterprise with \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"pip\"), \". We recommend using a virtual enviornment.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"pip install deepsparse-ent\\n\")), mdx(\"p\", null, \"See the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.neuralmagic.com/get-started/install/deepsparse-ent\"\n  }, \"DeepSparse Enterprise Installation page\"), \" for further installation options.\"), mdx(\"h2\", null, \"Getting a License\"), mdx(\"p\", null, \"DeepSparse Enterprise requires a valid license to run the engine and can be licensed for production, commercial applications.\\nThere are two options available:\"), mdx(\"h3\", null, \"90-Day Enterprise Trial License\"), mdx(\"p\", null, \"To try out DeepSparse Enterprise and get a Neural Magic Trial License, complete our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/deepsparse-engine-free-trial\"\n  }, \"registration form\"), \".\\nUpon submission, the license will be emailed to you and your 90-day term starts right then.\"), mdx(\"h3\", null, \"DeepSparse Enterprise License\"), mdx(\"p\", null, \"To learn more about DeepSparse Enterprise pricing, \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/deepsparse-engine/#form\"\n  }, \"contact our Sales team\"), \" to discuss your use case further for a custom quote.\"), mdx(\"h2\", null, \"Installing a License\"), mdx(\"p\", null, \"Once you have obtained a license, you will need to initialize it to be able to run DeepSparse Enterprise.\\nYou can initialize your license by running:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"deepsparse.license <license_string> or <path/to/license.txt>\\n\")), mdx(\"p\", null, \"To initialize a license on a machine:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Confirm you have deepsparse-ent installed in a fresh virtual environment.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Installing deepsparse and deepsparse-ent on the same virtual environment may result in unsupported behaviors.\"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Run \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.license\"), \" with the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"<license_string>\"), \" or \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"path/to/license.txt\"), \" as an argument:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.license <samplelicensetring>\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.license ./license.txt\")))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If successful, \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.license\"), \" will write the license file to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"~/.config/neuralmagic/license.txt\"), \". You may overwrite this path by setting the environment variable \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"NM_CONFIG_DIR\"), \" (before and after running the script) with the following command:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"export NM_CONFIG_DIR=path/to/license.txt\")))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Once the license is authenticated, you should see a splash message indicating that you are now running DeepSparse Enterprise.\")), mdx(\"p\", null, \"If you encounter issues initializing your DeepSparse Enterprise License, contact \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"mailto:license@neuralmagic.com\"\n  }, \"license@neuralmagic.com\"), \" for help.\"), mdx(\"h2\", null, \"Validating a License\"), mdx(\"p\", null, \"Once you have initialized your license, you may want to check that it is still valid before running a workload on DeepSparse Enterprise. To confirm your license is still active with DeepSparse Enterprise, run the command:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"deepsparse.validate_license\\n\")), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"deepsparse.validate_license\"), \" can be run with no arguments, which will reference an existing environment variable (if set), or with one argument that is a reference to the license and can be referenced in the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"deepsparse.validate_license\"), \" command as  \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"path/to/license.txt\"), \".\"), mdx(\"p\", null, \"To validate a license on a machine:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If you have successfully run \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.license\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.validate_license\"), \" can be used to validate that the license file is in the correct location:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Run the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.validate_license\"), \" with no arguments. If the referenced license is valid, the DeepSparse Enterprise splash screen should display in your terminal window.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"NM_CONFIG_DIR\"), \" environment variable was set when creating the license, ensure this variable is still set to the same value.\"))), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"If you want to supply the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"path/to/license.txt\"), \":\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Run \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.validate_license\"), \" with \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"path/to/license.txt\"), \" as an argument as:\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"deepsparse.validate_license --license_path path/to/license.txt\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If the referenced license is valid, the DeepSparse Enterprise splash screen should display in your terminal window.\")))), mdx(\"h2\", null, \"Deployment APIs\"), mdx(\"p\", null, \"DeepSparse includes three deployment APIs:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Engine\"), \" is the lowest-level API. With Engine, you pass tensors and receive the raw logits.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Pipeline\"), \" wraps the Engine with pre- and post-processing. With Pipeline, you pass raw data and receive the prediction.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Server\"), \" wraps Pipelines with a REST API using FastAPI. With Server, you send raw data over HTTP and receive the prediction.\")), mdx(\"h3\", null, \"Engine\"), mdx(\"p\", null, \"The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, compiles the model, and runs inference on randomly generated input.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"from deepsparse import Engine\\nfrom deepsparse.utils import generate_random_inputs, model_to_path\\n\\n# download onnx, compile\\nzoo_stub = \\\"zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\\\"\\nbatch_size = 1\\ncompiled_model = Engine(model=zoo_stub, batch_size=batch_size)\\n\\n# run inference (input is raw numpy tensors, output is raw scores)\\ninputs = generate_random_inputs(model_to_path(zoo_stub), batch_size)\\noutput = compiled_model(inputs)\\nprint(output)\\n\\n# > [array([[-0.3380675 ,  0.09602544]], dtype=float32)] << raw scores\\n\")), mdx(\"h3\", null, \"DeepSparse Pipelines\"), mdx(\"p\", null, \"Pipeline is the default API for interacting with DeepSparse. Similar to Hugging Face Pipelines, DeepSparse Pipelines wrap Engine with pre- and post-processing (as well as other utilities), enabling you to send raw data to DeepSparse and receive the post-processed prediction.\"), mdx(\"p\", null, \"The example below downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo, sets up a pipeline, and runs inference on sample data.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"from deepsparse import Pipeline\\n\\n# download onnx, set up pipeline\\nzoo_stub = \\\"zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\\\"  \\nsentiment_analysis_pipeline = Pipeline.create(\\n  task=\\\"sentiment-analysis\\\",    # name of the task\\n  model_path=zoo_stub,          # zoo stub or path to local onnx file\\n)\\n\\n# run inference (input is a sentence, output is the prediction)\\nprediction = sentiment_analysis_pipeline(\\\"I love using DeepSparse Pipelines\\\")\\nprint(prediction)\\n# > labels=['positive'] scores=[0.9954759478569031]\\n\")), mdx(\"h4\", null, \"Additional Resources\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Check out the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases\"\n  }, \"Use Cases Page\"), \" for more details on supported tasks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Check out the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-pipelines.md\"\n  }, \"Pipelines User Guide\"), \" for more usage details.\")), mdx(\"h3\", null, \"DeepSparse Server\"), mdx(\"p\", null, \"Server wraps Pipelines with REST APIs, enabling you to stand up model serving endpoint running DeepSparse. This enables you to send raw data to DeepSparse over HTTP and receive the post-processed predictions.\"), mdx(\"p\", null, \"DeepSparse Server is launched from the command line, configured via arguments or a server configuration file. The following downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format from SparseZoo and launches a sentiment analysis endpoint:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"deepsparse.server \\\\\\n  --task sentiment-analysis \\\\\\n  --model_path zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\\n\")), mdx(\"p\", null, \"Sending a request:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"import requests\\n\\nurl = \\\"http://localhost:5543/predict\\\" # Server's port default to 5543\\nobj = {\\\"sequences\\\": \\\"Snorlax loves my Tesla!\\\"}\\n\\nresponse = requests.post(url, json=obj)\\nprint(response.text)\\n# {\\\"labels\\\":[\\\"positive\\\"],\\\"scores\\\":[0.9965094327926636]}\\n\")), mdx(\"h4\", null, \"Additional Resources\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Check out the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases\"\n  }, \"Use Cases Page\"), \" for more details on supported tasks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Check out the \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-server.md\"\n  }, \"Server User Guide\"), \" for more usage details.\")), mdx(\"h2\", null, \"ONNX\"), mdx(\"p\", null, \"DeepSparse accepts models in the ONNX format. ONNX models can be passed in one of two ways:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"SparseZoo Stub\"), \": \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://sparsezoo.neuralmagic.com/\"\n  }, \"SparseZoo\"), \" is an open-source repository of sparse models. The examples on this page use SparseZoo stubs to identify models and download them for deployment in DeepSparse.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Local ONNX File\"), \": Users can provide their own ONNX models, whether dense or sparse. For example:\"))), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"wget https://github.com/onnx/models/raw/main/vision/classification/mobilenet/model/mobilenetv2-7.onnx\\n\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"from deepsparse import Engine\\nfrom deepsparse.utils import generate_random_inputs\\nonnx_filepath = \\\"mobilenetv2-7.onnx\\\"\\nbatch_size = 16\\n\\n# Generate random sample input\\ninputs = generate_random_inputs(onnx_filepath, batch_size)\\n\\n# Compile and run\\ncompiled_model = Engine(model=onnx_filepath, batch_size=batch_size)\\noutputs = compiled_model(inputs)\\nprint(outputs[0].shape)\\n# (16, 1000) << batch, num_classes\\n\")), mdx(\"h2\", null, \"Inference Modes\"), mdx(\"p\", null, \"DeepSparse offers different inference scenarios based on your use case.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Single-stream\"), \" scheduling: the latency/synchronous scenario, requests execute serially. \", \"[\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"default\"), \"]\"), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/old/source/single-stream.png\",\n    \"alt\": \"single stream diagram\"\n  }), mdx(\"p\", null, \"It's highly optimized for minimum per-request latency, using all of the system's resources provided to it on every request it gets.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi-stream\"), \" scheduling: the throughput/asynchronous scenario, requests execute in parallel.\"), mdx(\"img\", {\n    \"src\": \"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/old/source/multi-stream.png\",\n    \"alt\": \"multi stream diagram\"\n  }), mdx(\"p\", null, \"The most common use cases for the multi-stream scheduler are where parallelism is low with respect to core count, and where requests need to be made asynchronously without time to batch them.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/scheduler.md\"\n  }, \"Check out the Scheduler User Guide\"), \" for more details.\")), mdx(\"h2\", null, \"Product Usage Analytics\"), mdx(\"p\", null, \"DeepSparse Community Edition gathers basic usage telemetry including, but not limited to, Invocations, Package, Version, and IP Address for Product Usage Analytics purposes. Review Neural Magic's \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/legal/\"\n  }, \"Products Privacy Policy\"), \" for further details on how we process this data.\"), mdx(\"p\", null, \"To disable Product Usage Analytics, run the command:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bash\"\n  }, \"export NM_DISABLE_ANALYTICS=True\\n\")), mdx(\"p\", null, \"Confirm that telemetry is shut off through info logs streamed with engine invocation by looking for the phrase \\\"Skipping Neural Magic's latest package version check.\\\" For additional assistance, reach out through the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/issues\"\n  }, \"DeepSparse GitHub Issue queue\"), \".\"), mdx(\"h2\", null, \"Additional Resources\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide/deepsparse-benchmarking.md\"\n  }, \"Benchmarking Performance\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/user-guide\"\n  }, \"User Guide\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/docs/use-cases\"\n  }, \"Use Cases\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/tree/main/examples/\"\n  }, \"Cloud Deployments and Demos\"))), mdx(\"h4\", null, \"Versions\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pypi.org/project/deepsparse\"\n  }, \"DeepSparse\"), \" | stable\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://pypi.org/project/deepsparse-nightly/\"\n  }, \"DeepSparse-Nightly\"), \" | nightly (dev)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/releases\"\n  }, \"GitHub\"), \" | releases\")), mdx(\"h4\", null, \"Info\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.neuralmagic.com/blog/\"\n  }, \"Blog\"), \" \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.neuralmagic.com/resources/\"\n  }, \"Resources\"))), mdx(\"h2\", null, \"Community\"), mdx(\"h3\", null, \"Be Part of the Future... And the Future is Sparse!\"), mdx(\"p\", null, \"Contribute with code, examples, integrations, and documentation as well as bug reports and feature requests! \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/blob/main/CONTRIBUTING.md\"\n  }, \"Learn how here.\")), mdx(\"p\", null, \"For user help or questions about DeepSparse, sign up or log in to our \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"a\", {\n    parentName: \"strong\",\n    \"href\": \"https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ\"\n  }, \"Neural Magic Community Slack\")), \". We are growing the community member by member and happy to see you there. Bugs, feature requests, or additional questions can also be posted to our \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/issues\"\n  }, \"GitHub Issue Queue.\"), \" You can get the latest news, webinar and event invites, research papers, and other ML Performance tidbits by \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/subscribe/\"\n  }, \"subscribing\"), \" to the Neural Magic community.\"), mdx(\"p\", null, \"For more general questions about Neural Magic, complete this \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://neuralmagic.com/contact/\"\n  }, \"form.\")), mdx(\"h3\", null, \"License\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.neuralmagic.com/products/deepsparse\"\n  }, \"DeepSparse Community\"), \" is licensed under the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/blob/main/LICENSE-NEURALMAGIC\"\n  }, \"Neural Magic DeepSparse Community License.\"), \"\\nSome source code, example files, and scripts included in the deepsparse GitHub repository or directory are licensed under the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/deepsparse/blob/main/LICENSE\"\n  }, \"Apache License Version 2.0\"), \" as noted.\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://docs.neuralmagic.com/products/deepsparse-ent\"\n  }, \"DeepSparse Enterprise\"), \" requires a Trial License or \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/legal/master-software-license-and-service-agreement/\"\n  }, \"can be fully licensed\"), \" for production, commercial applications.\"), mdx(\"h3\", null, \"Cite\"), mdx(\"p\", null, \"Find this project useful in your research or other communications? Please consider citing:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-bibtex\"\n  }, \"@InProceedings{\\n    pmlr-v119-kurtz20a, \\n    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, \\n    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, \\n    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, \\n    pages = {5533--5543}, \\n    year = {2020}, \\n    editor = {Hal Daum\\xE9 III and Aarti Singh}, \\n    volume = {119}, \\n    series = {Proceedings of Machine Learning Research}, \\n    address = {Virtual}, \\n    month = {13--18 Jul}, \\n    publisher = {PMLR}, \\n    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},\\n    url = {http://proceedings.mlr.press/v119/kurtz20a.html}\\n}\\n\\n@article{DBLP:journals/corr/abs-2111-13445,\\n  author    = {Eugenia Iofinova and\\n               Alexandra Peste and\\n               Mark Kurtz and\\n               Dan Alistarh},\\n  title     = {How Well Do Sparse Imagenet Models Transfer?},\\n  journal   = {CoRR},\\n  volume    = {abs/2111.13445},\\n  year      = {2021},\\n  url       = {https://arxiv.org/abs/2111.13445},\\n  eprinttype = {arXiv},\\n  eprint    = {2111.13445},\\n  timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-13445.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#installation","title":"Installation","items":[{"items":[{"url":"#install-via-pypi","title":"Install via PyPI"}]}]},{"url":"#getting-a-license","title":"Getting a License","items":[{"url":"#90-day-enterprise-trial-license","title":"90-Day Enterprise Trial License"},{"url":"#deepsparse-enterprise-license","title":"DeepSparse Enterprise License"}]},{"url":"#installing-a-license","title":"Installing a License"},{"url":"#validating-a-license","title":"Validating a License"},{"url":"#deployment-apis","title":"Deployment APIs","items":[{"url":"#engine","title":"Engine"},{"url":"#deepsparse-pipelines","title":"DeepSparse Pipelines","items":[{"url":"#additional-resources","title":"Additional Resources"}]},{"url":"#deepsparse-server","title":"DeepSparse Server","items":[{"url":"#additional-resources-1","title":"Additional Resources"}]}]},{"url":"#onnx","title":"ONNX"},{"url":"#inference-modes","title":"Inference Modes"},{"url":"#product-usage-analytics","title":"Product Usage Analytics"},{"url":"#additional-resources-2","title":"Additional Resources","items":[{"items":[{"url":"#versions","title":"Versions"},{"url":"#info","title":"Info"}]}]},{"url":"#community","title":"Community","items":[{"url":"#be-part-of-the-future-and-the-future-is-sparse","title":"Be Part of the Future... And the Future is Sparse!"},{"url":"#license","title":"License"},{"url":"#cite","title":"Cite"}]}]},"parent":{"relativePath":"products/deepsparse/enterprise.mdx"},"frontmatter":{"metaTitle":"DeepSparse Enterprise","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application","index":2000,"skipToChild":null}},"allMdx":{"edges":[{"node":{"fields":{"id":"ee9f8c1f-d776-5134-89e6-b60f31e11b65","slug":"/products","title":"Products"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":true,"metaTitle":"Products","metaDescription":"Products"}}},{"node":{"fields":{"id":"bfcfecba-6eb1-59e8-8379-9c6d0c7a6a46","slug":"/get-started","title":"Get Started"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":true,"metaTitle":"Get Started","metaDescription":"Getting started with the Neural Magic Platform"}}},{"node":{"fields":{"id":"220abd27-24cf-5408-9402-3e7b0591a7ec","slug":"/details","title":"Details"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":true,"metaTitle":"Details","metaDescription":"Details"}}},{"node":{"fields":{"id":"a500248d-7a7d-5be6-9c51-ffab9753b5e0","slug":"/user-guides","title":"User Guides"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":true,"metaTitle":"User Guides","metaDescription":"User Guides"}}},{"node":{"fields":{"id":"0a2bc29c-0df2-59e5-a94c-bce091e6dc6d","slug":"/use-cases","title":"Use Cases"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":true,"metaTitle":"Use Cases","metaDescription":"Use Cases for the Neural Magic Platform"}}},{"node":{"fields":{"id":"cba43102-b32f-5b1a-9c69-f46222a36403","slug":"/user-guides/deepsparse-engine","title":"DeepSparse"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"User Guides for DeepSparse Engine","metaDescription":"User Guides for DeepSparse Engine"}}},{"node":{"fields":{"id":"e0110db2-2460-5fd1-8cf7-16533e4ce767","slug":"/user-guides/recipes","title":"Recipes"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"What are Sparsification Recipes?","metaDescription":"Description of sparsification recipes enabling smaller and more performant neural networks in deep learning"}}},{"node":{"fields":{"id":"36031f98-c9cf-5658-b606-b28762ed208f","slug":"/user-guides/onnx-export","title":"ONNX Export"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Exporting to the ONNX Format","metaDescription":"Exporting to the ONNX Format"}}},{"node":{"fields":{"id":"d098532a-4bc3-5224-91ca-b51b42b779e7","slug":"/user-guides/sparsification","title":"Sparsification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"What is Sparsification?","metaDescription":"Description of model sparsification enabling smaller and more performant neural networks in deep learning"}}},{"node":{"fields":{"id":"ceb703c1-4adc-510d-95bd-fb316521b486","slug":"/user-guides/deploying-deepsparse","title":"Deploying DeepSparse"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying DeepSparse","metaDescription":"Deploying Deepsparse"}}},{"node":{"fields":{"id":"2b874d9c-9c67-549d-8c79-b9b261359734","slug":"/user-guides/recipes/creating","title":"Creating"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Creating Sparsification Recipes","metaDescription":"Creating Sparsification Recipes"}}},{"node":{"fields":{"id":"b35e8125-4431-5427-bd08-6b379e810c9b","slug":"/user-guides/deploying-deepsparse/aws-lambda","title":"AWS Lambda"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Using DeepSparse on AWS Lambda","metaDescription":"Deploy DeepSparse in a Serverless framework with AWS Lambda"}}},{"node":{"fields":{"id":"b16b5557-b4f4-5f6a-8c4d-ee3673781d7f","slug":"/user-guides/recipes/enabling","title":"Enabling Pipelines"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Enabling Pipelines to Work with SparseML Recipes","metaDescription":"Enabling Pipelines to work with SparseML Recipess"}}},{"node":{"fields":{"id":"5c89e3b2-04e2-59ed-9b42-1c5df17a2df4","slug":"/user-guides/deploying-deepsparse/google-cloud-run","title":"Google Cloud Run"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Using DeepSparse on Google Cloud Run","metaDescription":"Deploy DeepSparse in a Serverless framework with Google Cloud Run"}}},{"node":{"fields":{"id":"115b6c49-fc7b-5dc6-9872-5a690e2b4a25","slug":"/user-guides/deepsparse-engine/benchmarking","title":"Benchmarking"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Benchmarking ONNX Models With DeepSparse","metaDescription":"Benchmarking ONNX models with DeepSparse"}}},{"node":{"fields":{"id":"6085e38d-6a1d-58ac-a6c2-052f8208cf6e","slug":"/user-guides/deepsparse-engine/diagnostics-debugging","title":"Diagnostics/Debugging"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Logging Guidance for Diagnostics and Debugging","metaDescription":"Logging Guidance for Diagnostics and Debugging"}}},{"node":{"fields":{"id":"49f60901-1870-5d76-9fcc-12408f802b71","slug":"/user-guides/deploying-deepsparse/amazon-sagemaker","title":"Amazon SageMaker"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying With DeepSparse on Amazon SageMaker","metaDescription":"Deploying with DeepSparse on Amazon SageMaker for faster and cheaper model deployments behind an HTTP API"}}},{"node":{"fields":{"id":"88604204-656d-5710-a23b-b7cc05eca1de","slug":"/user-guides/deepsparse-engine/hardware-support","title":"Supported Hardware"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Supported Hardware for DeepSparse","metaDescription":"Supported hardware for DeepSparse including CPU types and instruction sets"}}},{"node":{"fields":{"id":"86938249-6f57-5091-84c8-9ad393d417b1","slug":"/user-guides/deepsparse-engine/numactl-utility","title":"numactl Utility"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Using the numactl Utility to Control Resource Utilization With DeepSparse","metaDescription":"Using the numactl Utility to Control Resource Utilization With DeepSparse"}}},{"node":{"fields":{"id":"8da43520-8b07-5bcc-a88f-f328137a2270","slug":"/user-guides/deepsparse-engine/logging","title":"DeepSparse Logging"},"frontmatter":{"index":6000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Logging","metaDescription":"System and Data Logging with DeepSparse"}}},{"node":{"fields":{"id":"ddada45f-c6b9-5146-91e4-f56c28e31284","slug":"/user-guides/deepsparse-engine/scheduler","title":"Inference Types"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Inference Types With DeepSparse Scheduler","metaDescription":"Inference Types with DeepSparse Scheduler"}}},{"node":{"fields":{"id":"113c5c42-984c-53b3-8392-7058bdd946cb","slug":"/use-cases/embedding-extraction","title":"Embedding Extraction"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Embedding Extraction Deployment","metaDescription":"Generalized Embedding Extraction Deployment"}}},{"node":{"fields":{"id":"b7365467-d2b2-5dad-be10-a3aaa773d3a3","slug":"/use-cases/natural-language-processing","title":"Natural Language Processing"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":true,"metaTitle":"Natural Language Processing","metaDescription":"NLP with HuggingFace Transformers"}}},{"node":{"fields":{"id":"baad04ad-efd7-5e85-8645-6455fce34324","slug":"/use-cases/image-classification","title":"Image Classification"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":true,"metaTitle":"Image Classification","metaDescription":"Image Classification with PyTorch Torchvision"}}},{"node":{"fields":{"id":"1ef7c7c8-073e-5abf-b57a-af03628c0714","slug":"/use-cases/object-detection","title":"Object Detection"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":true,"metaTitle":"Object Detection","metaDescription":"Object Detection with Ultralytics YOLOv5"}}},{"node":{"fields":{"id":"d1d7a8f3-9061-539a-aff8-d3957da959c6","slug":"/use-cases/object-detection/deploying","title":"Deploying"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Object Detection Deployments With DeepSparse","metaDescription":"Object Detection deployments with Ultralytics YOLOv5 and DeepSparse to create cheaper and more performant models"}}},{"node":{"fields":{"id":"d363f68d-8b46-5da1-a1ef-fc14776d0a03","slug":"/use-cases/natural-language-processing/deploying","title":"Deploying"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Deployments with DeepSparse","metaDescription":"NLP deployments with Hugging Face Transformers and DeepSparse to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"bd363f12-8b17-5aac-af39-0ae0e7b0b829","slug":"/use-cases/natural-language-processing/question-answering","title":"Question Answering"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Question Answering","metaDescription":"Question Answering with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"3f751c96-1e87-5f4a-a0cb-d277724c2a0b","slug":"/use-cases/object-detection/sparsifying","title":"Sparsifying"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying Object Detection With Ultralytics YOLOv5 and SparseML","metaDescription":"Sparsifying Object Detection with Ultralytics YOLOv5 and SparseML to create cheaper and more performant models"}}},{"node":{"fields":{"id":"ae7d3726-718a-5be3-91a4-2559a2445fc4","slug":"/use-cases/natural-language-processing/text-classification","title":"Text Classification"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Text Classification","metaDescription":"Text Classification with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"c3c4bea9-f7cd-5044-9f34-8e33c8d2b36d","slug":"/use-cases/natural-language-processing/token-classification","title":"Token Classification"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Token Classification","metaDescription":"Token Classification with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"c1d1d524-e761-5294-a8e7-e21f3164f48a","slug":"/","title":"Home"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"Neural Magic Documentation","metaDescription":"Documentation for the Neural Magic Platform"}}},{"node":{"fields":{"id":"4ba00f44-34e5-5677-a8c6-862b44d48e7f","slug":"/products/deepsparse","title":"DeepSparse"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"0d031520-ee95-58db-bdb3-86689d6a0941","slug":"/use-cases/image-classification/deploying","title":"Deploying"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Image Classification Deployments With DeepSparse","metaDescription":"Image classification deployments with DeepSparse to create cheaper and more performant models"}}},{"node":{"fields":{"id":"ecaebb25-0fd8-572a-9d98-52302d0a0e4e","slug":"/use-cases/image-classification/sparsifying","title":"Sparsifying"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying Image Classification Models With SparseML","metaDescription":"Sparsifying Image Classification models with SparseML to create cheaper and more performant models"}}},{"node":{"fields":{"id":"be1fa4d5-bae3-5c19-9da5-f56a08f2fa40","slug":"/products/sparsezoo","title":"SparseZoo"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseZoo","metaDescription":"Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes"}}},{"node":{"fields":{"id":"76657780-4f8a-5f37-8f3c-1c6c04637503","slug":"/products/sparseml","title":"SparseML"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML","metaDescription":"Sparsity-aware neural network inference engine for GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"08d07abb-4c37-5f58-8bdd-277facdeaf05","slug":"/products/sparsezoo/cli","title":"CLI"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML CLI","metaDescription":"Documentation for the CLI commands installed with SparseZoo enabling use of SOTA sparsified models and recipes"}}},{"node":{"fields":{"id":"2b513d53-228c-5cf7-9d36-42d82e22cc0b","slug":"/user-guides/deploying-deepsparse/deepsparse-server","title":"DeepSparse Server"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying With DeepSparse Server","metaDescription":"Deploying With DeepSparse Server for faster and cheaper model deployments behind an HTTP API"}}},{"node":{"fields":{"id":"3b06a681-3381-5cdc-9dad-827cdc6f60ac","slug":"/use-cases/deploying-deepsparse/docker","title":"Docker"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Using/Creating a DeepSparse Docker Image","metaDescription":"Using/Creating a DeepSparse Docker Image for repeatable build processes"}}},{"node":{"fields":{"id":"aafbc5b1-5291-5291-ab86-da09c479a4c1","slug":"/products/sparsezoo/python-api","title":"Python API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Python API","metaDescription":"Documentation for the Python APIs available with SparseZoo enabling use of SOTA sparsified models and recipes"}}},{"node":{"fields":{"id":"018cd1d6-6542-547d-ac3b-2743f179ac04","slug":"/products/sparseml/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Python API","metaDescription":"Documentation for the Python APIs available with SparseML enabling SOTA model sparsification"}}},{"node":{"fields":{"id":"97cb8313-bc03-5a24-bf92-015086fc9071","slug":"/products/sparsezoo/models","title":"Models"},"frontmatter":{"index":1000,"targetURL":"https://sparsezoo.neuralmagic.com/","skipToChild":null,"metaTitle":"Models","metaDescription":"Models"}}},{"node":{"fields":{"id":"b6a850d0-afc3-5418-ae82-146d2cb68706","slug":"/products/sparseml/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML CLI","metaDescription":"Documentation for the CLI commands installed with SparseML enabling SOTA model sparsification"}}},{"node":{"fields":{"id":"e3c4460c-c09f-5838-8047-fea6cf8f0511","slug":"/products/deepsparse/enterprise/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse CLI","metaDescription":"Documentation for the CLI commands installed with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"d6bf031b-89e2-5374-9997-1676f340f25a","slug":"/products/deepsparse/enterprise/cpp-api","title":"C++ API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse C++ API","metaDescription":"Documentation for the C++ APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"233b31f0-deac-5b52-9101-caf3e3474595","slug":"/products/deepsparse/community","title":"DeepSparse Community"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Community","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"5e0531ff-a237-587a-a318-72f000810bb0","slug":"/products/deepsparse/enterprise/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Python API","metaDescription":"Documentation for the Python APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"0d2bd4a5-a7f1-508a-8652-91e3eed40ceb","slug":"/products/deepsparse/community/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Python API","metaDescription":"Documentation for the Python APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"d79d91ee-f596-5f3a-aafd-02c85a389909","slug":"/products/deepsparse/enterprise","title":"DeepSparse Enterprise"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Enterprise","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"27a0b7a5-09e0-56ab-9ee5-68f45d8795fd","slug":"/index/optimize-workflow","title":"Optimize for Inference"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Optimize for Inference","metaDescription":"Overview of deployment capabilities in the Neural Magic Platform"}}},{"node":{"fields":{"id":"665dac23-0ae5-5037-a151-d359bad8f8c2","slug":"/get-started/deploy-a-model","title":"Deploy a Model"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy a Model","metaDescription":"Deploy a model with DeepSparse Server for easy and performant ML deployments"}}},{"node":{"fields":{"id":"478d6817-c021-5181-922d-2689a65470c5","slug":"/index/quick-tour","title":"Quick Tour"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Quick Tour","metaDescription":"Quick tour of the available functionality"}}},{"node":{"fields":{"id":"43a5dd03-4ffe-5c02-ac47-7ee94de63602","slug":"/get-started/sparsify-a-model","title":"Sparsify a Model"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsify a Model","metaDescription":"Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"4491ba6b-e373-50e3-a8b5-e8e173ecba81","slug":"/index/deploy-workflow","title":"Deploy on CPUs"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy on CPUs","metaDescription":"Overview of deployment capabilities in the Neural Magic Platform"}}},{"node":{"fields":{"id":"cc2c9b73-3a5b-5764-ad54-38c919cb3e78","slug":"/get-started/install","title":"Installation"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"Install Neural Magic Platform","metaDescription":"Installation instructions for the Neural Magic Platform including DeepSparse, SparseML, SparseZoo"}}},{"node":{"fields":{"id":"4ac6dd90-ac18-5c3c-8c85-06021527df5e","slug":"/get-started/use-a-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Text Classification Model","metaDescription":"Use a NLP Text Classification Model with DeepSparse for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"3bd4ca0d-2d04-5d9f-9037-06f9ae7dfb73","slug":"/get-started/transfer-a-sparsified-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model for Object Detection","metaDescription":"Transfer a Sparsified Object Detection Model to your dataset enabling performant deep learning deployments in a faster amount of time"}}},{"node":{"fields":{"id":"00c56759-713b-5731-92b9-027dab853257","slug":"/get-started/use-a-model/custom-use-case","title":"Custom Use Case"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Custom Use Case","metaDescription":"Use a Custom Use Case and Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"5ba330af-8819-52cd-8818-2374113da636","slug":"/get-started/use-a-model","title":"Use a Model"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Model","metaDescription":"Use a Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"5689a5f7-7564-5b91-b21d-0f6aed1218a9","slug":"/get-started/use-a-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Use an Object Detection Model","metaDescription":"Use an Object Detection Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"8323781e-a7b4-5bb5-a453-86e2c472d6cf","slug":"/get-started/install/deepsparse-ent","title":"DeepSparse Enterprise"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Enterprise Installation","metaDescription":"Installation instructions for DeepSparse enabling performant neural network deployments"}}},{"node":{"fields":{"id":"5cebfec6-69f9-59bb-9a47-58bd80ce5b29","slug":"/get-started/install/deepsparse","title":"DeepSparse Community"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Community Installation","metaDescription":"Installation instructions for DeepSparse enabling performant neural network deployments"}}},{"node":{"fields":{"id":"6662f291-f43f-5616-8f60-dea883911f57","slug":"/get-started/sparsify-a-model/custom-integrations","title":"Custom Integrations"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Creating a Custom Integration for Sparsifying Models","metaDescription":"Creating a Custom Integration for Sparsifying Models with SparseML for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"c32bcbda-4c9f-5b2a-b18d-5083b0003aef","slug":"/get-started/transfer-a-sparsified-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model for Text Classification","metaDescription":"Transfer a Sparsified NLP Model to your sentiment analysis dataset enabling performant deep learning deployments with limited training"}}},{"node":{"fields":{"id":"ac411c38-91fa-56bf-8dfd-c00cab6602ad","slug":"/get-started/install/sparsezoo","title":"SparseZoo"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseZoo Installation","metaDescription":"Installation instructions for the SparseZoo sparse model repository"}}},{"node":{"fields":{"id":"f92e631d-dc67-592a-b7c2-dd4bb5e7a3fe","slug":"/products/deepsparse/community/cpp-api","title":"C++ API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse C++ API","metaDescription":"Documentation for the C++ APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"8efaf662-8889-5bc3-bcae-9868aaa8aa53","slug":"/get-started/sparsify-a-model/supported-integrations","title":"Supported Integrations"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying a Model for SparseML Integrations","metaDescription":"Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"dc9f9ab1-1fb3-57b4-910a-fa3200519b0d","slug":"/get-started/install/sparseml","title":"SparseML"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Installation","metaDescription":"Installation instructions for SparseML neural network optimization, training, and sparsification"}}},{"node":{"fields":{"id":"02f4ebab-32cf-58e1-a39b-db269f740d8b","slug":"/get-started/deploy-a-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy an Object Detection Model","metaDescription":"Deploy an object detection model with DeepSparse Server for easier, faster, and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"fb19b2bb-688f-5f4e-98e0-d1dbf478a015","slug":"/get-started/transfer-a-sparsified-model","title":"Transfer a Sparsified Model"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model","metaDescription":"Transfer a Sparsified Model to your dataset, enabling performant deep learning deployments with limited training"}}},{"node":{"fields":{"id":"a597113f-8c9b-5620-baaa-95555edb534c","slug":"/details/faqs","title":"FAQs"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"FAQs","metaDescription":"FAQs for the Neural Magic Platform"}}},{"node":{"fields":{"id":"7d12ac36-8c45-565c-9b28-6472fdeb4a99","slug":"/get-started/deploy-a-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy a Text Classification Model","metaDescription":"Deploy a text classification model with DeepSparse Server for easier, faster, and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"b386abe1-47d1-5b57-aa77-a3f73f3ebe21","slug":"/products/deepsparse/community/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse CLI","metaDescription":"Documentation for the CLI commands installed with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"f2d385fc-cb79-5c11-8873-f4e6f6d6da23","slug":"/details/glossary","title":"Glossary"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Glossary","metaDescription":"Glossary for the Neural Magic product"}}},{"node":{"fields":{"id":"e5f9763e-9c0a-5d84-9f65-550685441793","slug":"/details/research-papers","title":"Research Papers"},"frontmatter":{"index":1000,"targetURL":"https://neuralmagic.com/resources/technical-papers/","skipToChild":null,"metaTitle":"Research Papers","metaDescription":"Research Papers"}}}]}},"pageContext":{"id":"d79d91ee-f596-5f3a-aafd-02c85a389909","pageType":"docs"}},"staticQueryHashes":[]}