<!DOCTYPE html>
<html class="writer-html5" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   sparseml.pytorch.utils.quantization package — SparseML 0.3.1.20210514 documentation
  </title>
  <link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/css/nm-theme-adjustment.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/favicon.ico" rel="shortcut icon"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/clipboard.min.js">
  </script>
  <script src="../_static/copybutton.js">
  </script>
  <script src="../_static/js/theme.js" type="text/javascript">
  </script>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="sparseml.sparsification.html" rel="next" title="sparseml.sparsification package"/>
  <link href="sparseml.pytorch.utils.html" rel="prev" title="sparseml.pytorch.utils package"/>
 </head>
 <body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
   <nav class="wy-nav-side" data-toggle="wy-nav-shift">
    <div class="wy-side-scroll">
     <div class="wy-side-nav-search">
      <a class="icon icon-home" href="../index.html">
       SparseML
       <img alt="Logo" class="logo" src="../_static/icon-sparseml.png"/>
      </a>
      <div class="version">
       0.3
      </div>
      <div role="search">
       <form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
        <input name="q" placeholder="Search docs" type="text"/>
        <input name="check_keywords" type="hidden" value="yes"/>
        <input name="area" type="hidden" value="default"/>
       </form>
      </div>
     </div>
     <div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
      <p class="caption">
       <span class="caption-text">
        General
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/quicktour.html">
         Quick Tour
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/installation.html">
         Installation
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/recipes.html">
         Sparsification Recipes
        </a>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        API
       </span>
      </p>
      <ul class="current">
       <li class="toctree-l1 current">
        <a class="reference internal" href="sparseml.html">
         sparseml package
        </a>
        <ul class="current">
         <li class="toctree-l2 current">
          <a class="reference internal" href="sparseml.html#subpackages">
           Subpackages
          </a>
          <ul class="current">
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.deepsparse.html">
             sparseml.deepsparse package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.framework.html">
             sparseml.framework package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.keras.html">
             sparseml.keras package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.onnx.html">
             sparseml.onnx package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.optim.html">
             sparseml.optim package
            </a>
           </li>
           <li class="toctree-l3 current">
            <a class="reference internal" href="sparseml.pytorch.html">
             sparseml.pytorch package
            </a>
            <ul class="current">
             <li class="toctree-l4 current">
              <a class="reference internal" href="sparseml.pytorch.html#subpackages">
               Subpackages
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#submodules">
               Submodules
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch.base">
               sparseml.pytorch.base module
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">
               Module contents
              </a>
             </li>
            </ul>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.sparsification.html">
             sparseml.sparsification package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.tensorflow_v1.html">
             sparseml.tensorflow_v1 package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.utils.html">
             sparseml.utils package
            </a>
           </li>
          </ul>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#submodules">
           Submodules
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.base">
           sparseml.base module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.log">
           sparseml.log module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.version">
           sparseml.version module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml">
           Module contents
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        Connect Online
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">
         Bugs, Feature Requests
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://discuss.neuralmagic.com/">
         Support, General Q&amp;A Forums
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ">
         Deep Sparse Community Slack
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://docs.neuralmagic.com">
         Neural Magic Docs
        </a>
       </li>
      </ul>
     </div>
    </div>
   </nav>
   <section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
    <nav aria-label="top navigation" class="wy-nav-top">
     <i class="fa fa-bars" data-toggle="wy-nav-top">
     </i>
     <a href="../index.html">
      SparseML
     </a>
    </nav>
    <div class="wy-nav-content">
     <div class="rst-content">
      <div aria-label="breadcrumbs navigation" role="navigation">
       <ul class="wy-breadcrumbs">
        <li>
         <a class="icon icon-home" href="../index.html">
         </a>
         »
        </li>
        <li>
         <a href="sparseml.html">
          sparseml package
         </a>
         »
        </li>
        <li>
         <a href="sparseml.pytorch.html">
          sparseml.pytorch package
         </a>
         »
        </li>
        <li>
         <a href="sparseml.pytorch.utils.html">
          sparseml.pytorch.utils package
         </a>
         »
        </li>
        <li>
         sparseml.pytorch.utils.quantization package
        </li>
        <li class="wy-breadcrumbs-aside">
         <a href="../_sources/api/sparseml.pytorch.utils.quantization.rst.txt" rel="nofollow">
          View page source
         </a>
        </li>
       </ul>
       <hr/>
      </div>
      <div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <div itemprop="articleBody">
        <div class="section" id="sparseml-pytorch-utils-quantization-package">
         <h1>
          sparseml.pytorch.utils.quantization package
          <a class="headerlink" href="#sparseml-pytorch-utils-quantization-package" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <div class="section" id="submodules">
          <h2>
           Submodules
           <a class="headerlink" href="#submodules" title="Permalink to this headline">
            ¶
           </a>
          </h2>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization.helpers">
          <span id="sparseml-pytorch-utils-quantization-helpers-module">
          </span>
          <h2>
           sparseml.pytorch.utils.quantization.helpers module
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization.helpers" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Helper functions for performing quantization aware training with PyTorch
          </p>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.helpers.add_quant_dequant">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              add_quant_dequant
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#add_quant_dequant">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.add_quant_dequant" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Wraps all Conv and Linear submodule with a qconfig with a QuantWrapper
:param module: the module to modify
            </p>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.helpers.fuse_module_conv_bn_relus">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              fuse_module_conv_bn_relus
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inplace
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               override_bn_subclasses_forward
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               bool
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             torch.nn.modules.module.Module
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#fuse_module_conv_bn_relus">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.fuse_module_conv_bn_relus" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Performs fusion of Conv2d, BatchNorm2d, and ReLU layers found in the
given module. To be fused, these layers must appear sequentially in
module.named_modules() and be in the same submodule.
Fuses either Conv2d -&gt; BatchNorm2d or Conv2d -&gt; BatchNorm2d -&gt; ReLU blocks
            </p>
            <p>
             If this function does not fuse the model in the desired way, implement an
in place fusing function for the model.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to fuse
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inplace
                 </strong>
                 – set True to perform fusions in-place. default is True
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  override_bn_subclasses_forward
                 </strong>
                 – if True, modules that are subclasses of
BatchNorm2d will be modified to be BatchNorm2d but with the forward
pass and state variables copied from the subclass. This is so these
BN modules can pass PyTorch type checking when fusing. Can set to
“override-only” and only parameters will be overwritten, not the
forward pass. Default is True
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the fused module
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.helpers.get_qat_qconfig">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.helpers.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              get_qat_qconfig
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             torch.quantization.qconfig.QConfig
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/helpers.html#get_qat_qconfig">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.helpers.get_qat_qconfig" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Returns
             </dt>
             <dd class="field-odd">
              <p>
               A QAT fake quantization config for symmetric weight quantization and
asymmetric activation quantization.  The difference between this and
torch.quantization.default_qat_qconfig is that the activation observer
will not have reduce_range enabled.
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization.quantize_qat_export">
          <span id="sparseml-pytorch-utils-quantization-quantize-qat-export-module">
          </span>
          <h2>
           sparseml.pytorch.utils.quantization.quantize_qat_export module
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization.quantize_qat_export" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Helper functions for parsing an exported pytorch model trained with
quantization aware training.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              QuantizationParams
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               scale
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               zero_point
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               target
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               tuple
              </span>
             </code>
            </p>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.scale">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                scale
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.scale" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 0
              </p>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.target">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                target
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.target" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 2
              </p>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.zero_point">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                zero_point
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams.zero_point" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Alias for field number 1
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.get_quantization_params">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              get_quantization_params
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               onnx.onnx_ONNX_REL_1_7_ml_pb2.ModelProto
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               node
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               onnx.onnx_ONNX_REL_1_7_ml_pb2.NodeProto
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               include_target
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams" title="sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.QuantizationParams
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#get_quantization_params">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.get_quantization_params" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – ONNX model to read from
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  node
                 </strong>
                 – A QuantizeLinear or DequantizeLinear Node
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  include_target
                 </strong>
                 – Set True include quantization target. If False,
target value will be returned as None. Default is None
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               QuantizationParams object with scale and zero point, will include the
quantization target if it is an initializer otherwise target will be None
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.quantize_torch_qat_export">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              quantize_torch_qat_export
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               onnx.onnx_ONNX_REL_1_7_ml_pb2.ModelProto
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_file_path
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inplace
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             onnx.onnx_ONNX_REL_1_7_ml_pb2.ModelProto
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#quantize_torch_qat_export">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.quantize_torch_qat_export" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – The model to convert, or a file path to it
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_file_path
                 </strong>
                 – File path to save the converted model to
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inplace
                 </strong>
                 – If true, does conversion of model in place. Default is true
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               Converts a model exported from a torch QAT session from a QAT graph with
fake quantize ops surrounding operations to a quantized graph with quantized
operations. All quantized Convs and FC inputs and outputs be surrounded by
fake quantize ops
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.utils.quantization.quantize_qat_export.skip_onnx_input_quantize">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.utils.quantization.quantize_qat_export.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              skip_onnx_input_quantize
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               onnx.onnx_ONNX_REL_1_7_ml_pb2.ModelProto
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_file_path
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/utils/quantization/quantize_qat_export.html#skip_onnx_input_quantize">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.utils.quantization.quantize_qat_export.skip_onnx_input_quantize" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             If the given model has a single FP32 input that feeds into a QuantizeLinear
node, then the input will be changed to uint8 and the QuantizeLinear node will be
deleted. This enables quantize graphs to take quantized inputs instead of floats.
            </p>
            <p>
             If no optimization is made, a RuntimeError will be raised.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  model
                 </strong>
                 – The model to convert, or a file path to it
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_file_path
                 </strong>
                 – File path to save the converted model to
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.utils.quantization">
          <span id="module-contents">
          </span>
          <h2>
           Module contents
           <a class="headerlink" href="#module-sparseml.pytorch.utils.quantization" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Tools for quantizing and exporting PyTorch models
          </p>
         </div>
        </div>
       </div>
      </div>
      <footer>
       <div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
        <a accesskey="n" class="btn btn-neutral float-right" href="sparseml.sparsification.html" rel="next" title="sparseml.sparsification package">
         Next
         <span aria-hidden="true" class="fa fa-arrow-circle-right">
         </span>
        </a>
        <a accesskey="p" class="btn btn-neutral float-left" href="sparseml.pytorch.utils.html" rel="prev" title="sparseml.pytorch.utils package">
         <span aria-hidden="true" class="fa fa-arrow-circle-left">
         </span>
         Previous
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the "License").
        </p>
       </div>
       Built with
       <a href="https://www.sphinx-doc.org/">
        Sphinx
       </a>
       using a
       <a href="https://github.com/readthedocs/sphinx_rtd_theme">
        theme
       </a>
       provided by
       <a href="https://readthedocs.org">
        Read the Docs
       </a>
       .
      </footer>
     </div>
    </div>
   </section>
  </div>
  <div aria-label="versions" class="rst-versions" data-toggle="rst-versions" role="note">
   <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book">
     Other Versions
    </span>
    v: v0.3.1
    <span class="fa fa-caret-down">
    </span>
   </span>
   <div class="rst-other-versions">
    <dl>
     <dt>
      Tags
     </dt>
     <dd>
      <a href="../v0.3.0/api/sparseml.pytorch.utils.quantization.html">
       v0.3.0
      </a>
     </dd>
     <dd>
      <a href="sparseml.pytorch.utils.quantization.html">
       v0.3.1
      </a>
     </dd>
    </dl>
    <dl>
     <dt>
      Branches
     </dt>
     <dd>
      <a href="../main/api/sparseml.pytorch.utils.quantization.html">
       main
      </a>
     </dd>
    </dl>
   </div>
  </div>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <!-- Theme Analytics -->
  <script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
  </script>
 </body>
</html>