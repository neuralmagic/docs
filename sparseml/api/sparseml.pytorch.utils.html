

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>sparseml.pytorch.utils package &mdash; SparseML 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sparseml.tensorflow_v1 package" href="sparseml.tensorflow_v1.html" />
    <link rel="prev" title="sparseml.pytorch.optim.quantization package" href="sparseml.pytorch.optim.quantization.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> SparseML
          

          
            
            <img src="../_static/icon-sparseml.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes.html">Sparsification Recipes</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="sparseml.html">sparseml package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sparseml.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sparseml.keras.html">sparseml.keras package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.onnx.html">sparseml.onnx package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.optim.html">sparseml.optim package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="sparseml.pytorch.html">sparseml.pytorch package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="sparseml.pytorch.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.tensorflow_v1.html">sparseml.tensorflow_v1 package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.utils.html">sparseml.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.log">sparseml.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml">Module contents</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Help</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">Bugs, Feature Requests</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/discussions">Support, General Q&amp;A</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.neuralmagic.com">Neural Magic Docs</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseML</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="sparseml.html">sparseml package</a> &raquo;</li>
        
          <li><a href="sparseml.pytorch.html">sparseml.pytorch package</a> &raquo;</li>
        
      <li>sparseml.pytorch.utils package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api/sparseml.pytorch.utils.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sparseml-pytorch-utils-package">
<h1>sparseml.pytorch.utils package<a class="headerlink" href="#sparseml-pytorch-utils-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-sparseml.pytorch.utils.benchmarker">
<span id="sparseml-pytorch-utils-benchmarker-module"></span><h2>sparseml.pytorch.utils.benchmarker module<a class="headerlink" href="#module-sparseml.pytorch.utils.benchmarker" title="Permalink to this headline">¶</a></h2>
<p>Benchmarking PyTorch models on a given device for given batch sizes</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.benchmarker.</code><code class="sig-name descname">BatchBenchmarkResults</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/benchmarker.html#BatchBenchmarkResults"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container class for the results of a benchmark run for a given batch size.
Contains convenience methods for calculating different metrics around the time
to run each batch and the items.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_size</strong> – the batch size the results are for</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_sec</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">e2e_sec</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/benchmarker.html#BatchBenchmarkResults.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a new batch result</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_sec</strong> – the seconds it took to execute the model</p></li>
<li><p><strong>e2e_sec</strong> – the seconds it took to execute model and transfer to and
from device</p></li>
<li><p><strong>batch_size</strong> – the size of the batch recorded</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.batch_size">
<em class="property">property </em><code class="sig-name descname">batch_size</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>the batch size the results are for</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batch_seconds">
<em class="property">property </em><code class="sig-name descname">e2e_batch_seconds</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batch_seconds" title="Permalink to this definition">¶</a></dt>
<dd><p>the average overall time to execute the batches through the model
and the system.
Includes model execution time as well as time to transfer the data to
and from a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batch_timings">
<em class="property">property </em><code class="sig-name descname">e2e_batch_timings</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batch_timings" title="Permalink to this definition">¶</a></dt>
<dd><p>the overall timings in seconds for each batch to run through the model
and the system.
Includes model execution time as well as time to transfer the data to and
from a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batches_per_second">
<em class="property">property </em><code class="sig-name descname">e2e_batches_per_second</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_batches_per_second" title="Permalink to this definition">¶</a></dt>
<dd><p>inverse of e2e_batch_seconds</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_item_seconds">
<em class="property">property </em><code class="sig-name descname">e2e_item_seconds</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_item_seconds" title="Permalink to this definition">¶</a></dt>
<dd><p>the batch averaged overall time to execute the batches through the
model and the system (e2e_batch_seconds / batch_size).
Includes model execution time as well as time to transfer the data to
and from a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_items_per_second">
<em class="property">property </em><code class="sig-name descname">e2e_items_per_second</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.e2e_items_per_second" title="Permalink to this definition">¶</a></dt>
<dd><p>inverse of e2e_item_seconds</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batch_seconds">
<em class="property">property </em><code class="sig-name descname">model_batch_seconds</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batch_seconds" title="Permalink to this definition">¶</a></dt>
<dd><p>the average time it took to execute the batches through the model.
Does not include time for transferring data to and from device (if any)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batch_timings">
<em class="property">property </em><code class="sig-name descname">model_batch_timings</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batch_timings" title="Permalink to this definition">¶</a></dt>
<dd><p>the overall timings in seconds for each batch to run through the model.
Does not include time for transferring data to and from device (if any)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batches_per_second">
<em class="property">property </em><code class="sig-name descname">model_batches_per_second</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_batches_per_second" title="Permalink to this definition">¶</a></dt>
<dd><p>inverse of model_batch_seconds</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_item_seconds">
<em class="property">property </em><code class="sig-name descname">model_item_seconds</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_item_seconds" title="Permalink to this definition">¶</a></dt>
<dd><p>the batch averaged time it took in seconds to execute one item
through the model (model_batch_seconds / batch_size).
Does not include time for transferring data to and from device (if any)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_items_per_second">
<em class="property">property </em><code class="sig-name descname">model_items_per_second</code><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults.model_items_per_second" title="Permalink to this definition">¶</a></dt>
<dd><p>inverse of model_items_per_second</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.benchmarker.ModuleBenchmarker">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.benchmarker.</code><code class="sig-name descname">ModuleBenchmarker</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/benchmarker.html#ModuleBenchmarker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.ModuleBenchmarker" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convenience class for benchmarking a model on a given device for given batches
at a given precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the module to benchmark</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.benchmarker.ModuleBenchmarker.run_batches_on_device">
<code class="sig-name descname">run_batches_on_device</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batches</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">full_precision</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">test_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">100</span></em>, <em class="sig-param"><span class="n">warmup_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">10</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults" title="sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults">sparseml.pytorch.utils.benchmarker.BatchBenchmarkResults</a><a class="reference internal" href="../_modules/sparseml/pytorch/utils/benchmarker.html#ModuleBenchmarker.run_batches_on_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.benchmarker.ModuleBenchmarker.run_batches_on_device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batches</strong> – the batches to run through the model and benchmark,
should all be of the same batch_size</p></li>
<li><p><strong>device</strong> – the device to run the model on, ex: cpu, cuda, cuda:0, cuda:0,1</p></li>
<li><p><strong>full_precision</strong> – True to run at float32, False to run at float16</p></li>
<li><p><strong>test_size</strong> – the number of batches to run and calculate timings over</p></li>
<li><p><strong>warmup_size</strong> – the number of batches to run before calculating timings</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the batch results for benchmarking</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.exporter">
<span id="sparseml-pytorch-utils-exporter-module"></span><h2>sparseml.pytorch.utils.exporter module<a class="headerlink" href="#module-sparseml.pytorch.utils.exporter" title="Permalink to this headline">¶</a></h2>
<p>Export PyTorch models to the local device</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.exporter.ModuleExporter">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.exporter.</code><code class="sig-name descname">ModuleExporter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">output_dir</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/exporter.html#ModuleExporter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.exporter.ModuleExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An exporter for exporting PyTorch modules into ONNX format
as well as numpy arrays for the input and output tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to export</p></li>
<li><p><strong>output_dir</strong> – the directory to export the module and extras to</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.exporter.ModuleExporter.export_onnx">
<code class="sig-name descname">export_onnx</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sample_batch</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'model.onnx'</span></em>, <em class="sig-param"><span class="n">opset</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">11</span></em>, <em class="sig-param"><span class="n">disable_bn_fusing</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/exporter.html#ModuleExporter.export_onnx"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.exporter.ModuleExporter.export_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Export an onnx file for the current module and for a sample batch.
Sample batch used to feed through the model to freeze the graph for a
particular execution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_batch</strong> – the batch to export an onnx for, handles creating the
static graph for onnx as well as setting dimensions</p></li>
<li><p><strong>name</strong> – name of the onnx file to save</p></li>
<li><p><strong>opset</strong> – onnx opset to use for exported model. Default is 11, if torch
version is 1.2 or below, default is 9</p></li>
<li><p><strong>disable_bn_fusing</strong> – torch &gt;= 1.7.0 only. Set True to disable batch norm
fusing during torch export. Default and suggested setting is True. Batch
norm fusing will change the exported parameter names as well as affect
sensitivity analyses of the exported graph.  Additionally, the DeepSparse
inference engine, and other engines, perform batch norm fusing at model
compilation.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.exporter.ModuleExporter.export_pytorch">
<code class="sig-name descname">export_pytorch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.optim.optimizer.Optimizer<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'model.pth'</span></em>, <em class="sig-param"><span class="n">use_zipfile_serialization_if_available</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_modifiers</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/exporter.html#ModuleExporter.export_pytorch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.exporter.ModuleExporter.export_pytorch" title="Permalink to this definition">¶</a></dt>
<dd><p>Export the pytorch state dicts into pth file within a
pytorch framework directory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – optional optimizer to export along with the module</p></li>
<li><p><strong>epoch</strong> – optional epoch to export along with the module</p></li>
<li><p><strong>name</strong> – name of the pytorch file to save</p></li>
<li><p><strong>use_zipfile_serialization_if_available</strong> – for torch &gt;= 1.6.0 only
exports the Module’s state dict using the new zipfile serialization</p></li>
<li><p><strong>include_modifiers</strong> – if True, and a ScheduledOptimizer is provided
as the optimizer, the associated ScheduledModifierManager and its
Modifiers will be exported under the ‘manager’ key. Default is False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.exporter.ModuleExporter.export_samples">
<code class="sig-name descname">export_samples</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sample_batches</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sample_labels</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">exp_counter</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/exporter.html#ModuleExporter.export_samples"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.exporter.ModuleExporter.export_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Export a set list of sample batches as inputs and outputs through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_batches</strong> – a list of the sample batches to feed through the module
for saving inputs and outputs</p></li>
<li><p><strong>sample_labels</strong> – an optional list of sample labels that correspond to the
the batches for saving</p></li>
<li><p><strong>exp_counter</strong> – the counter to start exporting the tensor files at</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.helpers">
<span id="sparseml-pytorch-utils-helpers-module"></span><h2>sparseml.pytorch.utils.helpers module<a class="headerlink" href="#module-sparseml.pytorch.utils.helpers" title="Permalink to this headline">¶</a></h2>
<p>Utility / helper functions</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.helpers.NamedLayerParam">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">NamedLayerParam</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer_name</span></em>, <em class="sig-param"><span class="n">layer</span></em>, <em class="sig-param"><span class="n">param_name</span></em>, <em class="sig-param"><span class="n">param</span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparseml.pytorch.utils.helpers.NamedLayerParam" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<dl class="py method">
<dt id="sparseml.pytorch.utils.helpers.NamedLayerParam.layer">
<em class="property">property </em><code class="sig-name descname">layer</code><a class="headerlink" href="#sparseml.pytorch.utils.helpers.NamedLayerParam.layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.helpers.NamedLayerParam.layer_name">
<em class="property">property </em><code class="sig-name descname">layer_name</code><a class="headerlink" href="#sparseml.pytorch.utils.helpers.NamedLayerParam.layer_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.helpers.NamedLayerParam.param">
<em class="property">property </em><code class="sig-name descname">param</code><a class="headerlink" href="#sparseml.pytorch.utils.helpers.NamedLayerParam.param" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.helpers.NamedLayerParam.param_name">
<em class="property">property </em><code class="sig-name descname">param_name</code><a class="headerlink" href="#sparseml.pytorch.utils.helpers.NamedLayerParam.param_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.any_str_or_regex_matches_param_name">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">any_str_or_regex_matches_param_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">param_name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">name_or_regex_patterns</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#any_str_or_regex_matches_param_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.any_str_or_regex_matches_param_name" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_name</strong> – The name of a parameter</p></li>
<li><p><strong>name_or_regex_patterns</strong> – List of full param names to match to the input or
regex patterns to match with that should be prefixed with ‘re:’</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if any given str or regex pattern matches the given name</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.default_device">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">default_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#default_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.default_device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the device that should be defaulted to for the current setup.
if multiple gpus are available then will return a string with all of them,
else if single gpu available then will return cuda,
else returns cpu</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.early_stop_data_loader">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">early_stop_data_loader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data_loader</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em>, <em class="sig-param"><span class="n">early_stop_steps</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#early_stop_data_loader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.early_stop_data_loader" title="Permalink to this definition">¶</a></dt>
<dd><p>An iterator that goes through the data_loader for yields and stops
after early_stop_steps instead of the full loader</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_loader</strong> – the data loader to continually repeat</p></li>
<li><p><strong>early_stop_steps</strong> – if set, the number of steps to run and break out early
instead of running all of the steps in the data loader,
if &lt; 1 then will run the full length</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an iterable for the never ending data loader</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_conv_layers">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_conv_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_conv_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_conv_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the module to grab all conv layers for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of all the conv layers in the module</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_layer">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_layer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the name of the layer to grab from the module</p></li>
<li><p><strong>module</strong> – the module containing the layer to grab</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the module representing the layer in the module</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_layer_param">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_layer_param</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">param</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; torch.nn.parameter.Parameter<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_layer_param"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_layer_param" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param</strong> – the name of the param to grab from the layer</p></li>
<li><p><strong>layer</strong> – the name of the layer to grab from the module</p></li>
<li><p><strong>module</strong> – the module containing the layer and the param</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the param taken from the given layer in the module</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_linear_layers">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_linear_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_linear_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_linear_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the module to grab all linear layers for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of all linear layers in the module</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_named_layers_and_params_by_regex">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_named_layers_and_params_by_regex</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">param_names</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">params_strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="#sparseml.pytorch.utils.helpers.NamedLayerParam" title="sparseml.pytorch.utils.helpers.NamedLayerParam">sparseml.pytorch.utils.helpers.NamedLayerParam</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_named_layers_and_params_by_regex"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_named_layers_and_params_by_regex" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to get the matching layers and params from</p></li>
<li><p><strong>param_names</strong> – a list of names or regex patterns to match with full parameter
paths. Regex patterns must be specified with the prefix ‘re:’</p></li>
<li><p><strong>params_strict</strong> – if True, this function will raise an exception if there a
parameter is not found to match every name or regex in param_names</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of NamedLayerParam tuples whose full parameter names in the given
module match one of the given regex patterns or parameter names</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_optim_learning_rate">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_optim_learning_rate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optim</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span> &#x2192; float<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_optim_learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_optim_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optim</strong> – The optimizer to get the learning rate for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>convenience function to get the first learning rate for any of
the param groups in the optimizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_prunable_layers">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_prunable_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>Tuple<span class="p">[</span>str<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_prunable_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_prunable_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the module to get the prunable layers from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list containing the names and modules of the prunable layers
(Linear, ConvNd)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.get_terminal_layers">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">get_terminal_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.nn.modules.module.Module<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#get_terminal_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.get_terminal_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the module to grab all terminal layers for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of all of the terminal layers in a model
(ie not containers; so convs, linears, activations, etc)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.infinite_data_loader">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">infinite_data_loader</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data_loader</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em>, <em class="sig-param"><span class="n">early_stop_steps</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">cache</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#infinite_data_loader"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.infinite_data_loader" title="Permalink to this definition">¶</a></dt>
<dd><p>A never ending data loader that will keep repeating the one passed in.
Will additionally cache the data if requested.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_loader</strong> – the data loader to continually repeat</p></li>
<li><p><strong>early_stop_steps</strong> – if set, the number of steps to run and break out early
instead of running all of the steps in the data loader</p></li>
<li><p><strong>cache</strong> – True to cache the results in memory and return those on
subsequent requests, False otherwise</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an iterable for the never ending data loader</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.mask_difference">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">mask_difference</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">old_mask</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">new_mask</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#mask_difference"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.mask_difference" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>old_mask</strong> – the old mask to compare against for calculating the difference</p></li>
<li><p><strong>new_mask</strong> – the new mask to compare with for calculating the difference</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tensor representing the change from the old_mask to the new_mask
specifically values returned as 1.0 are newly unmasked (0.0 =&gt; 1.0)
values returned as -1.0 are newly masked (1.0 =&gt; 0.0)
values returned as 0.0 had no change in (0.0 =&gt; 0.0 or 1.0 =&gt; 1.0)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.set_deterministic_seeds">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">set_deterministic_seeds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">seed</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#set_deterministic_seeds"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.set_deterministic_seeds" title="Permalink to this definition">¶</a></dt>
<dd><p>Manually seeds the numpy, random, and torch packages.
Also sets torch.backends.cudnn.deterministic to True
:param seed: the manual seed to use. Default is 0</p>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.set_optim_learning_rate">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">set_optim_learning_rate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optim</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#set_optim_learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.set_optim_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optim</strong> – The optimizer to set the learning rate for</p></li>
<li><p><strong>value</strong> – the learning rate to set for the optimizer,
will set all param groups in the optim to this value</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensor_density">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensor_density</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tens</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>Iterable<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensor_density"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensor_density" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tens</strong> – the tensor to calculate the density for</p></li>
<li><p><strong>dim</strong> – the dimension(s) to split the calculations over; ex, can split over
batch, channels, or combos</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the density of the input tens, ie the fraction of numbers that are non zero</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensor_export">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensor_export</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">export_dir</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">npz</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensor_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensor_export" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – tensor to export to a saved numpy array file</p></li>
<li><p><strong>export_dir</strong> – the directory to export the file in</p></li>
<li><p><strong>name</strong> – the name of the file, .npy will be appended to it</p></li>
<li><p><strong>npz</strong> – True to export as an npz file, False otherwise</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the path of the numpy file the tensor was exported to</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensor_sample">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensor_sample</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tens</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">sample_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>int<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensor_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensor_sample" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tens</strong> – the tensor to grab samples from</p></li>
<li><p><strong>sample_size</strong> – the number of samples to grab overall if dim is not supplied
or per each dim if it is</p></li>
<li><p><strong>dim</strong> – the dimension(s) to split the samples over;
ex, can split over batch, channels, or combos</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sampled tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensor_sparsity">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensor_sparsity</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tens</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>int<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensor_sparsity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensor_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tens</strong> – the tensor to calculate the sparsity for</p></li>
<li><p><strong>dim</strong> – the dimension(s) to split the calculations over;
ex, can split over batch, channels, or combos</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sparsity of the input tens, ie the fraction of numbers that are zero</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensors_batch_size">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensors_batch_size</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensors_batch_size"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensors_batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Default function for getting the batch size from a tensor or collection of tensors.
Returns the batch size (zeroth index for shape) of the first found tensor.</p>
<dl class="simple">
<dt>Supported use cases:</dt><dd><ul class="simple">
<li><p>single tensor</p></li>
<li><p>Dictionary of single tensors</p></li>
<li><p>Dictionary of iterable of tensors</p></li>
<li><p>Dictionary of dictionary of tensors</p></li>
<li><p>Iterable of single tensors</p></li>
<li><p>Iterable of iterable of tensors</p></li>
<li><p>Iterable of dictionary of tensors</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensors</strong> – the tensor or collection of tensors to get a batch size from,
taken from the first found tensor</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the batch size (0th element of shape) of the first contained
tensor in the data</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensors_export">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensors_export</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">export_dir</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">name_prefix</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">break_batch</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensors_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensors_export" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – the tensors to export to a saved numpy array file</p></li>
<li><p><strong>export_dir</strong> – the directory to export the files in</p></li>
<li><p><strong>name_prefix</strong> – the prefix name for the tensors to save as, will append
info about the position of the tensor in a list or dict in addition
to the .npy file format</p></li>
<li><p><strong>counter</strong> – the current counter to save the tensor at</p></li>
<li><p><strong>break_batch</strong> – treat the tensor as a batch and break apart into
multiple tensors</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the exported paths</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensors_module_forward">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensors_module_forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">check_feat_lab_inp</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensors_module_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensors_module_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Default function for calling into a model with data for a forward execution.
Returns the model result.
Note, if an iterable the features to be passed into the model are considered
to be at index 0 and other indices are for labels.</p>
<p>Supported use cases: single tensor,
iterable with first tensor taken as the features to pass into the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – the data to be passed into the model, if an iterable the features
to be passed into the model are considered to be at index 0 and other indices
are for labels</p></li>
<li><p><strong>module</strong> – the module to pass the data into</p></li>
<li><p><strong>check_feat_lab_inp</strong> – True to check if the incoming tensors looks like
it’s made up of features and labels ie a tuple or list with 2 items
(typical output from a data loader) and will call into the model with just
the first element assuming it’s the features False to not check</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the result of calling into the model for a forward pass</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensors_to_device">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensors_to_device</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensors_to_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensors_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Default function for putting a tensor or collection of tensors to the proper device.
Returns the tensor references after being placed on the proper device.</p>
<dl class="simple">
<dt>Supported use cases:</dt><dd><ul class="simple">
<li><p>single tensor</p></li>
<li><p>Dictionary of single tensors</p></li>
<li><p>Dictionary of iterable of tensors</p></li>
<li><p>Dictionary of dictionary of tensors</p></li>
<li><p>Iterable of single tensors</p></li>
<li><p>Iterable of iterable of tensors</p></li>
<li><p>Iterable of dictionary of tensors</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – the tensors or collection of tensors to put onto a device</p></li>
<li><p><strong>device</strong> – the string representing the device to put the tensors on,
ex: ‘cpu’, ‘cuda’, ‘cuda:1’</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the tensors or collection of tensors after being placed on the device</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.tensors_to_precision">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">tensors_to_precision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensors</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">full_precision</span><span class="p">:</span> <span class="n">bool</span></em><span class="sig-paren">)</span> &#x2192; Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Iterable<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>Any<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#tensors_to_precision"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.tensors_to_precision" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> – the tensors to change the precision of</p></li>
<li><p><strong>full_precision</strong> – True for full precision (float 32) and
False for half (float 16)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the tensors converted to the desired precision</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.helpers.torch_distributed_zero_first">
<code class="sig-prename descclassname">sparseml.pytorch.utils.helpers.</code><code class="sig-name descname">torch_distributed_zero_first</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">local_rank</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/helpers.html#torch_distributed_zero_first"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.helpers.torch_distributed_zero_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator to make all processes in distributed training wait for each
local 0 ranked process to do something.
:param local_rank: the local rank of this process</p>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.logger">
<span id="sparseml-pytorch-utils-logger-module"></span><h2>sparseml.pytorch.utils.logger module<a class="headerlink" href="#module-sparseml.pytorch.utils.logger" title="Permalink to this headline">¶</a></h2>
<p>Contains code for loggers that help visualize the information from each modifier</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.logger.</code><code class="sig-name descname">PyTorchLogger</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base class that all modifier loggers must implement.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> – name given to the logger, used for identification</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.log_histogram">
<em class="property">abstract </em><code class="sig-name descname">log_histogram</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bins</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'tensorflow'</span></em>, <em class="sig-param"><span class="n">max_bins</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger.log_histogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.log_histogram" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>values</strong> – values to log as a histogram</p></li>
<li><p><strong>bins</strong> – the type of bins to use for grouping the values,
follows tensorboard terminology</p></li>
<li><p><strong>max_bins</strong> – maximum number of bins to use (default None)</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.log_histogram_raw">
<em class="property">abstract </em><code class="sig-name descname">log_histogram_raw</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">min_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">max_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_vals</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">sum_vals</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sum_squares</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_limits</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_counts</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger.log_histogram_raw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.log_histogram_raw" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>min_val</strong> – min value</p></li>
<li><p><strong>max_val</strong> – max value</p></li>
<li><p><strong>num_vals</strong> – number of values</p></li>
<li><p><strong>sum_vals</strong> – sum of all the values</p></li>
<li><p><strong>sum_squares</strong> – sum of the squares of all the values</p></li>
<li><p><strong>bucket_limits</strong> – upper value per bucket</p></li>
<li><p><strong>bucket_counts</strong> – number of values per bucket</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.log_hyperparams">
<em class="property">abstract </em><code class="sig-name descname">log_hyperparams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger.log_hyperparams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.log_hyperparams" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – Each key-value pair in the dictionary is the name of the
hyper parameter and it’s corresponding value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.log_scalar">
<em class="property">abstract </em><code class="sig-name descname">log_scalar</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger.log_scalar"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.log_scalar" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the value with</p></li>
<li><p><strong>value</strong> – value to save</p></li>
<li><p><strong>step</strong> – global step for when the value was taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the value was taken</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.log_scalars">
<em class="property">abstract </em><code class="sig-name descname">log_scalars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PyTorchLogger.log_scalars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.log_scalars" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the values with</p></li>
<li><p><strong>values</strong> – values to save</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PyTorchLogger.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#sparseml.pytorch.utils.logger.PyTorchLogger.name" title="Permalink to this definition">¶</a></dt>
<dd><p>name given to the logger, used for identification</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.logger.PythonLogger">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.logger.</code><code class="sig-name descname">PythonLogger</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">logger</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>logging.Logger<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'python'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.logger.PyTorchLogger</span></code></a></p>
<p>Modifier logger that handles printing values into a python logger instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>logger</strong> – a logger instance to log to, if None then will create it’s own</p></li>
<li><p><strong>name</strong> – name given to the logger, used for identification;
defaults to python</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PythonLogger.log_histogram">
<code class="sig-name descname">log_histogram</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bins</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'tensorflow'</span></em>, <em class="sig-param"><span class="n">max_bins</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger.log_histogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger.log_histogram" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>values</strong> – values to log as a histogram</p></li>
<li><p><strong>bins</strong> – the type of bins to use for grouping the values,
follows tensorboard terminology</p></li>
<li><p><strong>max_bins</strong> – maximum number of bins to use (default None)</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PythonLogger.log_histogram_raw">
<code class="sig-name descname">log_histogram_raw</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">min_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">max_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_vals</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">sum_vals</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sum_squares</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_limits</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_counts</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger.log_histogram_raw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger.log_histogram_raw" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>min_val</strong> – min value</p></li>
<li><p><strong>max_val</strong> – max value</p></li>
<li><p><strong>num_vals</strong> – number of values</p></li>
<li><p><strong>sum_vals</strong> – sum of all the values</p></li>
<li><p><strong>sum_squares</strong> – sum of the squares of all the values</p></li>
<li><p><strong>bucket_limits</strong> – upper value per bucket</p></li>
<li><p><strong>bucket_counts</strong> – number of values per bucket</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PythonLogger.log_hyperparams">
<code class="sig-name descname">log_hyperparams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger.log_hyperparams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger.log_hyperparams" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – Each key-value pair in the dictionary is the name of the
hyper parameter and it’s corresponding value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PythonLogger.log_scalar">
<code class="sig-name descname">log_scalar</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger.log_scalar"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger.log_scalar" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the value with</p></li>
<li><p><strong>value</strong> – value to save</p></li>
<li><p><strong>step</strong> – global step for when the value was taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the value was taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.PythonLogger.log_scalars">
<code class="sig-name descname">log_scalars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#PythonLogger.log_scalars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.PythonLogger.log_scalars" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the values with</p></li>
<li><p><strong>values</strong> – values to save</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.logger.</code><code class="sig-name descname">TensorBoardLogger</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_path</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">writer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.utils.tensorboard.writer.SummaryWriter<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'tensorboard'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.logger.PyTorchLogger</span></code></a></p>
<p>Modifier logger that handles outputting values into a TensorBoard log directory
for viewing in TensorBoard.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_path</strong> – the path to create a SummaryWriter at. writer must be None
to use if not supplied (and writer is None),
will create a TensorBoard dir in cwd</p></li>
<li><p><strong>writer</strong> – the writer to log results to,
if none is given creates a new one at the log_path</p></li>
<li><p><strong>name</strong> – name given to the logger, used for identification;
defaults to tensorboard</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger.log_histogram">
<code class="sig-name descname">log_histogram</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bins</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'tensorflow'</span></em>, <em class="sig-param"><span class="n">max_bins</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger.log_histogram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger.log_histogram" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>values</strong> – values to log as a histogram</p></li>
<li><p><strong>bins</strong> – the type of bins to use for grouping the values,
follows tensorboard terminology</p></li>
<li><p><strong>max_bins</strong> – maximum number of bins to use (default None)</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger.log_histogram_raw">
<code class="sig-name descname">log_histogram_raw</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">min_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">max_val</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">num_vals</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">sum_vals</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sum_squares</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_limits</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">bucket_counts</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>torch.Tensor<span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger.log_histogram_raw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger.log_histogram_raw" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the histogram with</p></li>
<li><p><strong>min_val</strong> – min value</p></li>
<li><p><strong>max_val</strong> – max value</p></li>
<li><p><strong>num_vals</strong> – number of values</p></li>
<li><p><strong>sum_vals</strong> – sum of all the values</p></li>
<li><p><strong>sum_squares</strong> – sum of the squares of all the values</p></li>
<li><p><strong>bucket_limits</strong> – upper value per bucket</p></li>
<li><p><strong>bucket_counts</strong> – number of values per bucket</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger.log_hyperparams">
<code class="sig-name descname">log_hyperparams</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger.log_hyperparams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger.log_hyperparams" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> – Each key-value pair in the dictionary is the name of the
hyper parameter and it’s corresponding value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger.log_scalar">
<code class="sig-name descname">log_scalar</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger.log_scalar"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger.log_scalar" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the value with</p></li>
<li><p><strong>value</strong> – value to save</p></li>
<li><p><strong>step</strong> – global step for when the value was taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the value was taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.logger.TensorBoardLogger.log_scalars">
<code class="sig-name descname">log_scalars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tag</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">values</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">wall_time</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/logger.html#TensorBoardLogger.log_scalars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.logger.TensorBoardLogger.log_scalars" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tag</strong> – identifying tag to log the values with</p></li>
<li><p><strong>values</strong> – values to save</p></li>
<li><p><strong>step</strong> – global step for when the values were taken</p></li>
<li><p><strong>wall_time</strong> – global wall time for when the values were taken,
defaults to time.time()</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.loss">
<span id="sparseml-pytorch-utils-loss-module"></span><h2>sparseml.pytorch.utils.loss module<a class="headerlink" href="#module-sparseml.pytorch.utils.loss" title="Permalink to this headline">¶</a></h2>
<p>Code related to convenience functions for controlling the calculation of losses and
metrics.
Additionally adds in support for knowledge distillation</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.Accuracy">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">Accuracy</code><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#Accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.Accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Class for calculating the accuracy for a given prediction and the labels
for comparison.
Expects the inputs to be from a range of 0 to 1 and sets a crossing threshold at 0.5
the labels are similarly rounded.</p>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.Accuracy.calculate">
<em class="property">static </em><code class="sig-name descname">calculate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">lab</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#Accuracy.calculate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.Accuracy.calculate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – the models prediction to compare with</p></li>
<li><p><strong>lab</strong> – the labels for the data to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated accuracy</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.Accuracy.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">lab</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#Accuracy.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.Accuracy.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – the models prediction to compare with</p></li>
<li><p><strong>lab</strong> – the labels for the data to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated accuracy</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.utils.loss.Accuracy.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#sparseml.pytorch.utils.loss.Accuracy.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.BinaryCrossEntropyLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">BinaryCrossEntropyLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#BinaryCrossEntropyLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.BinaryCrossEntropyLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Convenience class for doing binary cross entropy loss calculations,
ie the default loss function is TF.binary_cross_entropy_with_logits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extras</strong> – extras representing other metrics that should be calculated
in addition to the loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.CrossEntropyLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">CrossEntropyLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#CrossEntropyLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.CrossEntropyLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Convenience class for doing cross entropy loss calculations,
ie the default loss function is TF.cross_entropy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extras</strong> – extras representing other metrics that should be calculated
in addition to the loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">InceptionCrossEntropyLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">aux_weight</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.4</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#InceptionCrossEntropyLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Loss wrapper for training an inception model that as an aux output
with cross entropy.</p>
<p>Defines the loss in the following way:
aux_weight * cross_entropy(aux_pred, lab) + cross_entropy(pred, lab)</p>
<p>Additionally adds cross_entropy into the extras.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>extras</strong> – extras representing other metrics that should be calculated
in addition to the loss</p></li>
<li><p><strong>aux_weight</strong> – the weight to use for the cross_entropy value
calculated from the aux output</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper.get_preds">
<code class="sig-name descname">get_preds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Union<span class="p">[</span>torch.Tensor<span class="p">, </span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#InceptionCrossEntropyLossWrapper.get_preds"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper.get_preds" title="Permalink to this definition">¶</a></dt>
<dd><p>Override get_preds for the inception training output.
Specifically expects the pred from the model to be a three tensor tuple:
(aux logits, logits, classes)</p>
<p>For the loss function returns a tuple containing (aux logits, logits),
for all other extras returns the logits tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader</p></li>
<li><p><strong>pred</strong> – the prediction from an inception model,
expected to be a tuple containing (aux logits, logits, classes)</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the
information for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the predictions from the model for the loss function;
a tuple containing (aux logits, logits),
for all other extras returns the logits tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper.loss">
<code class="sig-name descname">loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">preds</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#InceptionCrossEntropyLossWrapper.loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.InceptionCrossEntropyLossWrapper.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function for inception to combine the overall outputs from the model
along with the the auxiliary loss from an earlier point in the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> – the predictions tuple containing [aux output, output]</p></li>
<li><p><strong>labels</strong> – the labels to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the combined cross entropy value</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.KDLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">KDLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Callable<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">deconstruct_tensors</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">kd_settings</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span><a class="reference internal" href="#sparseml.pytorch.utils.loss.KDSettings" title="sparseml.pytorch.utils.loss.KDSettings">sparseml.pytorch.utils.loss.KDSettings</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#KDLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Special case of the loss wrapper that allows knowledge distillation.
Makes some assumptions specifically for image classification tasks,
so may not work out of the box for everything.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_fn</strong> – the loss function to calculate on forward call of this object,
accessible in the returned Dict at DEFAULT_LOSS_KEY</p></li>
<li><p><strong>extras</strong> – extras representing other metrics that should be
calculated in addition to the loss</p></li>
<li><p><strong>deconstruct_tensors</strong> – True to break the tensors up into expected
predictions and labels, False to pass the tensors as is to loss and extras</p></li>
<li><p><strong>kd_settings</strong> – the knowledge distillation settings that guide
how to calculate the total loss</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDLossWrapper.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#KDLossWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDLossWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>override to calculate the knowledge distillation loss if kd_settings
is supplied and not None</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – the input data to the model, expected to contain the labels</p></li>
<li><p><strong>pred</strong> – the predicted output from the model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionary containing all calculated losses and metrics with
the loss from the loss_fn at DEFAULT_LOSS_KEY</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDLossWrapper.get_inputs">
<code class="sig-name descname">get_inputs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#KDLossWrapper.get_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDLossWrapper.get_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>overridable function that is responsible for extracting the inputs to the model
from the input data to the model and the output from the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader, expected to contain a tuple of
(features, labels)</p></li>
<li><p><strong>pred</strong> – the predicted output from a model</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the information
for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the input data for the model</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.KDSettings">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">KDSettings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">teacher</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">temp_student</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">5.0</span></em>, <em class="sig-param"><span class="n">temp_teacher</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">5.0</span></em>, <em class="sig-param"><span class="n">weight</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">contradict_hinton</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#KDSettings"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>properties class for settings for applying knowledge distillation as
part of the loss calculation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>teacher</strong> – the teacher that provides targets for the student to learn from</p></li>
<li><p><strong>temp_student</strong> – temperature coefficient for the student</p></li>
<li><p><strong>temp_teacher</strong> – temperature coefficient for the teacher</p></li>
<li><p><strong>weight</strong> – the weight for how much of the kd loss to use in proportion
with the original loss</p></li>
<li><p><strong>contradict_hinton</strong> – in hinton’s original paper they included T^2
as a scaling factor some implementations dropped this factor
so contradicting hinton does not scale by T^2</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDSettings.contradict_hinton">
<em class="property">property </em><code class="sig-name descname">contradict_hinton</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings.contradict_hinton" title="Permalink to this definition">¶</a></dt>
<dd><p>in hinton’s original paper they included T^2 as a scaling factor
some implementations dropped this factor so contradicting hinton
does not scale by T^2</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDSettings.teacher">
<em class="property">property </em><code class="sig-name descname">teacher</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings.teacher" title="Permalink to this definition">¶</a></dt>
<dd><p>the teacher that provides targets for the student to learn from</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDSettings.temp_student">
<em class="property">property </em><code class="sig-name descname">temp_student</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings.temp_student" title="Permalink to this definition">¶</a></dt>
<dd><p>temperature coefficient for the student</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDSettings.temp_teacher">
<em class="property">property </em><code class="sig-name descname">temp_teacher</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings.temp_teacher" title="Permalink to this definition">¶</a></dt>
<dd><p>temperature coefficient for the teacher</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.KDSettings.weight">
<em class="property">property </em><code class="sig-name descname">weight</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.KDSettings.weight" title="Permalink to this definition">¶</a></dt>
<dd><p>the weight for how much of the kd loss to use in proportion
with the original loss</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.LossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">LossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss_fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Callable<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">deconstruct_tensors</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#LossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.LossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Generic loss class for controlling how to feed inputs and compare
with predictions for standard loss functions and metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_fn</strong> – the loss function to calculate on forward call of this object,
accessible in the returned Dict at DEFAULT_LOSS_KEY</p></li>
<li><p><strong>extras</strong> – extras representing other metrics that should be calculated
in addition to the loss</p></li>
<li><p><strong>deconstruct_tensors</strong> – True to break the tensors up into expected
predictions and labels, False to pass the tensors as is to loss and extras</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.LossWrapper.available_losses">
<em class="property">property </em><code class="sig-name descname">available_losses</code><a class="headerlink" href="#sparseml.pytorch.utils.loss.LossWrapper.available_losses" title="Permalink to this definition">¶</a></dt>
<dd><p>a collection of all the loss and metrics keys available
for this instance</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.LossWrapper.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#LossWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.LossWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – the input data to the model, expected to contain the labels</p></li>
<li><p><strong>pred</strong> – the predicted output from the model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionary containing all calculated losses and metrics with
the loss from the loss_fn at DEFAULT_LOSS_KEY</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.LossWrapper.get_labels">
<code class="sig-name descname">get_labels</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#LossWrapper.get_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.LossWrapper.get_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>overridable function that is responsible for extracting the labels
for the loss calculation from the input data to the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader, expected to contain a tuple of
(features, labels)</p></li>
<li><p><strong>pred</strong> – the predicted output from a model</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the
information for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the label for the data</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.LossWrapper.get_preds">
<code class="sig-name descname">get_preds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#LossWrapper.get_preds"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.LossWrapper.get_preds" title="Permalink to this definition">¶</a></dt>
<dd><p>overridable function that is responsible for extracting the predictions
from a model’s output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader</p></li>
<li><p><strong>pred</strong> – the prediction from the model, if it is a tensor returns this,
if it is an iterable returns first</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the
information for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the predictions from the model for the loss function</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.SSDLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">SSDLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#SSDLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.SSDLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Loss wrapper for SSD models.  Implements the loss as the sum of:</p>
<ol class="arabic simple">
<li><p>Confidence Loss: All labels, with hard negative mining</p></li>
<li><p>Localization Loss: Only on positive labels</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>extras</strong> – extras representing other metrics that should be
calculated in addition to the loss</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.SSDLossWrapper.get_preds">
<code class="sig-name descname">get_preds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#SSDLossWrapper.get_preds"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.SSDLossWrapper.get_preds" title="Permalink to this definition">¶</a></dt>
<dd><p>Override get_preds for SSD model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader</p></li>
<li><p><strong>pred</strong> – the prediction from an ssd model: two tensors
representing object location and object label respectively</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the
information for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the predictions from the model without any changes</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.SSDLossWrapper.loss">
<code class="sig-name descname">loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">preds</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#SSDLossWrapper.loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.SSDLossWrapper.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the loss for a multibox SSD output as the sum of the confidence
and localization loss for the positive samples in the predictor with hard
negative mining.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> – the predictions tuple containing [predicted_boxes,
predicted_lables].</p></li>
<li><p><strong>labels</strong> – the labels to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the combined location and confidence losses</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.TopKAccuracy">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">TopKAccuracy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">topk</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#TopKAccuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.TopKAccuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Class for calculating the top k accuracy for a given prediction and the labels for
comparison; ie the top1 or top5 accuracy. top1 is equivalent to the Accuracy class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>topk</strong> – the numbers of buckets the model is considered to be correct within</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.TopKAccuracy.calculate">
<em class="property">static </em><code class="sig-name descname">calculate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">lab</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">topk</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#TopKAccuracy.calculate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.TopKAccuracy.calculate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – the models prediction to compare with</p></li>
<li><p><strong>lab</strong> – the labels for the data to compare to</p></li>
<li><p><strong>topk</strong> – the number of bins to be within for the correct label</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated topk accuracy</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.TopKAccuracy.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">lab</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#TopKAccuracy.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.TopKAccuracy.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pred</strong> – the models prediction to compare with</p></li>
<li><p><strong>lab</strong> – the labels for the data to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated topk accuracy</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.utils.loss.TopKAccuracy.training">
<code class="sig-name descname">training</code><em class="property">: bool</em><a class="headerlink" href="#sparseml.pytorch.utils.loss.TopKAccuracy.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.loss.YoloLossWrapper">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.loss.</code><code class="sig-name descname">YoloLossWrapper</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">extras</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Dict<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">anchor_groups</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#YoloLossWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.YoloLossWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.loss.LossWrapper</span></code></a></p>
<p>Loss wrapper for Yolo models.  Implements the loss as a sum of class loss,
objectness loss, and GIoU</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>extras</strong> – extras representing other metrics that should be
calculated in addition to the loss</p></li>
<li><p><strong>anchor_groups</strong> – List of n,2 tensors of the Yolo model’s anchor points
for each output group</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.YoloLossWrapper.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#YoloLossWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.YoloLossWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – the input data to the model, expected to contain the labels</p></li>
<li><p><strong>pred</strong> – the predicted output from the model</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionary containing all calculated losses (default, giou,
object, and class) and metrics with the loss from the loss_fn at
DEFAULT_LOSS_KEY</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.YoloLossWrapper.get_preds">
<code class="sig-name descname">get_preds</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#YoloLossWrapper.get_preds"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.YoloLossWrapper.get_preds" title="Permalink to this definition">¶</a></dt>
<dd><p>Override get_preds for SSD model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – data from a data loader</p></li>
<li><p><strong>pred</strong> – the prediction from an ssd model: two tensors
representing object location and object label respectively</p></li>
<li><p><strong>name</strong> – the name of the loss function that is asking for the
information for calculation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the predictions from the model without any changes</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.loss.YoloLossWrapper.loss">
<code class="sig-name descname">loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">preds</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/loss.html#YoloLossWrapper.loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.loss.YoloLossWrapper.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the loss for a Yolo model output as the sum of the box, object,
and class losses</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> – the predictions list containing objectness, class, and location
values for each detector in the Yolo model.</p></li>
<li><p><strong>labels</strong> – the labels to compare to</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the combined box, object, and class losses</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.model">
<span id="sparseml-pytorch-utils-model-module"></span><h2>sparseml.pytorch.utils.model module<a class="headerlink" href="#module-sparseml.pytorch.utils.model" title="Permalink to this headline">¶</a></h2>
<p>Code related to interacting with a trained model such as saving, loading, etc</p>
<dl class="py function">
<dt id="sparseml.pytorch.utils.model.device_to_name_ids">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">device_to_name_ids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>str<span class="p">, </span>Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#device_to_name_ids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.device_to_name_ids" title="Permalink to this definition">¶</a></dt>
<dd><p>Split a device string into a device and ids</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – the device string to push to; ex: cpu, cuda, cuda:0,1</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the device string and devices</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.is_parallel_model">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">is_parallel_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#is_parallel_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.is_parallel_model" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> – the model to test</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the given model is wrapped as a DataPararallel or
DistributedDataParallel Module. False otherwise</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.load_epoch">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">load_epoch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">map_location</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'cpu'</span></em><span class="sig-paren">)</span> &#x2192; Optional<span class="p">[</span>int<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#load_epoch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.load_epoch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.load_model">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">load_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">ignore_error_tensors</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">fix_data_parallel</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#load_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.load_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the state dict into a model from a given file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path to the pth file to load the state dict from.
May also be a SparseZoo stub path preceded by ‘zoo:’ with the optional
<cite>?recipe_type=</cite> argument. If given a recipe type, the base model weights
for that recipe will be loaded.</p></li>
<li><p><strong>model</strong> – the model to load the state dict into</p></li>
<li><p><strong>strict</strong> – True to enforce that all tensors match between the model
and the file; False otherwise</p></li>
<li><p><strong>ignore_error_tensors</strong> – names of tensors to ignore if they are not found
in either the model or the file</p></li>
<li><p><strong>fix_data_parallel</strong> – fix the keys in the model state dict if they
look like they came from DataParallel type setup (start with module.).
This removes “module.” all keys</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.load_optimizer">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">load_optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">map_location</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'cpu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#load_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.load_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the state dict into an optimizer from a given file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path to the pth file to load the state dict from</p></li>
<li><p><strong>optimizer</strong> – the optimizer to load the state dict into</p></li>
<li><p><strong>map_location</strong> – the location to map the values to when loading the</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the epoch saved in the file, if any</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.model_to_device">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">model_to_device</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">ddp</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>torch.nn.modules.module.Module<span class="p">, </span>str<span class="p">, </span>Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#model_to_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.model_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>The model to push onto a device or multiple devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – the model to push to a device</p></li>
<li><p><strong>device</strong> – the device string to push to; ex: cpu, cuda, cuda:0,1. For
DDP, device should be the local_rank int value; ex: 0</p></li>
<li><p><strong>ddp</strong> – set True to wrap module as a DDP object. If True, device should
be set to the local_rank int value. Default is False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the model on desired device(s),
the device name, and the ids for the device</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.parallelize_model">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">parallelize_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">ids</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.nn.modules.module.Module<a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#parallelize_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.parallelize_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Data parallelize a model across multiple devices</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – the model to parallelize across multiple devices</p></li>
<li><p><strong>ids</strong> – the ides of the devices to parallelize across</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a parallelized model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.model.save_model">
<code class="sig-prename descclassname">sparseml.pytorch.utils.model.</code><code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.optim.optimizer.Optimizer<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">use_zipfile_serialization_if_available</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">include_modifiers</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/model.html#save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.model.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save a model’s state dict into a file at the given path.
Additionally can save an optimizer’s state and the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path to save the file the states to</p></li>
<li><p><strong>model</strong> – the model to save state for</p></li>
<li><p><strong>optimizer</strong> – the optimizer, if any, to save state for</p></li>
<li><p><strong>epoch</strong> – the epoch to save</p></li>
<li><p><strong>use_zipfile_serialization_if_available</strong> – for torch &gt;= 1.6.0 only
exports the model’s state dict using the new zipfile serialization</p></li>
<li><p><strong>include_modifiers</strong> – if True, and a ScheduledOptimizer is provided
as the optimizer, the associated ScheduledModifierManager and its
Modifiers will be exported under the ‘manager’ key. Default is False</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.module">
<span id="sparseml-pytorch-utils-module-module"></span><h2>sparseml.pytorch.utils.module module<a class="headerlink" href="#module-sparseml.pytorch.utils.module" title="Permalink to this headline">¶</a></h2>
<p>Code related to running a module through training and testing over a dataset.
Allows reporting of progress and override functions and hooks.</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleDeviceContext">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleDeviceContext</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">use_mixed_precision</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">world_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleDeviceContext"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleDeviceContext" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Simple class to define device settings or context to be used when running a Module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_mixed_precision</strong> – set True to execute model using mixed precision with
torch.cuda.amp. Default is False</p></li>
<li><p><strong>world_size</strong> – the world size (total number of devices) used when running
the given module using DistributedDataParallel. Losses will be scaled by the
world size. Default is 1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleDeviceContext.default_context">
<em class="property">static </em><code class="sig-name descname">default_context</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleDeviceContext.default_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleDeviceContext.default_context" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A ModuleDeviceContext with default settings enabled</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleDeviceContext.use_mixed_precision">
<em class="property">property </em><code class="sig-name descname">use_mixed_precision</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleDeviceContext.use_mixed_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>True if mixed precision with torch.cuda.amp should be used.
False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleDeviceContext.world_size">
<em class="property">property </em><code class="sig-name descname">world_size</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleDeviceContext.world_size" title="Permalink to this definition">¶</a></dt>
<dd><p>the world size (total number of devices) used when running
the given module using DistributedDataParallel. Losses will be scaled by the
world size</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleRunFuncs</code><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunFuncs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Functions used as callables to calculate or perform necessary operations
for running a model through training or testing.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- data batch size callback</div>
<div class="line">- data to device callback</div>
<div class="line">- batch start hook</div>
<div class="line">- data model forward callback</div>
<div class="line">- batch forward hook</div>
<div class="line">- loss calculation</div>
<div class="line">- batch loss hook</div>
<div class="line">- model backward callback</div>
<div class="line">- batch backward hook</div>
<div class="line">- optimizer / gradient update</div>
<div class="line">- batch end hook</div>
</div>
</div>
<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs.batch_size">
<em class="property">property </em><code class="sig-name descname">batch_size</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs.batch_size" title="Permalink to this definition">¶</a></dt>
<dd><p>return used to calculate the batch size of a given grouping of tensors.
Expected to be called with the output from a data loader and
then return an int representing the batch size.</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs.copy">
<code class="sig-name descname">copy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">run_funcs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunFuncs.copy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Copy the functions from the current instance into a new instance</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>run_funcs</strong> – the instance to copy the functions into</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs.model_backward">
<em class="property">property </em><code class="sig-name descname">model_backward</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs.model_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>return used to call backward for a given model and the calculated losses.
Expected to be called with the model and the output from the loss function
as a dict mapping of names to tensors returns nothing</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs.model_forward">
<em class="property">property </em><code class="sig-name descname">model_forward</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs.model_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>return used to propagate a given grouping of tensors through a model and
return the result.
Expected to be called with the model and the output from a data loader
then return the result from the model forward pass.</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunFuncs.to_device">
<em class="property">property </em><code class="sig-name descname">to_device</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunFuncs.to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>return used to place a given grouping of tensors onto the proper device.
Expected to be called with the output from a data loader and the
desired device as a string then return the grouping on the proper device.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleRunHooks</code><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Container for hooks that can be added to module runs like training and testing
for different stages of running a batch through a model.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- data batch size callback</div>
<div class="line">- data to device callback</div>
<div class="line">- batch start hook</div>
<div class="line">- data model forward callback</div>
<div class="line">- batch forward hook</div>
<div class="line">- loss calculation</div>
<div class="line">- batch loss hook</div>
<div class="line">- model backward callback</div>
<div class="line">- batch backward hook</div>
<div class="line">- optimizer / gradient update</div>
<div class="line">- batch end hook</div>
</div>
</div>
<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_backward">
<code class="sig-name descname">invoke_batch_backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_count</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.invoke_batch_backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_end">
<code class="sig-name descname">invoke_batch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_count</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.invoke_batch_end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_end" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_forward">
<code class="sig-name descname">invoke_batch_forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_count</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.invoke_batch_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_loss">
<code class="sig-name descname">invoke_batch_loss</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_count</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">pred</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.invoke_batch_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_start">
<code class="sig-name descname">invoke_batch_start</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_count</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.invoke_batch_start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.invoke_batch_start" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_backward_hook">
<code class="sig-name descname">register_batch_backward_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">, </span>Any<span class="p">, </span>Any<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><span class="p">, </span>None<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.register_batch_backward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after calling backward on the loss for the batch with the following info:
(counter, step_count, batch_size, data, pred, losses)
where counter is passed in to the run (ex: epoch),
step_count is the number of items run so far,
batch_size is the number of elements fed in the batch,
data is the data output from the loader,
pred is the result from the model after the forward,
losses are the resulting loss dictionary</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> – the hook to add that is called into when reached in the
batch process</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a removable handle to remove the hook when desired</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_end_hook">
<code class="sig-name descname">register_batch_end_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">, </span>Any<span class="p">, </span>Any<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><span class="p">, </span>None<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.register_batch_end_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_end_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after all calculations are done for the batch with the following info:
(counter, step_count, batch_size, data, pred, losses)
where counter is passed in to the run (ex: epoch),
step_count is the number of items run so far,
batch_size is the number of elements fed in the batch,
data is the data output from the loader,
pred is the result from the model after the forward,
losses are the resulting loss dictionary</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> – the hook to add that is called into when reached in the
batch process</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a removable handle to remove the hook when desired</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_forward_hook">
<code class="sig-name descname">register_batch_forward_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">, </span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>None<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.utils.hooks.RemovableHandle<a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.register_batch_forward_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after forward execution of a batch in the model with the following info:
(counter, step_count, batch_size, data, pred)
where counter is passed in to the run (ex: epoch),
step_count is the number of items run so far,
batch_size is the number of elements fed in the batch,
data is the data output from the loader,
pred is the result from the model after the forward</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> – the hook to add that is called into when reached in the
batch process</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a removable handle to remove the hook when desired</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_loss_hook">
<code class="sig-name descname">register_batch_loss_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">, </span>Any<span class="p">, </span>Any<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><span class="p">, </span>None<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.register_batch_loss_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_loss_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after loss calculation of the batch with the following info:
(counter, step_count, batch_size, data, pred, losses)
where counter is passed in to the run (ex: epoch),
step_count is the number of items run so far,
batch_size is the number of elements fed in the batch,
data is the data output from the loader,
pred is the result from the model after the forward,
losses are the resulting loss dictionary</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> – the hook to add that is called into when reached in the
batch process</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a removable handle to remove the hook when desired</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_start_hook">
<code class="sig-name descname">register_batch_start_hook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">hook</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>int<span class="p">, </span>int<span class="p">, </span>int<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>None<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.utils.hooks.RemovableHandle<a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunHooks.register_batch_start_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunHooks.register_batch_start_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the start of a batch with the following info:
(counter, step_count, batch_size, data)
where counter is passed in to the run (ex: epoch),
step_count is the number of items run so far,
batch_size is the number of elements fed in the batch,
data is the data output from the loader</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook</strong> – the hook to add that is called into when reached in the
batch process</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a removable handle to remove the hook when desired</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleRunResults</code><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class containing the results / losses from a model run for training or testing
Keeps all result values as a dictionary and Tensor containing all values</p>
<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.append">
<code class="sig-name descname">append</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults.append"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.append" title="Permalink to this definition">¶</a></dt>
<dd><p>add new losses to the current stored results</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>losses</strong> – the losses to be added</p></li>
<li><p><strong>batch_size</strong> – the batch size the losses were run for</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.result">
<code class="sig-name descname">result</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">key</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults.result"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.result" title="Permalink to this definition">¶</a></dt>
<dd><p>The result of a single loss function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – the name of the loss function to get the results for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of tensors containing all of the results for that loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.result_list_tensor">
<code class="sig-name descname">result_list_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">key</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults.result_list_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.result_list_tensor" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the results as a list tensor where all items have been stacked into
the first index of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – the name of the loss function to get the results for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tensor containing all of the tensors for that result</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.result_mean">
<code class="sig-name descname">result_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">key</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults.result_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.result_mean" title="Permalink to this definition">¶</a></dt>
<dd><p>The mean result of a single loss function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – the name of the loss function to get the mean result for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a single tensor containing the average of all the results for that loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.result_std">
<code class="sig-name descname">result_std</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">key</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleRunResults.result_std"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.result_std" title="Permalink to this definition">¶</a></dt>
<dd><p>The standard deviation of the result for a single loss function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>key</strong> – the name of the loss function to get the standard
deviation result for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a single tensor containing the standard deviation of all
the results for that loss</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleRunResults.results">
<em class="property">property </em><code class="sig-name descname">results</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleRunResults.results" title="Permalink to this definition">¶</a></dt>
<dd><p>All of the stored results for the loss functions</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a mapping of name (str) to a list of tensors
that were recorded for that loss</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleTester">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleTester</code><span class="sig-paren">(</span><em class="sig-param">module: torch.nn.modules.module.Module, device: str, loss: Union[sparseml.pytorch.utils.loss.LossWrapper, Callable[[Any, Any], torch.Tensor]], loggers: Optional[List[sparseml.pytorch.utils.logger.PyTorchLogger]] = None, log_name: str = 'Test', log_steps: int = 100, log_summary: bool = True, device_context: sparseml.pytorch.utils.module.ModuleDeviceContext = &lt;sparseml.pytorch.utils.module.ModuleDeviceContext object&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleTester"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleTester" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.module.ModuleRunner</span></code></p>
<p>Container for running a module through evaluation over a given data loader
for specific settings.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- data batch size callback</div>
<div class="line">- data to device callback</div>
<div class="line">- batch start hook</div>
<div class="line">- data model forward callback</div>
<div class="line">- batch forward hook</div>
<div class="line">- loss calculation</div>
<div class="line">- batch loss hook</div>
<div class="line">- batch end hook</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the model to run evaluation for</p></li>
<li><p><strong>device</strong> – the default device to run evaluation on
(where data will be copied to)</p></li>
<li><p><strong>loss</strong> – the loss functions callable used to calculate loss values after
executing a forward pass</p></li>
<li><p><strong>loggers</strong> – list of loggers to log training results to</p></li>
<li><p><strong>log_name</strong> – the key to store all log files under</p></li>
<li><p><strong>log_steps</strong> – The number of steps (batches) to log at,
ex 100 will log every 100 batches</p></li>
<li><p><strong>log_summary</strong> – True to log the final summary results after the run completes</p></li>
<li><p><strong>device_context</strong> – ModuleDeviceContext with settings to enable mixed precision
using torch.cuda.amp or adjust losses when using DistributedDataParallel.
Default settings do not use mixed precision or account for DDP.
Will raise an exception if torch version does not support amp.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.module.ModuleTrainer">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">ModuleTrainer</code><span class="sig-paren">(</span><em class="sig-param">module: torch.nn.modules.module.Module, device: str, loss: Union[sparseml.pytorch.utils.loss.LossWrapper, Callable[[Any, Any], torch.Tensor]], optimizer: torch.optim.optimizer.Optimizer, num_accumulated_batches: int = 1, optim_closure: Union[None, Callable] = None, loggers: Optional[List[sparseml.pytorch.utils.logger.PyTorchLogger]] = None, log_name: str = 'Train', log_steps: int = 100, log_summary: bool = True, device_context: sparseml.pytorch.utils.module.ModuleDeviceContext = &lt;sparseml.pytorch.utils.module.ModuleDeviceContext object&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#ModuleTrainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.utils.module.ModuleRunner</span></code></p>
<p>Container for running a module through training over a given data loader
for specific settings.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- data batch size callback</div>
<div class="line">- data to device callback</div>
<div class="line">- batch start hook</div>
<div class="line">- data model forward callback</div>
<div class="line">- batch forward hook</div>
<div class="line">- loss calculation</div>
<div class="line">- batch loss hook</div>
<div class="line">- model backward callback</div>
<div class="line">- batch backward hook</div>
<div class="line">- optimizer / gradient update</div>
<div class="line">- batch end hook</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the model to run training for</p></li>
<li><p><strong>device</strong> – the default device to run training on (where data will be copied to)</p></li>
<li><p><strong>loss</strong> – the loss functions callable used to calculate loss values
after executing a forward pass</p></li>
<li><p><strong>optimizer</strong> – the optimizer used to apply gradient updates with</p></li>
<li><p><strong>num_accumulated_batches</strong> – number of batches to accumulate before
updating the optimizer</p></li>
<li><p><strong>optim_closure</strong> – a closure passed into the optimizer on step</p></li>
<li><p><strong>loggers</strong> – list of loggers to log training results to</p></li>
<li><p><strong>log_name</strong> – the key to store all log files under</p></li>
<li><p><strong>log_steps</strong> – The number of steps (batches) to log at,
ex 100 will log every 100 batches</p></li>
<li><p><strong>log_summary</strong> – True to log the final summary results after the run completes</p></li>
<li><p><strong>device_context</strong> – ModuleDeviceContext with settings to enable mixed precision
using torch.cuda.amp or adjust losses when using DistributedDataParallel.
Default settings do not use mixed precision or account for DDP.
Will raise an exception if torch version does not support amp.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleTrainer.num_accumulated_batches">
<em class="property">property </em><code class="sig-name descname">num_accumulated_batches</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleTrainer.num_accumulated_batches" title="Permalink to this definition">¶</a></dt>
<dd><p>number of batches to accumulate before updating the optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleTrainer.optim_closure">
<em class="property">property </em><code class="sig-name descname">optim_closure</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleTrainer.optim_closure" title="Permalink to this definition">¶</a></dt>
<dd><p>a closure passed into the optimizer on step</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.module.ModuleTrainer.optimizer">
<em class="property">property </em><code class="sig-name descname">optimizer</code><a class="headerlink" href="#sparseml.pytorch.utils.module.ModuleTrainer.optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>the optimizer used to apply gradient updates with</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.module.def_model_backward">
<code class="sig-prename descclassname">sparseml.pytorch.utils.module.</code><code class="sig-name descname">def_model_backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">losses</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">scaler</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>torch.cuda.amp.grad_scaler.GradScaler<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/module.html#def_model_backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.module.def_model_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Default function to perform a backwards pass for a model and the calculated losses
Calls backwards for the DEFAULT_LOSS_KEY in losses Dict</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – the model to run the backward for</p></li>
<li><p><strong>losses</strong> – the losses dictionary containing named tensors,
DEFAULT_LOSS_KEY is expected to exist and backwards is called on that</p></li>
<li><p><strong>scaler</strong> – GradScaler object for running in mixed precision with amp. If scaler
is not None will call scaler.scale on the loss object. Default is None</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.ssd_helpers">
<span id="sparseml-pytorch-utils-ssd-helpers-module"></span><h2>sparseml.pytorch.utils.ssd_helpers module<a class="headerlink" href="#module-sparseml.pytorch.utils.ssd_helpers" title="Permalink to this headline">¶</a></h2>
<p>Helper functions and classes for creating and training PyTorch SSD models</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.ssd_helpers.</code><code class="sig-name descname">DefaultBoxes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">feature_maps</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">steps</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">scales</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">aspect_ratios</span><span class="p">:</span> <span class="n">List<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">scale_xy</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">scale_wh</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.2</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#DefaultBoxes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convenience class for creating, representing, encoding, and decoding default boxes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_size</strong> – input image size</p></li>
<li><p><strong>feature_maps</strong> – list of feature map sizes</p></li>
<li><p><strong>steps</strong> – steps to use between boxes in a feature map</p></li>
<li><p><strong>scales</strong> – list of ranges of size scales to use for each feature map</p></li>
<li><p><strong>aspect_ratios</strong> – list of aspect ratios to construct boxes with</p></li>
<li><p><strong>scale_xy</strong> – parameter to scale box center by when encoding</p></li>
<li><p><strong>scale_wh</strong> – parameter to scale box dimensions by when encoding</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.as_ltrb">
<code class="sig-name descname">as_ltrb</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#DefaultBoxes.as_ltrb"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.as_ltrb" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The default boxes represented by this object in
top left, top right pixel representation</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.as_xywh">
<code class="sig-name descname">as_xywh</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#DefaultBoxes.as_xywh"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.as_xywh" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The default boxes represented by this object in
center pixel, width, height representation</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.decode_output_batch">
<code class="sig-name descname">decode_output_batch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">boxes</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">scores</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">score_threhsold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">iou_threshold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.45</span></em>, <em class="sig-param"><span class="n">max_detections</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">200</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#DefaultBoxes.decode_output_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.decode_output_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a batch detection model outputs from default box offsets and class
scores to ltrb formatted bounding boxes, predicted labels, and scores
for each image of the batch using non maximum suppression.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>boxes</strong> – Encoded default-box offsets. Expected shape:
batch_size,4,num_default_boxes</p></li>
<li><p><strong>scores</strong> – Class scores for each image, class, box combination.
Expected shape: batch_size,num_classes,num_default_boxes</p></li>
<li><p><strong>score_threhsold</strong> – minimum softmax score to be considered a positive
prediction. Default is 0.01 following the SSD paper</p></li>
<li><p><strong>iou_threshold</strong> – The minimum IoU between two boxes to be considered the
same object in non maximum suppression</p></li>
<li><p><strong>max_detections</strong> – the maximum number of detections to keep per image.
Default is 200</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Detected object boudning boxes, predicted labels, and class score for
each image in this batch</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.encode_image_box_labels">
<code class="sig-name descname">encode_image_box_labels</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">boxes</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.5</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#DefaultBoxes.encode_image_box_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.encode_image_box_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Given the bounding box and image annotations for a single image with N objects
will encode the box annotations as offsets to the default boxes and labels
to the associated default boxes based on the annotation boxes and default
boxes with an intersection over union (IoU) greater than the given threshold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>boxes</strong> – Bounding box annotations for objects in an image. Should have
shape N,4 and be represented in ltrb format</p></li>
<li><p><strong>labels</strong> – Label annotations for N objects in an image.</p></li>
<li><p><strong>threshold</strong> – The minimum IoU bounding boxes and default boxes should share
to be encoded</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple of the offset encoded bounding boxes and default box encoded
labels</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.num_default_boxes">
<em class="property">property </em><code class="sig-name descname">num_default_boxes</code><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.num_default_boxes" title="Permalink to this definition">¶</a></dt>
<dd><p>the number of default boxes this object defines</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.scale_wh">
<em class="property">property </em><code class="sig-name descname">scale_wh</code><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.scale_wh" title="Permalink to this definition">¶</a></dt>
<dd><p>parameter to scale box dimensions by when encoding</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.scale_xy">
<em class="property">property </em><code class="sig-name descname">scale_xy</code><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes.scale_xy" title="Permalink to this definition">¶</a></dt>
<dd><p>parameter to scale box center by when encoding</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.ssd_helpers.</code><code class="sig-name descname">MeanAveragePrecision</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">postprocessing_fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span>Any<span class="p">, </span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">iou_threshold</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">iou_steps</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.05</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#MeanAveragePrecision"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for computing the mean average precision of an object detection model output.
Inputs will be decoded by the provided post-processing function.
Each batch update tracks the cumulative ground truth objects of each class, and the
scores the model gives each class.</p>
<p>calculate_map object uses the aggregated results to find the mAP at the given
threshold(s)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>postprocessing_fn</strong> – function that takes in detection model output and returns
post-processed tuple of predicted bounding boxes, classification labels, and
scores</p></li>
<li><p><strong>iou_threshold</strong> – IoU thresholds to match predicted objects to ground truth
objects. Can provide a single IoU or a tuple of two representing a range.
mAP will be averaged over the range of values at each IoU</p></li>
<li><p><strong>iou_steps</strong> – the amount of IoU to shift between measurements between
iou_threshold values</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.batch_forward">
<code class="sig-name descname">batch_forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model_output</span><span class="p">:</span> <span class="n">Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">ground_truth_annotations</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#MeanAveragePrecision.batch_forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.batch_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes the model outputs using non maximum suppression, then stores the
number of ground truth objects per class, true positives, and true negatives
that can be used to calculate the overall mAP in the calculate_map function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_output</strong> – the predictions tuple containing [predicted_boxes,
predicted_labels] batch size should match length of ground_truth_annotations</p></li>
<li><p><strong>ground_truth_annotations</strong> – annotations from data loader to compare the
batch results to, should be</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.calculate_map">
<code class="sig-name descname">calculate_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_recall_levels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">11</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>float<span class="p">, </span>Dict<span class="p">[</span>float<span class="p">, </span>Dict<span class="p">[</span>int<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#MeanAveragePrecision.calculate_map"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.calculate_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates mAP at the given threshold values based on the results stored in
forward passes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_recall_levels</strong> – the number of recall levels to use between
0 and 1 inclusive. Defaults to 11, the VOC dataset standard</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple of the overall mAP, and a dictionary that maps threshold level
to class to average precision for that class</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#MeanAveragePrecision.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the ground truth class count and results dictionaries</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.get_recall_levels">
<em class="property">static </em><code class="sig-name descname">get_recall_levels</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_recall_levels</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">11</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#MeanAveragePrecision.get_recall_levels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.MeanAveragePrecision.get_recall_levels" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_recall_levels</strong> – the number of recall levels to use between
0 and 1 inclusive. Defaults to 11, the VOC dataset standard</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of evenly spaced recall levels between 0 and 1 inclusive with
num_recall_levels elements</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.ssd_helpers.get_default_boxes_300">
<code class="sig-prename descclassname">sparseml.pytorch.utils.ssd_helpers.</code><code class="sig-name descname">get_default_boxes_300</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">voc</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.utils.ssd_helpers.DefaultBoxes" title="sparseml.pytorch.utils.ssd_helpers.DefaultBoxes">sparseml.pytorch.utils.ssd_helpers.DefaultBoxes</a><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#get_default_boxes_300"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.get_default_boxes_300" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function for generating DefaultBoxes object for standard SSD300 model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>voc</strong> – set True if default boxes should be made for VOC dataset.
Will set scales to be slightly larger than for the default
COCO dataset configuration</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>DefaultBoxes object implemented for standard SSD300 models</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.ssd_helpers.ssd_random_crop">
<code class="sig-prename descclassname">sparseml.pytorch.utils.ssd_helpers.</code><code class="sig-name descname">ssd_random_crop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image</span><span class="p">:</span> <span class="n">PIL.Image.Image</span></em>, <em class="sig-param"><span class="n">boxes</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">labels</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>PIL.Image.Image<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/ssd_helpers.html#ssd_random_crop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.ssd_helpers.ssd_random_crop" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs one of the random SSD crops on a given image, bounding boxes,
and labels as implemented in the original paper.</p>
<div class="line-block">
<div class="line">Chooses between following 3 conditions:</div>
<div class="line-block">
<div class="line">1. Preserve the original image</div>
<div class="line">2. Random crop minimum IoU is among 0.1, 0.3, 0.5, 0.7, 0.9</div>
<div class="line">3. Random crop</div>
</div>
</div>
<p>Adapted from: <a class="reference external" href="https://github.com/chauhan-utk/src.DomainAdaptation">https://github.com/chauhan-utk/src.DomainAdaptation</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> – the image to potentially crop</p></li>
<li><p><strong>boxes</strong> – a tensor of bounding boxes in ltrb format with shape n_boxes,4</p></li>
<li><p><strong>labels</strong> – a tensor of labels for each of the bounding boxes</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cropped image, boxes, and labels</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils.yolo_helpers">
<span id="sparseml-pytorch-utils-yolo-helpers-module"></span><h2>sparseml.pytorch.utils.yolo_helpers module<a class="headerlink" href="#module-sparseml.pytorch.utils.yolo_helpers" title="Permalink to this headline">¶</a></h2>
<p>Helper functions and classes for creating and training PyTorch Yolo models</p>
<dl class="py class">
<dt id="sparseml.pytorch.utils.yolo_helpers.YoloGrids">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">YoloGrids</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">anchor_groups</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#YoloGrids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.YoloGrids" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Helper class to compute and store Yolo output and anchor box grids</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>anchor_groups</strong> – List of n,2 tensors of the Yolo model’s anchor points
for each output group. Defaults to yolo_v3_anchor_groups</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.utils.yolo_helpers.YoloGrids.get_anchor_grid">
<code class="sig-name descname">get_anchor_grid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">group_idx</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#YoloGrids.get_anchor_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.YoloGrids.get_anchor_grid" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>group_idx</strong> – Index of output group for this anchor grid</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>grid tensor of shape 1, num_anchors, 1, 1, 2</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.yolo_helpers.YoloGrids.get_grid">
<code class="sig-name descname">get_grid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">size_x</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">size_y</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#YoloGrids.get_grid"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.YoloGrids.get_grid" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size_x</strong> – grid size x</p></li>
<li><p><strong>size_y</strong> – grid size y</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Yolo output box grid for size x,y to be used for model output decoding.
will have shape (1, 1, size_y, size_x, 2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.utils.yolo_helpers.YoloGrids.num_anchor_grids">
<code class="sig-name descname">num_anchor_grids</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#YoloGrids.num_anchor_grids"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.YoloGrids.num_anchor_grids" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The number of anchor grids available (number of yolo model outputs)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.yolo_helpers.box_giou">
<code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">box_giou</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">boxes_a</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">boxes_b</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#box_giou"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.box_giou" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>boxes_a</strong> – 4,N Tensor of xywh bounding boxes</p></li>
<li><p><strong>boxes_b</strong> – 4,N Tensor of xywh bounding boxes</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Shape N Tensor of GIoU values between boxes in the input tensors</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.yolo_helpers.build_targets">
<code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">build_targets</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">targets</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">anchors_groups</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">grid_shapes</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">iou_threshold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.2</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">, </span>List<span class="p">[</span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#build_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.build_targets" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a representation of the image targets according to the given
anchor groups and grid shapes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>targets</strong> – Yolo data targets tensor of shape n,6 with columns image number,
class, center_x, center_y, width, height</p></li>
<li><p><strong>anchors_groups</strong> – List of n,2 Tensors of anchor point coordinates for
each of the Yolo model’s detectors</p></li>
<li><p><strong>grid_shapes</strong> – List of n,2 Tensors of the Yolo models output grid shapes
for a particular input shape</p></li>
<li><p><strong>iou_threshold</strong> – the minimum IoU value to consider an object box to match
to an anchor point. Default is 0.2</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.yolo_helpers.get_output_grid_shapes">
<code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">get_output_grid_shapes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">outputs</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#get_output_grid_shapes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.get_output_grid_shapes" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> – List of Yolo model outputs</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of the grid dimensions for each of the Yolo outputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.yolo_helpers.postprocess_yolo">
<code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">postprocess_yolo</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">preds</span><span class="p">:</span> <span class="n">List<span class="p">[</span>torch.Tensor<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">input_shape</span><span class="p">:</span> <span class="n">Iterable<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">yolo_grids</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="#sparseml.pytorch.utils.yolo_helpers.YoloGrids" title="sparseml.pytorch.utils.yolo_helpers.YoloGrids">sparseml.pytorch.utils.yolo_helpers.YoloGrids</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">confidence_threshold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">iou_threshold</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.6</span></em>, <em class="sig-param"><span class="n">max_detections</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">300</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>Tuple<span class="p">[</span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#postprocess_yolo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.postprocess_yolo" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode the outputs of a Yolo model and perform non maximum suppression
on the predicted boxes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preds</strong> – list of Yolo model output tensors</p></li>
<li><p><strong>input_shape</strong> – shape of input image to model. Default is [640, 640]</p></li>
<li><p><strong>yolo_grids</strong> – optional YoloGrids object for caching previously used grid shapes</p></li>
<li><p><strong>confidence_threshold</strong> – minimum confidence score for a prediction to be
considered a detection. Default is 0.1</p></li>
<li><p><strong>iou_threshold</strong> – IoU threshold for non maximum suppression. Default is 0.6</p></li>
<li><p><strong>max_detections</strong> – maximum number of detections after nms. Default is 300</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of predicted bounding boxes (n,4), labels, and scores for each output
in the batch</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.utils.yolo_helpers.yolo_v3_anchor_groups">
<code class="sig-prename descclassname">sparseml.pytorch.utils.yolo_helpers.</code><code class="sig-name descname">yolo_v3_anchor_groups</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/utils/yolo_helpers.html#yolo_v3_anchor_groups"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.utils.yolo_helpers.yolo_v3_anchor_groups" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of the default anchor coordinate groups for Yolo V3 outputs</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.utils">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sparseml.pytorch.utils" title="Permalink to this headline">¶</a></h2>
<p>Generic code used as utilities and helpers for PyTorch</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="sparseml.tensorflow_v1.html" class="btn btn-neutral float-right" title="sparseml.tensorflow_v1 package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="sparseml.pytorch.optim.quantization.html" class="btn btn-neutral float-left" title="sparseml.pytorch.optim.quantization package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>