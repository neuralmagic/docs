

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>sparseml.tensorflow_v1.optim package &mdash; SparseML 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sparseml.tensorflow_v1.utils package" href="sparseml.tensorflow_v1.utils.html" />
    <link rel="prev" title="sparseml.tensorflow_v1.nn package" href="sparseml.tensorflow_v1.nn.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> SparseML
          

          
            
            <img src="../_static/icon-sparseml.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes.html">Sparsification Recipes</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="sparseml.html">sparseml package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sparseml.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sparseml.keras.html">sparseml.keras package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.onnx.html">sparseml.onnx package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.optim.html">sparseml.optim package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.pytorch.html">sparseml.pytorch package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="sparseml.tensorflow_v1.html">sparseml.tensorflow_v1 package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="sparseml.tensorflow_v1.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.tensorflow_v1.html#module-sparseml.tensorflow_v1">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.utils.html">sparseml.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.log">sparseml.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml">Module contents</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Help</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">Bugs, Feature Requests</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/discussions">Support, General Q&amp;A</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.neuralmagic.com">Neural Magic Docs</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseML</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="sparseml.html">sparseml package</a> &raquo;</li>
        
          <li><a href="sparseml.tensorflow_v1.html">sparseml.tensorflow_v1 package</a> &raquo;</li>
        
      <li>sparseml.tensorflow_v1.optim package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api/sparseml.tensorflow_v1.optim.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sparseml-tensorflow-v1-optim-package">
<h1>sparseml.tensorflow_v1.optim package<a class="headerlink" href="#sparseml-tensorflow-v1-optim-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.analyzer_module">
<span id="sparseml-tensorflow-v1-optim-analyzer-module-module"></span><h2>sparseml.tensorflow_v1.optim.analyzer_module module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.analyzer_module" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.analyzer_module.analyze_module">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.analyzer_module.</code><code class="sig-name descname">analyze_module</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">session</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.client.session.Session<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">op_names</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">op_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/analyzer_module.html#analyze_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.analyzer_module.analyze_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Analyze a module at certain layers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>session</strong> – running session encapsulating the analyzed module</p></li>
<li><p><strong>graph</strong> – graph of the module; if None then the session is required,
and the encapsulated graph is to be analyzed</p></li>
<li><p><strong>op_names</strong> – list of names of layers to be analyzed;
if None then all layers are analyzed for an aggregated result</p></li>
<li><p><strong>op_types</strong> – the operation types that will be analyzed, default (Conv2D, MatMul)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analyzed layer descriptions or the module description if no op_names</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.manager">
<span id="sparseml-tensorflow-v1-optim-manager-module"></span><h2>sparseml.tensorflow_v1.optim.manager module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.manager" title="Permalink to this headline">¶</a></h2>
<p>Contains base code related to modifier managers: modifier managers handle
grouping modifiers and running them together.
Also handles loading modifiers from yaml files</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.manager.</code><code class="sig-name descname">ScheduledModifierManager</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">modifiers</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.manager.BaseManager" title="sparseml.optim.manager.BaseManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.manager.BaseManager</span></code></a>, <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.Modifier" title="sparseml.tensorflow_v1.optim.modifier.Modifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.Modifier</span></code></a></p>
<p>The base modifier manager, handles managing multiple ScheduledModifier.</p>
<div class="line-block">
<div class="line">Modifiers are expected to implement up to 3 different functions for TensorFlow:</div>
<div class="line-block">
<div class="line">- create_ops - inject ops into the graph before the training begins</div>
<div class="line">- create_extras - create extras like learning rate controls before training</div>
<div class="line">- complete_graph - finalize the graph after training has completed</div>
<div class="line"><br /></div>
</div>
<div class="line">Life cycle:</div>
<div class="line-block">
<div class="line">- create model graph</div>
<div class="line">- manager.create_ops()</div>
<div class="line">- manager.create_extras()</div>
<div class="line">- train graph</div>
<div class="line">- manager.complete_graph()</div>
<div class="line">- export graph</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modifiers</strong> – the modifiers to wrap</p>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.RECAL_UPDATE">
<code class="sig-name descname">RECAL_UPDATE</code><em class="property"> = 'recal_update'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.RECAL_UPDATE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.complete_graph">
<code class="sig-name descname">complete_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.client.session.Session<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager.complete_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.complete_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Complete modifying the graph. Should be called after modifying is complete.
Cleans up any ops that should be removed or reordered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the modified graph that should be completed and cleaned.
if not supplied, then will use the default graph</p></li>
<li><p><strong>sess</strong> – the session to use for completing the modified graph.
if not supplied, then will use the default session</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cleaned graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create modifying operations and tensors in the graph.</p>
<div class="line-block">
<div class="line">Returns a tuple containing:</div>
<div class="line-block">
<div class="line">- modifying ops that should be run in a session on each global step.</div>
<div class="line">- named extras (ops / tensors) created in the graph that can be used</div>
<div class="line-block">
<div class="line">by other ops such as a learning rate for the optimizer</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training.
if not supplied, then will use get_or_create_global_step()</p></li>
<li><p><strong>graph</strong> – the graph to be modified,
if not supplied, then will use the default graph</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of ops, dict of named ops / tensors)
to be run or used for modifying the training process</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.from_yaml">
<em class="property">static </em><code class="sig-name descname">from_yaml</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">file_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>sparsezoo.objects.optimization_recipe.OptimizationRecipe<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">add_modifiers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.Modifier" title="sparseml.tensorflow_v1.optim.modifier.Modifier">sparseml.tensorflow_v1.optim.modifier.Modifier</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager.from_yaml"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.from_yaml" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function used to create the manager of multiple modifiers from a
recipe file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> – the path to the recipe file to load the modifier from, or
a SparseZoo model stub to load a recipe for a model stored in SparseZoo.
SparseZoo stubs should be preceded by ‘zoo:’, and can contain an optional
‘?recipe_type=&lt;type&gt;’ parameter. Can also be a SparseZoo OptimizationRecipe
object. i.e. ‘/path/to/local/recipe.yaml’, ‘zoo:model/stub/path’,
‘zoo:model/stub/path?recipe_type=transfer’</p></li>
<li><p><strong>add_modifiers</strong> – additional modifiers that should be added to the
returned manager alongside the ones loaded from the recipe file</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ScheduledModifierManager() created from the recipe file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.initialize_session">
<code class="sig-name descname">initialize_session</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.client.session.Session<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager.initialize_session"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.initialize_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize any state for a session such as variables.
This is an optional call, only needed if global_variables_initializer
is not used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sess</strong> – the session to use for initializing</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.modifiers_to_string_lines">
<code class="sig-name descname">modifiers_to_string_lines</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">modifiers</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseScheduled" title="sparseml.optim.modifier.BaseScheduled">sparseml.optim.modifier.BaseScheduled</a><span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>str<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/manager.html#ScheduledModifierManager.modifiers_to_string_lines"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.manager.ScheduledModifierManager.modifiers_to_string_lines" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modifiers</strong> – ignored and overwritten with the original
(non grouped) modifiers</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of lines for a string / yaml representation of the
modifiers in the manager</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.mask_creator_pruning">
<span id="sparseml-tensorflow-v1-optim-mask-creator-pruning-module"></span><h2>sparseml.tensorflow_v1.optim.mask_creator_pruning module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.mask_creator_pruning" title="Permalink to this headline">¶</a></h2>
<p>Classes for defining sparsity masks based on model parameters.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.BlockPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">BlockPruningMaskCreator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">block_shape</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">grouping_op_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#BlockPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.BlockPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups the input tensor into blocks of
shape block_shape.
block_shape must divide the shape of any input tensor evenly and must have exactly
2 elements for the shape of in and out channels in the blocks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>block_shape</strong> – The shape of blocks to strucure blocks of in and out channels
in the mask by.  -1 represents blocking along the entire dimension.</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor">
<code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#BlockPruningMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The absolute mean values of the tensor grouped by blocks of
shape self._block_shape</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.DimensionPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">DimensionPruningMaskCreator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">grouping_op_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#DimensionPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.DimensionPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups sparsity blocks by the given
dimension(s)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> – The index or list of indices of dimensions to group the mask by or
the type of dims to prune ([‘channel’, ‘filter’])</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.DimensionPruningMaskCreator.group_tensor">
<code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#DimensionPruningMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.DimensionPruningMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The absolute mean values of the tensor grouped by the
dimension(s) in self._dim</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">GroupedPruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#GroupedPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator</span></code></a></p>
<p>Abstract class for a sparsity mask creator that structures masks according to
grouping functions.  Subclasses should implement group_tensor and
_map_mask_to_tensor</p>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask">
<code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – A tensor of a model layer’s weights</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A sparsity mask close to the set sparsity based on the values of
the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_grouping_op">
<em class="property">static </em><code class="sig-name descname">get_grouping_op</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">grouping_op_name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Operation<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.get_grouping_op"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_grouping_op" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grouping_op_name</strong> – name of grouping operation to get tf operation for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tf operation for grouping_op_name if available, raises error otherwise</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_mask_initializer">
<code class="sig-name descname">get_mask_initializer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; Callable<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.get_mask_initializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_mask_initializer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – A tensor of a model layer’s weights</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor initializer function for this sparsity mask</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor">
<em class="property">abstract </em><code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to reduce in groups</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The grouped tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">PruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#PruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base abstract class for a sparsity mask creator.
Subclasses should define all methods for creating masks and their initializers</p>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask">
<em class="property">abstract </em><code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – A tensor of a model layer’s weights</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A sparsity mask close to the set sparsity based on the values of
the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator.get_mask_initializer">
<em class="property">abstract </em><code class="sig-name descname">get_mask_initializer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; Callable<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#PruningMaskCreator.get_mask_initializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator.get_mask_initializer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – A tensor of a model layer’s weights</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor initializer function for this sparsity mask</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">UnstructuredPruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</span></code></a></p>
<p>Class for creating unstructured sparsity masks.
Masks will be created using unstructured sparsity by pruning weights ranked
by their magnitude.</p>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask">
<code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – A tensor of a model layer’s weights</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A sparsity mask close to the set sparsity based on the values of
the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.get_mask_initializer">
<code class="sig-name descname">get_mask_initializer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em><span class="sig-paren">)</span> &#x2192; Callable<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.get_mask_initializer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.get_mask_initializer" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – A tensor of a model layer’s weights</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Initializer for tensor where an element is 1.0 for nonzero weights
and zero for all other weights</p>
</dd>
<dt class="field-odd">Raise</dt>
<dd class="field-odd"><p>ValueError If the dtype is not numeric or boolean</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_creator_pruning.load_mask_creator">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_creator_pruning.</code><code class="sig-name descname">load_mask_creator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>Iterable<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_creator_pruning.html#load_mask_creator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.load_mask_creator" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> – Formatted string or iterable of block_shape specifying
SparsityMaskCreator object to return</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>SparsityMaskCreator object created from obj</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.mask_pruning">
<span id="sparseml-tensorflow-v1-optim-mask-pruning-module"></span><h2>sparseml.tensorflow_v1.optim.mask_pruning module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.mask_pruning" title="Permalink to this headline">¶</a></h2>
<p>Code related to applying a mask onto a variable to impose kernel sparsity,
aka model pruning, on a TensorFlow graph.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">PruningOpVars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span></em>, <em class="sig-param"><span class="n">op_input</span></em>, <em class="sig-param"><span class="n">update</span></em>, <em class="sig-param"><span class="n">mask</span></em>, <em class="sig-param"><span class="n">masked</span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.mask">
<em class="property">property </em><code class="sig-name descname">mask</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.mask" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.masked">
<em class="property">property </em><code class="sig-name descname">masked</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.masked" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.op">
<em class="property">property </em><code class="sig-name descname">op</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.op" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.op_input">
<em class="property">property </em><code class="sig-name descname">op_input</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.op_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.update">
<em class="property">property </em><code class="sig-name descname">update</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">PruningScope</code><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#PruningScope"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Convenience class for dealing with scope and names for kernel sparsity
in the tf graph.</p>
<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.NM_KS">
<code class="sig-name descname">NM_KS</code><em class="property"> = 'nm_ks'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.NM_KS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.NM_KS_OPS">
<code class="sig-name descname">NM_KS_OPS</code><em class="property"> = 'nm_ks_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.NM_KS_OPS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS">
<code class="sig-name descname">OPS</code><em class="property"> = 'ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_INPUT">
<code class="sig-name descname">OPS_INPUT</code><em class="property"> = 'input_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_INPUT" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SCHEDULE">
<code class="sig-name descname">OPS_SCHEDULE</code><em class="property"> = 'schedule_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SCHEDULE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SPARSITY">
<code class="sig-name descname">OPS_SPARSITY</code><em class="property"> = 'sparsity_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SPARSITY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SUMMARY">
<code class="sig-name descname">OPS_SUMMARY</code><em class="property"> = 'summary_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_SUMMARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_UPDATE">
<code class="sig-name descname">OPS_UPDATE</code><em class="property"> = 'update_ops'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OPS_UPDATE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_COND_UPDATE">
<code class="sig-name descname">OP_COND_UPDATE</code><em class="property"> = 'nm_conditional_update'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_COND_UPDATE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASKED_VAR">
<code class="sig-name descname">OP_MASKED_VAR</code><em class="property"> = 'nm_masked_var'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASKED_VAR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_ASSIGN">
<code class="sig-name descname">OP_MASK_ASSIGN</code><em class="property"> = 'nm_mask_assign'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_ASSIGN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_UPDATE">
<code class="sig-name descname">OP_MASK_UPDATE</code><em class="property"> = 'nm_mask_update'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_UPDATE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_UPDATE_NO_OP">
<code class="sig-name descname">OP_MASK_UPDATE_NO_OP</code><em class="property"> = 'nm_mask_update_no_op'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_MASK_UPDATE_NO_OP" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_PRUNE_VARS_ASSIGN">
<code class="sig-name descname">OP_PRUNE_VARS_ASSIGN</code><em class="property"> = 'nm_prune_vars_assign'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_PRUNE_VARS_ASSIGN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_SAVE">
<code class="sig-name descname">OP_SAVE</code><em class="property"> = 'nm_save'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_SAVE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_SPARSITY">
<code class="sig-name descname">OP_SPARSITY</code><em class="property"> = 'nm_sparsity'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_SPARSITY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_UPDATE_READY">
<code class="sig-name descname">OP_UPDATE_READY</code><em class="property"> = 'nm_update_ready'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_UPDATE_READY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_WEIGHT_UPDATE">
<code class="sig-name descname">OP_WEIGHT_UPDATE</code><em class="property"> = 'nm_weight_update'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.OP_WEIGHT_UPDATE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.VAR_MASK">
<code class="sig-name descname">VAR_MASK</code><em class="property"> = 'nm_mask'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.VAR_MASK" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.VAR_THRESHOLD">
<code class="sig-name descname">VAR_THRESHOLD</code><em class="property"> = 'nm_threshold'</em><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.VAR_THRESHOLD" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.collection_name">
<em class="property">static </em><code class="sig-name descname">collection_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#PruningScope.collection_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.collection_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a predictable name for a given variable / op in a group for lookup /
storage in a collection</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ks_group</strong> – the group identifier the name belongs under</p></li>
<li><p><strong>name</strong> – the name of the op or variable to be stored or retrieved</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the formatted name for use in a collection</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.general">
<em class="property">static </em><code class="sig-name descname">general</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">additional</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trailing_slash</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#PruningScope.general"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.general" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a general kernel sparsity scope in the tf graph.
Use cases are for generic ops like target sparsity, conditional updates, etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>additional</strong> – any additional scope that should be added to the end</p></li>
<li><p><strong>trailing_slash</strong> – include a trailing forward slash if True, else False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the proper scope</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.model">
<em class="property">static </em><code class="sig-name descname">model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_tens</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">additional</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trailing_slash</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#PruningScope.model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningScope.model" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a model specific kernel sparsity scope in the tf graph.
Use cases are for the specific mask, threshold, etc variables
to induce sparsity along with the ops to update those vars.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op_tens</strong> – the op tensor to create the scope for</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>additional</strong> – any additional scope that should be added to the end</p></li>
<li><p><strong>trailing_slash</strong> – include a trailing forward slash if True, else False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the proper scope</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.apply_op_vars_masks">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">apply_op_vars_masks</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pruning_op_vars</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#apply_op_vars_masks"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.apply_op_vars_masks" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the masks to the original ops input var so that it can be saved
with the desired sparsity for later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pruning_op_vars</strong> – the list of named tuples containing the sparse mask
and the op variable to apply the sparse mask to</p></li>
<li><p><strong>ks_group</strong> – the group to create the assign ops under</p></li>
<li><p><strong>sess</strong> – the session to use to run the assign</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.create_graph_ops_pruning">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">create_graph_ops_pruning</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">update_ready</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">is_after_end_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">mask_creator</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#create_graph_ops_pruning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.create_graph_ops_pruning" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the necessary variables and operators to gradually
apply sparsity to a given list of operators in a graph.</p>
<p>Handles setting a mask on an operator to the given sparsity.
Sets the mask based on pruning away the lowest absolute magnitude weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the tf graph to pull the operator out of for applying the pruning to</p></li>
<li><p><strong>var_names</strong> – the names or regex patterns of names of variables to prune in the
graph to the given sparsity</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
<li><p><strong>update_ready</strong> – the tensor where if true will update the mask from sparsity,
if false will not update the mask</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking</p></li>
<li><p><strong>is_after_end_step</strong> – tensor that is true if the current global step
is after end_epoch</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>mask_creator</strong> – optional object to define sparisty mask creation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of the created named tuples each containing the
assignment op, mask variable, threshold tensor, and masked tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.create_ks_schedule_ops">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">create_ks_schedule_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.ops.variables.VariableV1</span></em>, <em class="sig-param"><span class="n">begin_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">update_step_freq</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">init_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">final_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">exponent</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#create_ks_schedule_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.create_ks_schedule_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a gradual schedule for model pruning (kernel sparsity).
Creates a sparsity tensor that goes from init_sparsity til final_sparsity
starting at begin_step and ending at end_step.
Uses the global_step to map those.
Additionally creates an update_ready tensor that is True if an update
to the sparsity tensor should be run, False otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_step</strong> – the global optimizer step for the training graph</p></li>
<li><p><strong>begin_step</strong> – the global step to begin pruning at</p></li>
<li><p><strong>end_step</strong> – the global step to end pruning at</p></li>
<li><p><strong>update_step_freq</strong> – the number of global steps between each weight update</p></li>
<li><p><strong>init_sparsity</strong> – the starting value for sparsity of a
weight tensor to be enforce</p></li>
<li><p><strong>final_sparsity</strong> – the end value for sparsity for a weight tensor to be enforce</p></li>
<li><p><strong>exponent</strong> – the exponent to use for interpolating between
init_sparsity and final_sparsity higher values will lead to larger sparsity
steps at the beginning vs the end ie: linear (1) vs cubic (3)</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the signal for update_ready and the target sparsity</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.create_ks_scheduled_constant_graph_ops">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">create_ks_scheduled_constant_graph_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.ops.variables.VariableV1</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">begin_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#create_ks_scheduled_constant_graph_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.create_ks_scheduled_constant_graph_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates constant model pruning ops.  Does not modify the graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the tf graph to pull the operator out of for applying the pruning to</p></li>
<li><p><strong>global_step</strong> – the global optimizer step for the training graph</p></li>
<li><p><strong>var_names</strong> – a list of names or regex patterns to create constant ops
for within the graph</p></li>
<li><p><strong>begin_step</strong> – the global step to begin pruning at</p></li>
<li><p><strong>end_step</strong> – the global step to end pruning at</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the update operation to run in a session,
a list of the pruning ops and vars for each desired op in the graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.create_op_pruning">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">create_op_pruning</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Operation</span></em>, <em class="sig-param"><span class="n">op_input</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">update_ready</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">is_after_end_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">mask_creator</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#create_op_pruning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.create_op_pruning" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the necessary variables and operators to gradually
apply sparsity to an operators variable.</p>
<p>Handles setting a mask on an operator to the given sparsity.
Sets the mask based on pruning away the lowest absolute magnitude weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op</strong> – the operation to prune to the given sparsity</p></li>
<li><p><strong>op_input</strong> – the variable of the parameter within op to prune</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
<li><p><strong>update_ready</strong> – the tensor where if true will update the mask from sparsity,
if false will not update the mask</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking</p></li>
<li><p><strong>is_after_end_step</strong> – tensor that is true if the current global step
is after end_epoch</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>mask_creator</strong> – object to define sparisty mask creation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a named tuple containing the assignment op, mask variable,
threshold tensor, and masked tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.create_summaries_pruning">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">create_summaries_pruning</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pruning_op_vars</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#create_summaries_pruning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.create_summaries_pruning" title="Permalink to this definition">¶</a></dt>
<dd><p>Create TensorBoard summary ops in the current graph for the
given list of PruningOpVars.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pruning_op_vars</strong> – the list of named tuples containing the masked input to the
pruned op to record sparsity for in TensorBoard.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the created summaries for the pruned op vars</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_graph_ops_pruning">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">get_or_create_graph_ops_pruning</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">update_ready</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">is_after_end_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">mask_creator</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#get_or_create_graph_ops_pruning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_graph_ops_pruning" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates or retrieves (if previously created) the necessary variables
and operators to gradually apply sparsity to a given list of operators in a graph.</p>
<p>Handles setting a mask on an operator to the given sparsity.
Sets the mask based on pruning away the lowest absolute magnitude weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the tf graph to pull the operator out of for applying the pruning to</p></li>
<li><p><strong>var_names</strong> – the names or regex patterns of names of variables to prune in the
graph to the given sparsity</p></li>
<li><p><strong>sparsity</strong> – the target sparsity to use for assigning the masks</p></li>
<li><p><strong>update_ready</strong> – the tensor where if true will update the mask from sparsity,
if false will not update the mask</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking</p></li>
<li><p><strong>is_after_end_step</strong> – tensor that is true if the current global step
is after end_epoch</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>mask_creator</strong> – optional object to define sparisty mask creation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of the created or retrieved named tuples each containing the
assignment op, mask variable, threshold tensor, and masked tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_ks_schedule_ops">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">get_or_create_ks_schedule_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">begin_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">update_step_freq</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">init_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">final_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">exponent</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#get_or_create_ks_schedule_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_ks_schedule_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates or retrieves (if previously created) a gradual schedule
for model pruning (kernel sparsity).
Creates a sparsity tensor that goes from init_sparsity til final_sparsity
starting at begin_step and ending at end_step.
Uses the global_step to map those.
Additionally creates an update_ready tensor that is True if an update
to the sparsity tensor should be run, False otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_step</strong> – the global optimizer step for the training graph</p></li>
<li><p><strong>begin_step</strong> – the global step to begin pruning at</p></li>
<li><p><strong>end_step</strong> – the global step to end pruning at</p></li>
<li><p><strong>update_step_freq</strong> – the number of global steps between each weight update</p></li>
<li><p><strong>init_sparsity</strong> – the starting value for sparsity of a
weight tensor to be enforce</p></li>
<li><p><strong>final_sparsity</strong> – the end value for sparsity for a weight tensor to be enforce</p></li>
<li><p><strong>exponent</strong> – the exponent to use for interpolating between
init_sparsity and final_sparsity higher values will lead to larger sparsity
steps at the beginning vs the end ie: linear (1) vs cubic (3)</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the signal for update_ready and the target sparsity</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_ks_scheduled_graph_ops">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.mask_pruning.</code><code class="sig-name descname">get_or_create_ks_scheduled_graph_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.ops.variables.VariableV1</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">begin_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">update_step_freq</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">init_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">final_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">exponent</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">ks_group</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">mask_creator</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a></span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars" title="sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars">sparseml.tensorflow_v1.optim.mask_pruning.PruningOpVars</a><span class="p">]</span><span class="p">, </span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/mask_pruning.html#get_or_create_ks_scheduled_graph_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.mask_pruning.get_or_create_ks_scheduled_graph_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets or creates model pruning (kernel sparsity) ops and vars in the graph
to be applied over a specific schedule.
Creates them for the var_names in the graph such that they follow a schedule
from begin_step to end_step starting at init_sparsity and ending at final_sparsity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the tf graph to pull the operator out of for applying the pruning to</p></li>
<li><p><strong>global_step</strong> – the global optimizer step for the training graph</p></li>
<li><p><strong>var_names</strong> – the names or regex patterns of names of variables to prune in the
graph</p></li>
<li><p><strong>begin_step</strong> – the global step to begin pruning at</p></li>
<li><p><strong>end_step</strong> – the global step to end pruning at</p></li>
<li><p><strong>update_step_freq</strong> – the number of global steps between each weight update</p></li>
<li><p><strong>init_sparsity</strong> – the starting value for sparsity of a
weight tensor to be enforce</p></li>
<li><p><strong>final_sparsity</strong> – the end value for sparsity for a weight tensor to be enforce</p></li>
<li><p><strong>exponent</strong> – the exponent to use for interpolating between
init_sparsity and final_sparsity higher values will lead to larger sparsity
steps at the beginning vs the end ie: linear (1) vs cubic (3)</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking</p></li>
<li><p><strong>ks_group</strong> – the group identifier the scope should be created under</p></li>
<li><p><strong>mask_creator</strong> – optional object to define sparisty mask creation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing the update operation to run in a session,
a list of the pruning ops and vars for each desired op in the graph,
the tensor containing the update_ready signal for the pruning ops,
the tensor containing the set sparsity for the pruning ops</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.modifier">
<span id="sparseml-tensorflow-v1-optim-modifier-module"></span><h2>sparseml.tensorflow_v1.optim.modifier module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.modifier" title="Permalink to this headline">¶</a></h2>
<p>Contains base code related to modifiers: objects that modify some aspect
of the training process for a model.
For example, learning rate schedules or kernel sparsity (weight pruning)
are implemented as modifiers.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">Modifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseModifier" title="sparseml.optim.modifier.BaseModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseModifier</span></code></a></p>
<p>Base modifier class that all TensorFlow modifiers should derive themselves from.
Handles setting up the expected contracts for modifying graphs, ops, and extras.</p>
<div class="line-block">
<div class="line">Modifiers are expected to implement up to 3 different functions for TensorFlow:</div>
<div class="line-block">
<div class="line">- create_ops - inject ops into the graph before the training begins</div>
<div class="line">- create_extras - create extras like learning rate controls before training</div>
<div class="line">- complete_graph - finalize the graph after training has completed</div>
<div class="line"><br /></div>
</div>
<div class="line">Life cycle:</div>
<div class="line-block">
<div class="line">- create model graph</div>
<div class="line">- manager.create_ops()</div>
<div class="line">- manager.create_extras()</div>
<div class="line">- train graph</div>
<div class="line">- manager.complete_graph()</div>
<div class="line">- export graph</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – the loggers that can be used by the modifier instance</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.complete_graph">
<code class="sig-name descname">complete_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.complete_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.complete_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Complete modifying the graph. Should be called after modifying is complete.
Cleans up any ops that should be removed or reordered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the modified graph that should be completed and cleaned</p></li>
<li><p><strong>sess</strong> – the session to use for completing the modified graph</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cleaned graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create modifying operations and tensors in the graph.</p>
<div class="line-block">
<div class="line">Returns a tuple containing:</div>
<div class="line-block">
<div class="line">- modifying ops that should be run in a session on each global step.</div>
<div class="line">- named extras (ops / tensors) created in the graph that can be used</div>
<div class="line-block">
<div class="line">by other ops such as a learning rate for the optimizer</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of ops, dict of named ops / tensors)
to be run or used for modifying the training process</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.get_group">
<code class="sig-name descname">get_group</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Any<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.get_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.get_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to be override by a subclass indicating the modifier container
into which the subclass should be combined
As an example, the two learning rate modifier classes SetLearningRateModifier
and LearningRateModifier return GroupLearningRateModifier, meaning that
a sequence of those LR modifier instances are grouped into the
GroupLearningRateModifier, which is where the final learning rate is computed</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.initialize_session">
<code class="sig-name descname">initialize_session</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.initialize_session"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.initialize_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize any state for a session such as variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sess</strong> – the session to use for initializing</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.load_list">
<em class="property">static </em><code class="sig-name descname">load_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">yaml_str</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.load_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.load_list" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>yaml_str</strong> – a string representation of the yaml syntax to
load modifiers from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the loaded modifiers list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.load_obj">
<em class="property">static </em><code class="sig-name descname">load_obj</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">yaml_str</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.load_obj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.load_obj" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>yaml_str</strong> – a string representation of the yaml syntax to
load a modifier from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the loaded modifier object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.Modifier.modify_estimator">
<code class="sig-name descname">modify_estimator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">estimator</span><span class="p">:</span> <span class="n">tensorflow_estimator.python.estimator.estimator.Estimator</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#Modifier.modify_estimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.Modifier.modify_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Modify a tf Estimator. Overrides the model_fn so that on invocation
it creates the original graph and then calls into create_ops.
Additionally will recreate the Scaffold with a new Save instance
to save all variables in the modified graph.</p>
<p>Note, learning_rate and other specific tensors that needed to be
retrieved from the extras in create_ops and passed to another implementation
will not work with this flow.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> – the tf Estimator to modify</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps per training epoch</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">ModifierProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">serializable</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">restrict_initialized</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">restrict_enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">restrict_extras</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_serialize_val</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Any<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">func_get</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">func_set</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">doc</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseProp</span></code></a></p>
<p>Property used to decorate a modifier.
Use for creating getters and setters in a modifier.
Handles making sure props cannot be changed after a certain point;
ex after initialized.
Also, marks the properties so they can be easily collected and serialized later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>serializable</strong> – True if the property should be serialized (ex in yaml),
False otherwise. Default True</p></li>
<li><p><strong>restrict_initialized</strong> – True to keep the property from being set after
initialized, False otherwise. Default True</p></li>
<li><p><strong>restrict_enabled</strong> – True to keep the property from being set after enabled,
False otherwise. Default False</p></li>
<li><p><strong>restrict_extras</strong> – extra attributes to check, if any are truthy then keep
from being set. Default None</p></li>
<li><p><strong>no_serialize_val</strong> – If prop is equal to this value, will not serialize the prop</p></li>
<li><p><strong>func_get</strong> – The function getter</p></li>
<li><p><strong>func_set</strong> – The function setter</p></li>
<li><p><strong>doc</strong> – The docs function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp.getter">
<code class="sig-name descname">getter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func_get</span><span class="p">:</span> <span class="n">Callable</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">sparseml.optim.modifier.BaseProp</a><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.getter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp.getter" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a ModifierProp based off the current instance with the getter function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func_get</strong> – the getter function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the recreated instance with the new getter function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp.no_serialize_val">
<em class="property">property </em><code class="sig-name descname">no_serialize_val</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp.no_serialize_val" title="Permalink to this definition">¶</a></dt>
<dd><p>a value that if the prop is equal to, will not serialize the prop</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp.restrictions">
<em class="property">property </em><code class="sig-name descname">restrictions</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp.restrictions" title="Permalink to this definition">¶</a></dt>
<dd><p>The attributes to check for restricting when the attribute can be set</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp.serializable">
<em class="property">property </em><code class="sig-name descname">serializable</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp.serializable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the property should be serialized (ex in yaml), False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierProp.setter">
<code class="sig-name descname">setter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func_set</span><span class="p">:</span> <span class="n">Callable</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">sparseml.optim.modifier.BaseProp</a><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.setter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierProp.setter" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a ModifierProp based off the current instance with the setter function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func_set</strong> – the setter function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the recreated instance with the new setter function</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierSessionRunHook">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">ModifierSessionRunHook</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">mod_ops</span><span class="p">:</span> <span class="n">List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ModifierSessionRunHook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierSessionRunHook" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.training.session_run_hook.SessionRunHook</span></code></p>
<p>A session run hook for the tf Estimator flow.
Used to integrate so that any extra ops for modifying the graph
can be executed each on each step of the estimator training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>modifier</strong> – the modifier to run the hook for</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps (or batches) taken per epoch</p></li>
<li><p><strong>mod_ops</strong> – the ops returned from calling create_ops on the modifier</p></li>
<li><p><strong>mod_extras</strong> – the extras returned from calling create_ops on the modifier</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ModifierSessionRunHook.after_run">
<code class="sig-name descname">after_run</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">run_context</span></em>, <em class="sig-param"><span class="n">run_values</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ModifierSessionRunHook.after_run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ModifierSessionRunHook.after_run" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by the estimator after each call to run()</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>run_context</strong> – run_context passed in during training</p></li>
<li><p><strong>run_values</strong> – a SessionRunValues object passed in during training</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">ScheduledModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_start</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_end</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_comparator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ScheduledModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.Modifier" title="sparseml.tensorflow_v1.optim.modifier.Modifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.Modifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseScheduled" title="sparseml.optim.modifier.BaseScheduled"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseScheduled</span></code></a></p>
<p>The base scheduled update modifier implementation, all scheduled modifiers should
inherit from this class.
Offers convenient properties needed for scheduled update modifiers:
start_epoch, end_epoch</p>
<div class="line-block">
<div class="line">Modifiers are expected to implement up to 3 different functions for TensorFlow:</div>
<div class="line-block">
<div class="line">- create_ops - inject ops into the graph before the training begins</div>
<div class="line">- create_extras - create extras like learning rate controls before training</div>
<div class="line">- complete_graph - finalize the graph after training has completed</div>
<div class="line"><br /></div>
</div>
<div class="line">Life cycle:</div>
<div class="line-block">
<div class="line">- create model graph</div>
<div class="line">- manager.create_ops()</div>
<div class="line">- manager.create_extras()</div>
<div class="line">- train graph</div>
<div class="line">- manager.complete_graph()</div>
<div class="line">- export graph</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – the loggers that can be used by the modifier instance</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>min_start</strong> – The minimum acceptable value for start_epoch, default -1</p></li>
<li><p><strong>min_end</strong> – The minimum acceptable value for end_epoch, default 0</p></li>
<li><p><strong>end_comparator</strong> – integer value representing how the end_epoch should be
compared to start_epoch.
if == None, then end_epoch can only be set to what its initial value was.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier.start_end_steps">
<code class="sig-name descname">start_end_steps</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">after_optim</span><span class="p">:</span> <span class="n">bool</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>int<span class="p">, </span>int<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ScheduledModifier.start_end_steps"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier.start_end_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the start and end steps for this modifier given a certain
amount of steps per epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (or batches) taken per epoch</p></li>
<li><p><strong>after_optim</strong> – True if the start and end are for an operation after
the optimizer update step has run, False for before</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing (the converted start step,
the converted end step)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">ScheduledUpdateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_start</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_end</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_comparator</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ScheduledUpdateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseUpdate" title="sparseml.optim.modifier.BaseUpdate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseUpdate</span></code></a></p>
<p>The base scheduled update modifier implementation,
all scheduled update modifiers should inherit from this class.
Offers convenient properties needed for scheduled update modifiers: update_frequency</p>
<div class="line-block">
<div class="line">Modifiers are expected to implement up to 3 different functions for TensorFlow:</div>
<div class="line-block">
<div class="line">- create_ops - inject ops into the graph before the training begins</div>
<div class="line">- create_extras - create extras like learning rate controls before training</div>
<div class="line">- complete_graph - finalize the graph after training has completed</div>
<div class="line"><br /></div>
</div>
<div class="line">Life cycle:</div>
<div class="line-block">
<div class="line">- create model graph</div>
<div class="line">- manager.create_ops()</div>
<div class="line">- manager.create_extras()</div>
<div class="line">- train graph</div>
<div class="line">- manager.complete_graph()</div>
<div class="line">- export graph</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – the loggers that can be used by the modifier instance</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>min_start</strong> – The minimum acceptable value for start_epoch, default -1</p></li>
<li><p><strong>min_end</strong> – The minimum acceptable value for end_epoch, default 0</p></li>
<li><p><strong>end_comparator</strong> – integer value representing how the end_epoch should be
compared to start_epoch.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.</p></li>
<li><p><strong>update_frequency</strong> – The number of epochs or fraction of epochs to
update at between start and end</p></li>
<li><p><strong>min_frequency</strong> – The minimum acceptable value for update_frequency, default -1</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier.update_frequency_steps">
<code class="sig-name descname">update_frequency_steps</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#ScheduledUpdateModifier.update_frequency_steps"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier.update_frequency_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the update frequency steps for this modifier given a certain
amount of steps per epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>steps_per_epoch</strong> – the number of steps (or batches) taken per epoch</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple containing (the converted start step,
the converted end step)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier.TensorFlowModifierYAML">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier.</code><code class="sig-name descname">TensorFlowModifierYAML</code><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier.html#TensorFlowModifierYAML"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier.TensorFlowModifierYAML" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.ModifierYAML" title="sparseml.optim.modifier.ModifierYAML"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.ModifierYAML</span></code></a></p>
<p>A decorator to handle making a TensorFlow modifier class YAML ready.
IE it can be loaded in through the yaml plugin easily.</p>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.modifier_epoch">
<span id="sparseml-tensorflow-v1-optim-modifier-epoch-module"></span><h2>sparseml.tensorflow_v1.optim.modifier_epoch module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.modifier_epoch" title="Permalink to this headline">¶</a></h2>
<p>Contains code for epoch modifiers in tensorflow_v1</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_epoch.EpochRangeModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_epoch.</code><code class="sig-name descname">EpochRangeModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_epoch.html#EpochRangeModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_epoch.EpochRangeModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Simple modifier to set the range of epochs to train over for
the recalibration process.
Note, that if other modifiers exceed the range of this one for min or max epochs,
this modifier will not have an effect.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!EpochRangeModifier:</div>
<div class="line-block">
<div class="line">start_epoch: 0</div>
<div class="line">end_epoch: 90</div>
</div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.modifier_lr">
<span id="sparseml-tensorflow-v1-optim-modifier-lr-module"></span><h2>sparseml.tensorflow_v1.optim.modifier_lr module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.modifier_lr" title="Permalink to this headline">¶</a></h2>
<p>Learning rate modifiers for TensorFlow models</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.GroupLearningRateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_lr.</code><code class="sig-name descname">GroupLearningRateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr_modifiers</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#GroupLearningRateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.GroupLearningRateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Combining multiple LR modifiers, correctly compute the learning rate</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>lr_modifiers</strong> – List of LR modifiers to combine</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.GroupLearningRateModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.ops.variables.VariableV1</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#GroupLearningRateModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.GroupLearningRateModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create switch case computing the learning rate at a given global step and
extras created by individual LR modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of empty ops, dict of named ops/tensors for learning
rate and summaries as extras)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_lr.</code><code class="sig-name descname">LearningRateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr_class</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">lr_kwargs</span><span class="p">:</span> <span class="n">Dict</span></em>, <em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#LearningRateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.LearningRate" title="sparseml.optim.learning_rate.LearningRate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.learning_rate.LearningRate</span></code></a></p>
<p>Modifier to set the learning rate to follow specific schedulers
within a period of epochs.
The following schedulers are current supported: ExponentialLR,
StepLR, MultiStepLR</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!LearningRateModifier</div>
<div class="line-block">
<div class="line">lr_class: ExponentialDecay</div>
<div class="line">lr_kwargs:</div>
<div class="line-block">
<div class="line">initial_learning_rate: 0.01</div>
<div class="line">decay_steps: 10000</div>
<div class="line">decay_rate: 0.96</div>
</div>
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">log_types: __ALL__</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_class</strong> – The name of the lr scheduler class to use:
[StepLR, MultiStepLR, ExponentialLR]</p></li>
<li><p><strong>lr_kwargs</strong> – The dictionary of keyword arguments to pass to the constructor
for the lr_class</p></li>
<li><p><strong>init_lr</strong> – The initial learning rate to use once this modifier starts</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at
(set to -1.0 so it starts immediately)</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at,
(set to -1.0 so it doesn’t end)</p></li>
<li><p><strong>update_frequency</strong> – unused and should not be set</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.ops.variables.VariableV1<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#LearningRateModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create ops to update the learning rate at the current global step</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (empty list of ops, dict of learning rate and summaries)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier.get_group">
<code class="sig-name descname">get_group</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#LearningRateModifier.get_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.LearningRateModifier.get_group" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The group modifier class into which this modifier needs to be combined</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_lr.</code><code class="sig-name descname">SetLearningRateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#SetLearningRateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.SetLearningRate" title="sparseml.optim.learning_rate.SetLearningRate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.learning_rate.SetLearningRate</span></code></a></p>
<p>Modifier to set the learning rate to a specific value at a certain point
in the training process. Once that point is reached, will update the optimizer’s
params with the learning rate</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!SetLearningRateModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">learning_rate: 0.001</div>
<div class="line">log_types: __ALL__</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The learning rate to use once this modifier starts</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – unused and should not be set</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.ops.variables.VariableV1<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#SetLearningRateModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create ops to set the learning rate at a given value if the global step reaches
a given value</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (empty list of ops,
dict of learning rate and logging summaries)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier.get_group">
<code class="sig-name descname">get_group</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_lr.html#SetLearningRateModifier.get_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_lr.SetLearningRateModifier.get_group" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The group modifier class into which this modifier needs to be combined</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.modifier_params">
<span id="sparseml-tensorflow-v1-optim-modifier-params-module"></span><h2>sparseml.tensorflow_v1.optim.modifier_params module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.modifier_params" title="Permalink to this headline">¶</a></h2>
<p>Modifier for changing the state of a modules params while training according to
certain update formulas or patterns.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_params.</code><code class="sig-name descname">TrainableParamsModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">trainable</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">params_strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_params.html#TrainableParamsModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Modifier to control the params for a given list of parameters.
Applies the trainability over all epochs.
To select all params in the graph, set to the ALL_TOKEN string: __ALL__</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!TrainableParamsModifier:</div>
<div class="line-block">
<div class="line">params: [“conv_net/conv1/weight”]</div>
<div class="line">trainable: True</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. Can also use the token __ALL__ to specify all
params</p></li>
<li><p><strong>trainable</strong> – True if the param(s) should be made trainable,
False to make them non-trainable</p></li>
<li><p><strong>params_strict</strong> – True if the given param(s) must be found in each layer and
will raise an err if not found,
False if missing params are ok and will not raise an err</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.complete_graph">
<code class="sig-name descname">complete_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_params.html#TrainableParamsModifier.complete_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.complete_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Complete modifying the graph.
Resets the objects filtered variables to their original trainability</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the modified graph that should be completed and cleaned.
if not supplied, then will use the default graph</p></li>
<li><p><strong>sess</strong> – the session to use for completing the modified graph.
if not supplied, then will use the default session</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_params.html#TrainableParamsModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the sparsity ops to modify the training graph according to the settings
for the current instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of ops, dict of named ops / tensors)
to be run or used for modifying the training process.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. Can also use the token __ALL__ to specify all
params</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.params_strict">
<code class="sig-name descname">params_strict</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.params_strict" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the given param(s) must be found in each layer and
will raise an err if not found,
False if missing params are ok and will not raise an err</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.trainable">
<code class="sig-name descname">trainable</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.trainable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the param(s) should be made trainable,
False to make them non-trainable</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_params.html#TrainableParamsModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_params.TrainableParamsModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.modifier_pruning">
<span id="sparseml-tensorflow-v1-optim-modifier-pruning-module"></span><h2>sparseml.tensorflow_v1.optim.modifier_pruning module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.modifier_pruning" title="Permalink to this headline">¶</a></h2>
<p>Modifiers for inducing / enforcing kernel sparsity (model pruning)
on models while pruning.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_pruning.</code><code class="sig-name descname">ConstantPruningModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#ConstantPruningModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Holds the sparsity level and shape for a given param constant while training.
Useful for transfer learning use cases.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!ConstantPruningModifier</div>
<div class="line-block">
<div class="line">params: __ALL__</div>
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">log_types: __ALL__</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – List of str names or regex patterns of names for the parameter
variables to apply the pruning modifier to. Regex patterns must be specified
with the prefix ‘re:’. Can also use the token __ALL__ to specify all
prunable layers and weights</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.complete_graph">
<code class="sig-name descname">complete_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#ConstantPruningModifier.complete_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.complete_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Complete modifying the graph.
Resets the pruned op’s variables using the created masks to zero out
the pruned weights for saving.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the modified graph that should be completed and cleaned.
if not supplied, then will use the default graph</p></li>
<li><p><strong>sess</strong> – the session to use for completing the modified graph.
if not supplied, then will use the default session</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cleaned graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#ConstantPruningModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the sparsity ops to modify the training graph according to the settings
for the current instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of ops, dict of named ops / tensors)
to be run or used for modifying the training process.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.initialize_session">
<code class="sig-name descname">initialize_session</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#ConstantPruningModifier.initialize_session"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.initialize_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the mask variables for pruning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sess</strong> – the session to use for initializing</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.ks_group">
<code class="sig-name descname">ks_group</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.ks_group" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>List of str for the variable names or regex patterns of names
to apply the pruning modifier to. Regex patterns must be specified with
the prefix ‘re:’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.prune_op_vars">
<em class="property">property </em><code class="sig-name descname">prune_op_vars</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.prune_op_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>the created pruning op vars in the graph if create_ops has been called,
else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.sparsity">
<em class="property">property </em><code class="sig-name descname">sparsity</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>the created sparsity tensor for setting the pruning ops
if create_ops has been called, else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.update_ready">
<em class="property">property </em><code class="sig-name descname">update_ready</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.ConstantPruningModifier.update_ready" title="Permalink to this definition">¶</a></dt>
<dd><p>the created update_ready tensor for setting the pruning ops
if create_ops has been called, else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.modifier_pruning.</code><code class="sig-name descname">GMPruningModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">init_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">final_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">inter_func</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'cubic'</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em>, <em class="sig-param"><span class="n">mask_type</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'unstructured'</span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#GMPruningModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier" title="sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.tensorflow_v1.optim.modifier.ScheduledUpdateModifier</span></code></a></p>
<p>Gradually applies kernel sparsity to a given variable or variables from
init_sparsity until final_sparsity is reached over a given amount of time and
applied with an interpolated function for each step taken.</p>
<p>Applies based on magnitude pruning without any structure to the pruning.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!GMPruningModifier</div>
<div class="line-block">
<div class="line">params: __ALL__</div>
<div class="line">init_sparsity: 0.05</div>
<div class="line">final_sparsity: 0.8</div>
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">update_frequency: 1.0</div>
<div class="line">inter_func: cubic</div>
<div class="line">log_types: __ALL__</div>
<div class="line">mask_type: unstructured</div>
<div class="line">leave_enabled: True</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – List of str names or name regex patterns for the variables in the
graph to apply the pruning modifier to.  Regex patterns must be specified with
the prefix ‘re:’.  __ALL__ will match to all parameters.</p></li>
<li><p><strong>init_sparsity</strong> – The initial sparsity for the variable to
start with at start_epoch</p></li>
<li><p><strong>final_sparsity</strong> – The final sparsity for the variable to end with at end_epoch</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>update_frequency</strong> – The number of epochs or fraction of epochs to
update at between start and end</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune</p></li>
<li><p><strong>inter_func</strong> – The type of interpolation function to use:
[linear, cubic, inverse_cubic]</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
<li><p><strong>mask_type</strong> – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameter’s in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.complete_graph">
<code class="sig-name descname">complete_graph</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#GMPruningModifier.complete_graph"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.complete_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Complete modifying the graph.
Resets the pruned op’s variables using the created masks to zero out
the pruned weights for saving.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the modified graph that should be completed and cleaned.
if not supplied, then will use the default graph</p></li>
<li><p><strong>sess</strong> – the session to use for completing the modified graph.
if not supplied, then will use the default session</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the cleaned graph</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.create_ops">
<code class="sig-name descname">create_ops</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Graph</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>List<span class="p">[</span>Union<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">, </span>tensorflow.python.framework.ops.Operation<span class="p">]</span><span class="p">]</span><span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#GMPruningModifier.create_ops"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.create_ops" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the sparsity ops to modify the training graph according to the settings
for the current instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>steps_per_epoch</strong> – the number of steps (batches) per training epoch</p></li>
<li><p><strong>global_step</strong> – the global step used while training</p></li>
<li><p><strong>graph</strong> – the graph to be modified</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a tuple (list of ops, dict of named ops / tensors)
to be run or used for modifying the training process.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.exponent">
<code class="sig-name descname">exponent</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.exponent" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.final_sparsity">
<code class="sig-name descname">final_sparsity</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.final_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>The final sparsity for the variable to end with at end_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.init_sparsity">
<code class="sig-name descname">init_sparsity</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.init_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>The initial sparsity for the variable to start with at start_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.initialize_session">
<code class="sig-name descname">initialize_session</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">tensorflow.python.client.session.Session</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#GMPruningModifier.initialize_session"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.initialize_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the mask variables for pruning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sess</strong> – the session to use for initializing</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.inter_func">
<code class="sig-name descname">inter_func</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.inter_func" title="Permalink to this definition">¶</a></dt>
<dd><p>The type of interpolation function to use:
[linear, cubic, inverse_cubic]</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.ks_group">
<code class="sig-name descname">ks_group</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.ks_group" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.leave_enabled">
<code class="sig-name descname">leave_enabled</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.leave_enabled" title="Permalink to this definition">¶</a></dt>
<dd><p>True to continue masking the weights after end_epoch,
False to stop masking. Note, if set as False, sparsity will not be enforced
and the model will likely deviate from the sparse solution</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.mask_type">
<code class="sig-name descname">mask_type</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.mask_type" title="Permalink to this definition">¶</a></dt>
<dd><p>the SparsityMaskCreator object used</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>List of str for the variable names or regex patterns of names
to apply the pruning modifier to. Regex patterns must be specified with
the prefix ‘re:’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.prune_op_vars">
<em class="property">property </em><code class="sig-name descname">prune_op_vars</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.prune_op_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>the created pruning op vars in the graph if create_ops has been called,
else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.sparsity">
<em class="property">property </em><code class="sig-name descname">sparsity</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>the created sparsity tensor for setting the pruning ops
if create_ops has been called, else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.update_ready">
<em class="property">property </em><code class="sig-name descname">update_ready</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.update_ready" title="Permalink to this definition">¶</a></dt>
<dd><p>the created update_ready tensor for setting the pruning ops
if create_ops has been called, else None</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/modifier_pruning.html#GMPruningModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.modifier_pruning.GMPruningModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.schedule_lr">
<span id="sparseml-tensorflow-v1-optim-schedule-lr-module"></span><h2>sparseml.tensorflow_v1.optim.schedule_lr module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.schedule_lr" title="Permalink to this headline">¶</a></h2>
<p>Learning rate schedules implementations for TensorFlow</p>
<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.schedule_lr.multi_step_lr_schedule">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.schedule_lr.</code><code class="sig-name descname">multi_step_lr_schedule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">start_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">milestone_steps</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'multi_step_lr_schedule'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/schedule_lr.html#multi_step_lr_schedule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.schedule_lr.multi_step_lr_schedule" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a multi step learning rate schedule in the current graph.
Multiplies init_lr by gamma after each milestone has passed.
Ex: lr = init_lr * (gamma ** NUM_UPDATES)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_step</strong> – the global step used for training</p></li>
<li><p><strong>start_step</strong> – the step to start the exponential schedule on</p></li>
<li><p><strong>milestone_steps</strong> – a list of steps to decrease the learning rate at,
these are the number of steps that must pass after start_step to decrease lr</p></li>
<li><p><strong>init_lr</strong> – the learning rate to start the schedule with</p></li>
<li><p><strong>gamma</strong> – the decay weight to decrease init_lr by after every step_size interval</p></li>
<li><p><strong>name</strong> – the name scope to create the graph under</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated learning rate tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.schedule_lr.step_lr_schedule">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.schedule_lr.</code><code class="sig-name descname">step_lr_schedule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">global_step</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">start_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">end_step</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">gamma</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'exponential_lr_schedule'</span></em><span class="sig-paren">)</span> &#x2192; tensorflow.python.framework.ops.Tensor<a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/schedule_lr.html#step_lr_schedule"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.schedule_lr.step_lr_schedule" title="Permalink to this definition">¶</a></dt>
<dd><p>Create an exponential learning rate schedule in the current graph.
Multiplies init_lr by gamma after each step_size interval has passed.
Ex: lr = init_lr * (gamma ** NUM_UPDATES)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>global_step</strong> – the global step used for training</p></li>
<li><p><strong>start_step</strong> – the step to start the exponential schedule on</p></li>
<li><p><strong>end_step</strong> – the step to end the exponential schedule on,
can be set to -1 and in that event will continually update the LR</p></li>
<li><p><strong>step_size</strong> – the number of steps between each gamma update to the init_lr</p></li>
<li><p><strong>init_lr</strong> – the learning rate to start the schedule with</p></li>
<li><p><strong>gamma</strong> – the decay weight to decrease init_lr by after every step_size interval</p></li>
<li><p><strong>name</strong> – the name scope to create the graph under</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the calculated learning rate tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim.sensitivity_pruning">
<span id="sparseml-tensorflow-v1-optim-sensitivity-pruning-module"></span><h2>sparseml.tensorflow_v1.optim.sensitivity_pruning module<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim.sensitivity_pruning" title="Permalink to this headline">¶</a></h2>
<p>Sensitivity analysis implementations for kernel sparsity on Graphs against loss funcs.</p>
<dl class="py class">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.sensitivity_pruning.</code><code class="sig-name descname">SparsePruningOpVars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_vars</span></em>, <em class="sig-param"><span class="n">sparsity</span></em><span class="sig-paren">)</span><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars.op_vars">
<em class="property">property </em><code class="sig-name descname">op_vars</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars.op_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars.sparsity">
<em class="property">property </em><code class="sig-name descname">sparsity</code><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars.sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_magnitude">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.sensitivity_pruning.</code><code class="sig-name descname">pruning_loss_sens_magnitude</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.client.session.Session<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sparsity_levels</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99)</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</a><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/sensitivity_pruning.html#pruning_loss_sens_magnitude"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_magnitude" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximated kernel sparsity (pruning) loss analysis for a given model.
Returns the results for each prunable param (conv, linear) in the model.
Approximated by taking the magnitudes of the weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the graph to inject pruning ops and vars into,
if not supplied uses get_default_graph()</p></li>
<li><p><strong>sess</strong> – the session to use</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to calculate the loss for for each param</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analysis results for the model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_one_shot">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.sensitivity_pruning.</code><code class="sig-name descname">pruning_loss_sens_one_shot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">op_vars</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars" title="sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars">sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars</a><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">loss_tensor</span><span class="p">:</span> <span class="n">tensorflow.python.framework.ops.Tensor</span></em>, <em class="sig-param"><span class="n">steps_per_measurement</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">add_ops_creator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span>int<span class="p">, </span>List<span class="p">[</span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">feed_dict_creator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">[</span>int<span class="p">, </span>Dict<span class="p">[</span>str<span class="p">, </span>tensorflow.python.framework.ops.Tensor<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sess</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.client.session.Session<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sparsity_levels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(0.0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99)</span></em>, <em class="sig-param"><span class="n">show_progress</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</a><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/sensitivity_pruning.html#pruning_loss_sens_one_shot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_one_shot" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a one shot sensitivity analysis for kernel sparsity.
It does not retrain, and instead puts the model to eval mode.
Moves operation by operation to calculate the sensitivity analysis for each and
resets the previously run layers.
Subsequent sparsity checks for layers and levels will be much faster.</p>
<p>Note: this should be run once a session has been created and
the variables have been created for the model.</p>
<p>Note: the graph should be recreated for later training as this creates
extra ops in the graph that should be reused before continuing in the system.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op_vars</strong> – the created pruning op vars from ks_loss_sensitivity_op_vars</p></li>
<li><p><strong>loss_tensor</strong> – the loss tensor in the model to measure for the sensitivity</p></li>
<li><p><strong>steps_per_measurement</strong> – the number of session.run calls to run through
for each sparsity level on each layer</p></li>
<li><p><strong>add_ops_creator</strong> – a callback to create an op/tens list to be run through
the session for each measurement. Called for each measurement</p></li>
<li><p><strong>feed_dict_creator</strong> – a callback to create a feed dict to be run through
the session for each measurement. Called for each measurement</p></li>
<li><p><strong>sess</strong> – the session to use</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to check for each layer to calculate
sensitivity</p></li>
<li><p><strong>show_progress</strong> – track progress of the runs if True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sensitivity results for every op that is prunable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_op_vars">
<code class="sig-prename descclassname">sparseml.tensorflow_v1.optim.sensitivity_pruning.</code><code class="sig-name descname">pruning_loss_sens_op_vars</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">graph</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>tensorflow.python.framework.ops.Graph<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">var_names</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">('re:.*')</span></em>, <em class="sig-param"><span class="n">mask_type</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator">sparseml.tensorflow_v1.optim.mask_creator_pruning.PruningMaskCreator</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'unstructured'</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars" title="sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars">sparseml.tensorflow_v1.optim.sensitivity_pruning.SparsePruningOpVars</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/tensorflow_v1/optim/sensitivity_pruning.html#pruning_loss_sens_op_vars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.tensorflow_v1.optim.sensitivity_pruning.pruning_loss_sens_op_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Edit the graph for to inject pruning ops and vars to allow for a ks loss
sensitivity analysis.</p>
<p>Note: this must be run outside of a session for it to take effect.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> – the graph to inject pruning ops and vars into,
if not supplied uses get_default_graph()</p></li>
<li><p><strong>var_names</strong> – List of variable names or regex patterns of variables to get
the op vars for.  Defaults to matching all variables</p></li>
<li><p><strong>mask_type</strong> – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameter’s in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the created pruning op vars to be used in approx_ks_loss_sensitivity and
one_shot_ks_loss_sensitivity</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.tensorflow_v1.optim">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sparseml.tensorflow_v1.optim" title="Permalink to this headline">¶</a></h2>
<p>Recalibration code for the TensorFlow framework.
Handles things like model pruning and increasing activation sparsity.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="sparseml.tensorflow_v1.utils.html" class="btn btn-neutral float-right" title="sparseml.tensorflow_v1.utils package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="sparseml.tensorflow_v1.nn.html" class="btn btn-neutral float-left" title="sparseml.tensorflow_v1.nn package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>