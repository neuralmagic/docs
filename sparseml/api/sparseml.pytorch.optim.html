<!DOCTYPE html>
<html class="writer-html5" lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   sparseml.pytorch.optim package — SparseML 0.3.1.20210514 documentation
  </title>
  <link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/css/nm-theme-adjustment.css" rel="stylesheet" type="text/css"/>
  <link href="../_static/favicon.ico" rel="shortcut icon"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js">
  </script>
  <script src="../_static/jquery.js">
  </script>
  <script src="../_static/underscore.js">
  </script>
  <script src="../_static/doctools.js">
  </script>
  <script src="../_static/clipboard.min.js">
  </script>
  <script src="../_static/copybutton.js">
  </script>
  <script src="../_static/js/theme.js" type="text/javascript">
  </script>
  <link href="../genindex.html" rel="index" title="Index"/>
  <link href="../search.html" rel="search" title="Search"/>
  <link href="sparseml.pytorch.sparsification.html" rel="next" title="sparseml.pytorch.sparsification package"/>
  <link href="sparseml.pytorch.nn.html" rel="prev" title="sparseml.pytorch.nn package"/>
 </head>
 <body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
   <nav class="wy-nav-side" data-toggle="wy-nav-shift">
    <div class="wy-side-scroll">
     <div class="wy-side-nav-search">
      <a class="icon icon-home" href="../index.html">
       SparseML
       <img alt="Logo" class="logo" src="../_static/icon-sparseml.png"/>
      </a>
      <div class="version">
       0.3
      </div>
      <div role="search">
       <form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
        <input name="q" placeholder="Search docs" type="text"/>
        <input name="check_keywords" type="hidden" value="yes"/>
        <input name="area" type="hidden" value="default"/>
       </form>
      </div>
     </div>
     <div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
      <p class="caption">
       <span class="caption-text">
        General
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/quicktour.html">
         Quick Tour
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/installation.html">
         Installation
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference internal" href="../source/recipes.html">
         Sparsification Recipes
        </a>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        API
       </span>
      </p>
      <ul class="current">
       <li class="toctree-l1 current">
        <a class="reference internal" href="sparseml.html">
         sparseml package
        </a>
        <ul class="current">
         <li class="toctree-l2 current">
          <a class="reference internal" href="sparseml.html#subpackages">
           Subpackages
          </a>
          <ul class="current">
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.deepsparse.html">
             sparseml.deepsparse package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.framework.html">
             sparseml.framework package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.keras.html">
             sparseml.keras package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.onnx.html">
             sparseml.onnx package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.optim.html">
             sparseml.optim package
            </a>
           </li>
           <li class="toctree-l3 current">
            <a class="reference internal" href="sparseml.pytorch.html">
             sparseml.pytorch package
            </a>
            <ul class="current">
             <li class="toctree-l4 current">
              <a class="reference internal" href="sparseml.pytorch.html#subpackages">
               Subpackages
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#submodules">
               Submodules
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch.base">
               sparseml.pytorch.base module
              </a>
             </li>
             <li class="toctree-l4">
              <a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">
               Module contents
              </a>
             </li>
            </ul>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.sparsification.html">
             sparseml.sparsification package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.tensorflow_v1.html">
             sparseml.tensorflow_v1 package
            </a>
           </li>
           <li class="toctree-l3">
            <a class="reference internal" href="sparseml.utils.html">
             sparseml.utils package
            </a>
           </li>
          </ul>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#submodules">
           Submodules
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.base">
           sparseml.base module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.log">
           sparseml.log module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml.version">
           sparseml.version module
          </a>
         </li>
         <li class="toctree-l2">
          <a class="reference internal" href="sparseml.html#module-sparseml">
           Module contents
          </a>
         </li>
        </ul>
       </li>
      </ul>
      <p class="caption">
       <span class="caption-text">
        Connect Online
       </span>
      </p>
      <ul>
       <li class="toctree-l1">
        <a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">
         Bugs, Feature Requests
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://discuss.neuralmagic.com/">
         Support, General Q&amp;A Forums
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ">
         Deep Sparse Community Slack
        </a>
       </li>
       <li class="toctree-l1">
        <a class="reference external" href="https://docs.neuralmagic.com">
         Neural Magic Docs
        </a>
       </li>
      </ul>
     </div>
    </div>
   </nav>
   <section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
    <nav aria-label="top navigation" class="wy-nav-top">
     <i class="fa fa-bars" data-toggle="wy-nav-top">
     </i>
     <a href="../index.html">
      SparseML
     </a>
    </nav>
    <div class="wy-nav-content">
     <div class="rst-content">
      <div aria-label="breadcrumbs navigation" role="navigation">
       <ul class="wy-breadcrumbs">
        <li>
         <a class="icon icon-home" href="../index.html">
         </a>
         »
        </li>
        <li>
         <a href="sparseml.html">
          sparseml package
         </a>
         »
        </li>
        <li>
         <a href="sparseml.pytorch.html">
          sparseml.pytorch package
         </a>
         »
        </li>
        <li>
         sparseml.pytorch.optim package
        </li>
        <li class="wy-breadcrumbs-aside">
         <a href="../_sources/api/sparseml.pytorch.optim.rst.txt" rel="nofollow">
          View page source
         </a>
        </li>
       </ul>
       <hr/>
      </div>
      <div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <div itemprop="articleBody">
        <div class="section" id="sparseml-pytorch-optim-package">
         <h1>
          sparseml.pytorch.optim package
          <a class="headerlink" href="#sparseml-pytorch-optim-package" title="Permalink to this headline">
           ¶
          </a>
         </h1>
         <div class="section" id="submodules">
          <h2>
           Submodules
           <a class="headerlink" href="#submodules" title="Permalink to this headline">
            ¶
           </a>
          </h2>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.analyzer_as">
          <span id="sparseml-pytorch-optim-analyzer-as-module">
          </span>
          <h2>
           sparseml.pytorch.optim.analyzer_as module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_as" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Code related to analyzing activation sparsity within PyTorch neural networks.
More information can be found in the paper
           <a class="reference external" href="https://arxiv.org/abs/1705.01626">
            here
           </a>
           .
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ASResultType">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.analyzer_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ASResultType
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               value
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ASResultType">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               enum.Enum
              </span>
             </code>
            </p>
            <p>
             Result type to track for activation sparsity.
            </p>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample">
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample
               </span>
              </span>
              <em class="property">
               <span class="pre">
                =
               </span>
               <span class="pre">
                'inputs_sample'
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                =
               </span>
               <span class="pre">
                'inputs_sparsity'
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample">
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample
               </span>
              </span>
              <em class="property">
               <span class="pre">
                =
               </span>
               <span class="pre">
                'outputs_sample'
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                =
               </span>
               <span class="pre">
                'outputs_sparsity'
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.analyzer_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModuleASAnalyzer
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dim
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               None
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Tuple
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ...
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               track_inputs_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               track_outputs_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inputs_sample_size
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               outputs_sample_size
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             An analyzer implementation used to monitor the activation sparsity with a module.
Generally used to monitor an individual layer.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – The module to analyze activation sparsity for
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  dim
                 </strong>
                 – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  track_inputs_sparsity
                 </strong>
                 – True to track the input sparsity to the module,
False otherwise
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  track_outputs_sparsity
                 </strong>
                 – True to track the output sparsity to the module,
False otherwise
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inputs_sample_size
                 </strong>
                 – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  outputs_sample_size
                 </strong>
                 – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  enabled
                 </strong>
                 – True to enable the hooks for analyzing and actively track,
False to disable and not track
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                analyze_layers
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 layers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 dim
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 None
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 Tuple
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ...
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 track_inputs_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 track_outputs_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 inputs_sample_size
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 outputs_sample_size
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 enabled
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.analyze_layers">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the module to analyze multiple layers activation sparsity in
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    layers
                   </strong>
                   – the names of the layers to analyze (from module.named_modules())
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    dim
                   </strong>
                   – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    track_inputs_sparsity
                   </strong>
                   – True to track the input sparsity to the module,
False otherwise
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    track_outputs_sparsity
                   </strong>
                   – True to track the output sparsity to the module,
False otherwise
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    inputs_sample_size
                   </strong>
                   – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    outputs_sample_size
                   </strong>
                   – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    enabled
                   </strong>
                   – True to enable the hooks for analyzing and actively track,
False to disable and not track
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 a list of the created analyzers, matches the ordering in layers
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear">
              <span class="sig-name descname">
               <span class="pre">
                clear
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 specific_result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 None
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.clear">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                dim
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                Union[None,
               </span>
               <span class="pre">
                int,
               </span>
               <span class="pre">
                Tuple[int,
               </span>
               <span class="pre">
                ...]]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable">
              <span class="sig-name descname">
               <span class="pre">
                disable
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.disable">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable">
              <span class="sig-name descname">
               <span class="pre">
                enable
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.enable">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                enabled
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample_max
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample_mean
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample_min
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample_size
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                int
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sample_std
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity_max
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity_mean
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity_min
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                inputs_sparsity_std
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                module
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.nn.modules.module.Module
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample_max
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample_mean
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample_min
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample_size
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                int
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sample_std
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity_max
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity_mean
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity_min
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                outputs_sparsity_std
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results">
              <span class="sig-name descname">
               <span class="pre">
                results
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max">
              <span class="sig-name descname">
               <span class="pre">
                results_max
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_max">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean">
              <span class="sig-name descname">
               <span class="pre">
                results_mean
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_mean">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min">
              <span class="sig-name descname">
               <span class="pre">
                results_min
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_min">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std">
              <span class="sig-name descname">
               <span class="pre">
                results_std
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 result_type
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">
                 <span class="pre">
                  sparseml.pytorch.optim.analyzer_as.ASResultType
                 </span>
                </a>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_std">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                track_inputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                track_outputs_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.analyzer_module">
          <span id="sparseml-pytorch-optim-analyzer-module-module">
          </span>
          <h2>
           sparseml.pytorch.optim.analyzer_module module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_module" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Code related to monitoring, analyzing, and reporting info for Modules in PyTorch.
Records things like FLOPS, input and output shapes, kernel shapes, etc.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.analyzer_module.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModuleAnalyzer
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             An analyzer implementation for monitoring the execution profile and graph of
a Module in PyTorch.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to analyze
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  enabled
                 </strong>
                 – True to enable the hooks for analyzing and actively track,
False to disable and not track
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                enabled
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if enabled and the hooks for analyzing are active, False otherwise
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs">
              <span class="sig-name descname">
               <span class="pre">
                ks_layer_descs
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc">
               <span class="pre">
                sparseml.optim.analyzer.AnalyzedLayerDesc
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.ks_layer_descs">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Get the descriptions for all layers in the module that support kernel sparsity
(model pruning). Ex: all convolutions and linear layers.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Returns
               </dt>
               <dd class="field-odd">
                <p>
                 a list of descriptions for all layers in the module that support ks
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc">
              <span class="sig-name descname">
               <span class="pre">
                layer_desc
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 name
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc">
               <span class="pre">
                sparseml.optim.analyzer.AnalyzedLayerDesc
               </span>
              </a>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.layer_desc">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Get a specific layer’s description within the Module.
Set to None to get the overall Module’s description.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  name
                 </strong>
                 – name of the layer to get a description for,
None for an overall description
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the analyzed layer description for the given name
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                module
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.nn.modules.module.Module
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The module that is being actively analyzed
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.analyzer_pruning">
          <span id="sparseml-pytorch-optim-analyzer-pruning-module">
          </span>
          <h2>
           sparseml.pytorch.optim.analyzer_pruning module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_pruning" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Code related to monitoring, analyzing, and reporting the kernel sparsity
(model pruning) for a model’s layers and params.
More info on kernel sparsity can be found
           <cite>
            here &lt;https://arxiv.org/abs/1902.09574&gt;
           </cite>
           __.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.analyzer_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModulePruningAnalyzer
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               param_name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'weight'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             An analyzer implementation monitoring the kernel sparsity of a given
param in a module.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module containing the param to analyze the sparsity for
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  name
                 </strong>
                 – name of the layer, used for tracking
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  param_name
                 </strong>
                 – name of the parameter to analyze the sparsity for,
defaults to weight
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                analyze_layers
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 layers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 param_name
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 str
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 'weight'
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.analyze_layers">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the module to create multiple analyzers for
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    layers
                   </strong>
                   – the names of the layers to create analyzer for that are
in the module
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    param_name
                   </strong>
                   – the name of the param to monitor within each layer
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 a list of analyzers, one for each layer passed in and in the same order
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                module
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.nn.modules.module.Module
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the module containing the param to analyze the sparsity for
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                name
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                str
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               name of the layer, used for tracking
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                param
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.nn.parameter.Parameter
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the parameter that is being monitored for kernel sparsity
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                param_name
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                str
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               name of the parameter to analyze the sparsity for, defaults to weight
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                param_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the sparsity of the contained parameter (how many zeros are in it)
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim">
              <span class="sig-name descname">
               <span class="pre">
                param_sparsity_dim
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 dim
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 None
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 Tuple
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ...
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.param_sparsity_dim">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  dim
                 </strong>
                 – a dimension(s) to calculate the sparsity over, ex over channels
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the sparsity of the contained parameter structured according
to the dim passed in
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                tag
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                str
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               combines the layer name and param name in to a single string
separated by a period
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.manager">
          <span id="sparseml-pytorch-optim-manager-module">
          </span>
          <h2>
           sparseml.pytorch.optim.manager module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.manager" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Contains base code related to modifier managers: modifier managers handle
grouping modifiers and running them together.
Also handles loading modifiers from yaml files
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.manager.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              RecipeManagerStepWrapper
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               wrap
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Any
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               optimizer
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.optim.optimizer.Optimizer
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               manager
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Any
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               steps_per_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             A wrapper class to handle wrapping an optimizer or optimizer like object
and override the step function.
The override calls into the ScheduledModifierManager when appropriate and enabled
and then calls step() as usual on the function with the original arguments.
All original attributes and methods are forwarded to the wrapped object
so this class can be a direct substitute for it.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  wrap
                 </strong>
                 – The object to wrap the step function and properties for.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  optimizer
                 </strong>
                 – The optimizer used in the training process.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – The model/module used in the training process.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  manager
                 </strong>
                 – The manager to forward lifecycle calls into such as step.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  epoch
                 </strong>
                 – The epoch to start the modifying process at.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  steps_per_epoch
                 </strong>
                 – The number of optimizer steps (batches) in each epoch.
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.emulated_step">
              <span class="sig-name descname">
               <span class="pre">
                emulated_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.emulated_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.emulated_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Emulated step function to be called in place of step when the
number of steps_per_epoch vary across epochs.
The emulated function should be called to keep the steps_per_epoch thee same.
Does not call into the step function for the wrapped object,
but does call into the manager to increment the steps.
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.loss_update">
              <span class="sig-name descname">
               <span class="pre">
                loss_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.loss_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.loss_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Optional call to update modifiers based on the calculated loss.
Not needed unless one or more of the modifier is using the loss
to make a modification or is modifying the loss itself.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  loss
                 </strong>
                 – the calculated loss after running a forward pass and loss_fn
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the modified loss tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.step">
              <span class="sig-name descname">
               <span class="pre">
                step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 *
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 args
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#RecipeManagerStepWrapper.step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Override for the step function.
Calls into the base step function with the args and kwargs.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    args
                   </strong>
                   – Any args to pass to the wrapped objects step function.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Any kwargs to pass to the wrapped objects step function.
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 The return, if any, from the wrapped objects step function
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The object to wrap the step function and properties for.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_epoch">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_epoch
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                float
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_epoch" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The current epoch the wrapped object is at.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_manager">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_manager
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_manager" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The manager to forward lifecycle calls into such as step.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_module">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_module
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.nn.modules.module.Module
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_module" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The model/module used in the training process.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_optimizer">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_optimizer
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                Any
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_optimizer" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The optimizer used in the training process.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_steps
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                int
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The current number of steps that have been called for
the wrapped object.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps_per_epoch">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                wrapped_steps_per_epoch
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                int
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper.wrapped_steps_per_epoch" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The number of optimizer steps (batches) in each epoch.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.manager.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ScheduledModifierManager
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               modifiers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.manager.BaseManager" title="sparseml.optim.manager.BaseManager">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.manager.BaseManager
               </span>
              </code>
             </a>
             ,
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.Modifier
               </span>
              </code>
             </a>
            </p>
            <p>
             The base modifier manager, handles managing multiple ScheduledModifers.
            </p>
            <div class="line-block">
             <div class="line">
              Lifecycle:
             </div>
             <div class="line-block">
              <div class="line">
               - initialize
              </div>
              <div class="line">
               - initialize_loggers
              </div>
              <div class="line">
               - modify
              </div>
              <div class="line">
               - finalize
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                modifiers
               </strong>
               – the modifiers to wrap
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.finalize">
              <span class="sig-name descname">
               <span class="pre">
                finalize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 reset_loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.finalize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.finalize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles any finalization of the modifier for the given model/module.
Applies any remaining logic and cleans up any hooks or attachments to the model.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – The model/module to finalize the modifier for.
Marked optional so state can still be cleaned up on delete,
but generally should always be passed in.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    reset_loggers
                   </strong>
                   – True to remove any currently attached loggers (default),
False to keep the loggers attached.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                from_yaml
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 file_path
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 sparsezoo.objects.recipe.Recipe
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 add_modifiers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier">
                 <span class="pre">
                  sparseml.pytorch.optim.modifier.Modifier
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.from_yaml">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Convenience function used to create the manager of multiple modifiers from a
recipe file.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    file_path
                   </strong>
                   – the path to the recipe file to load the modifier from, or
a SparseZoo model stub to load a recipe for a model stored in SparseZoo.
SparseZoo stubs should be preceded by ‘zoo:’, and can contain an optional
‘?recipe_type=&lt;type&gt;’ parameter. Can also be a SparseZoo Recipe
object. i.e. ‘/path/to/local/recipe.yaml’, ‘zoo:model/stub/path’,
‘zoo:model/stub/path?recipe_type=transfer’
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    add_modifiers
                   </strong>
                   – additional modifiers that should be added to the
returned manager alongside the ones loaded from the recipe file
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 ScheduledModifierManager() created from the recipe file
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles any initialization of the manager for the given model/module.
epoch and steps_per_epoch can optionally be passed in to initialize the manager
and module at a specific point in the training process.
If loggers is not None, will additionally call initialize_loggers.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the manager and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers">
              <span class="sig-name descname">
               <span class="pre">
                initialize_loggers
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 None
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize_loggers">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles initializing and setting up the loggers for the contained modifiers.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  loggers
                 </strong>
                 – the loggers to setup this manager with for logging important
info and milestones to
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict">
              <span class="sig-name descname">
               <span class="pre">
                load_state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 state_dict
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Dict
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 Dict
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 strict
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.load_state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Loads the given state dict into this manager.
All modifiers that match will be loaded.
If any are missing or extra and strict=True, then will raise a KeyError
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    state_dict
                   </strong>
                   – dictionary object as generated by this object’s state_dict
function
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    strict
                   </strong>
                   – True to raise a KeyError for any missing or extra information in
the state dict, False to ignore
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Raises
               </dt>
               <dd class="field-even">
                <p>
                 <strong>
                  IndexError
                 </strong>
                 – If any keys in the state dict do not correspond to a valid
index for this manager and strict=True
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update">
              <span class="sig-name descname">
               <span class="pre">
                loss_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.loss_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Optional call that can be made on the optimizer to update the contained
modifiers once loss has been calculated
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    loss
                   </strong>
                   – The calculated loss tensor
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the modified loss tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.modify">
              <span class="sig-name descname">
               <span class="pre">
                modify
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 wrap_optim
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 Any
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 float
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <a class="reference internal" href="#sparseml.pytorch.optim.manager.RecipeManagerStepWrapper" title="sparseml.pytorch.optim.manager.RecipeManagerStepWrapper">
               <span class="pre">
                sparseml.pytorch.optim.manager.RecipeManagerStepWrapper
               </span>
              </a>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.modify">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.modify" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Modify the given module and optimizer for training aware algorithms such as
pruning and quantization.
Initialize must be called first.
After training is complete, finalize should be called.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – The model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – The optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – The number of optimizer steps (batches) in each epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    wrap_optim
                   </strong>
                   – Optional object to wrap instead of the optimizer.
Useful for cases like amp (fp16 training) where a it should be wrapped
in place of the original optimizer since it doesn’t always call into
the optimizer.step() function.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – Optional epoch that can be passed in to start modifying at.
Defaults to the epoch that was supplied to the initialize function.
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 A wrapped optimizer object. The wrapped object makes all the
original properties for the wrapped object available so it can be
used without any additional code changes.
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step">
              <span class="sig-name descname">
               <span class="pre">
                optimizer_post_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_post_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called after the optimizer step happens and weights have updated
Calls into the contained modifiers
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step">
              <span class="sig-name descname">
               <span class="pre">
                optimizer_pre_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_pre_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called before the optimizer step happens (after backward has been called,
before optimizer.step)
Calls into the contained modifiers
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict">
              <span class="sig-name descname">
               <span class="pre">
                state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Returns
               </dt>
               <dd class="field-odd">
                <p>
                 Dictionary to store any state variables for this manager.
Includes all modifiers nested under this manager as sub keys in the dict.
Only modifiers that a non empty state dict are included.
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.manager.ScheduledModifierManager.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 log_updates
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles updating the contained modifiers’ states, module, or optimizer
Only calls scheduled_update on the each modifier if modifier.update_ready()
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    log_updates
                   </strong>
                   – True to log the updates for each modifier to the loggers,
False to skip logging
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.mask_creator_pruning">
          <span id="sparseml-pytorch-optim-mask-creator-pruning-module">
          </span>
          <h2>
           sparseml.pytorch.optim.mask_creator_pruning module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.mask_creator_pruning" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Classes for defining sparsity masks based on model parameters.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              BlockPruningMaskCreator
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               block_shape
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               grouping_fn_name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'mean'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator
               </span>
              </code>
             </a>
            </p>
            <p>
             Structured sparsity mask creator that groups the input tensor into blocks of
shape block_shape.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  block_shape
                 </strong>
                 – The shape in and out channel should take in blocks.  Should be
a list of exactly two integers that divide the input tensors evenly on the
channel dimensions.  -1 for a dimension blocks across the entire dimension
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  grouping_fn_name
                 </strong>
                 – The name of the torch grouping function to reduce
dimensions by
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor">
              <span class="sig-name descname">
               <span class="pre">
                group_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensor
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator.group_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  tensor
                 </strong>
                 – The tensor to transform
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 The absolute mean values of the tensor grouped by blocks of
shape self._block_shape
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              DimensionSparsityMaskCreator
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dim
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               grouping_fn_name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'mean'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator
               </span>
              </code>
             </a>
            </p>
            <p>
             Structured sparsity mask creator that groups sparsity blocks by the given
dimension(s)
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  dim
                 </strong>
                 – The index or list of indices of dimensions to group the mask by or
the type of dims to prune ([‘channel’, ‘filter’])
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  grouping_fn_name
                 </strong>
                 – The name of the torch grouping function to reduce
dimensions by
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.create_sparsity_masks">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 global_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.create_sparsity_masks">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.create_sparsity_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate masks from based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    sparsity
                   </strong>
                   – the desired sparsity to reach within the mask
(decimal fraction of zeros)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    global_sparsity
                   </strong>
                   – do not set True, unsupported for
DimensionSparsityMaskCreator
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity and all values mapped to the same group have the same
value
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor">
              <span class="sig-name descname">
               <span class="pre">
                group_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensor
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.group_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  tensor
                 </strong>
                 – The tensor to transform
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 The absolute mean values of the tensor grouped by the
dimension(s) in self._dim
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              GroupedPruningMaskCreator
             </span>
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator
               </span>
              </code>
             </a>
            </p>
            <p>
             Abstract class for a sparsity mask creator that structures masks according to
grouping functions.  Subclasses should implement group_tensor and
_map_mask_to_tensor
            </p>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 global_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate masks from based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    sparsity
                   </strong>
                   – the desired sparsity to reach within the mask
(decimal fraction of zeros)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    global_sparsity
                   </strong>
                   – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity and all values mapped to the same group have the same
value
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_abs_threshold">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks_from_abs_threshold
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 threshold
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 float
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks_from_abs_threshold">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_abs_threshold" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate masks from based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    threshold
                   </strong>
                   – a threshold of group_tensor values to determine cutoff
for sparsification
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks derived from the tensors and the grouped threshold
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_tensor">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks_from_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_masks_from_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_masks_from_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  tensors
                 </strong>
                 – list of tensors to calculate masks based on their values
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks derived from the values of the tensors grouped by
the group_tensor function.
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor">
              <em class="property">
               <span class="pre">
                abstract
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                group_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensor
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.group_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  tensor
                 </strong>
                 – The tensor to reduce in groups
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 The grouped tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.reduce_tensor">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                reduce_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensor
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 dim
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 reduce_fn_name
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 str
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 keepdim
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.reduce_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.reduce_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensor
                   </strong>
                   – the tensor to reduce
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    dim
                   </strong>
                   – dimension or list of dimension to reduce along
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    reduce_fn_name
                   </strong>
                   – function name to reduce tensor with. valid options
are ‘mean’, ‘max’, ‘min’
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    keepdim
                   </strong>
                   – preserves the reduced dimension(s) in returned tensor shape
as shape 1. default is True
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 Tensor reduced along the given dimension(s)
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              PruningMaskCreator
             </span>
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               abc.ABC
              </span>
             </code>
            </p>
            <p>
             Base abstract class for a sparsity mask creator.
Subclasses should define all methods for creating masks
            </p>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks">
              <em class="property">
               <span class="pre">
                abstract
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 global_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate a masks based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    sparsity
                   </strong>
                   – the desired sparsity to reach within the mask
(decimal fraction of zeros)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    global_sparsity
                   </strong>
                   – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity.
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_abs_threshold">
              <em class="property">
               <span class="pre">
                abstract
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks_from_abs_threshold
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 threshold
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 float
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks_from_abs_threshold">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_abs_threshold" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate a masks based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    threshold
                   </strong>
                   – a threshold to determine cutoff for sparsification
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks derived from each of the given tensors and the threshold
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_tensor">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks_from_tensor
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_masks_from_tensor">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_masks_from_tensor" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  tensors
                 </strong>
                 – list of tensors to calculate a masks based on their values
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks derived from each of the given tensors
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              UnstructuredPruningMaskCreator
             </span>
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </code>
             </a>
            </p>
            <p>
             Class for creating unstructured sparsity masks.
Masks will be created using unstructured sparsity by pruning weights ranked
by their magnitude.  Each mask will correspond to the given tensor.
            </p>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 global_sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 False
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_masks">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate a mask from based on their
contained values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    sparsity
                   </strong>
                   – the desired sparsity to reach within the mask
(decimal fraction of zeros)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    global_sparsity
                   </strong>
                   – if True, sparsity masks will be created such that the
average sparsity across all given tensors is the target sparsity with the
lowest global values masked. If False, each tensor will be masked to the
target sparsity ranking values within each individual tensor. Default is
False
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors such that the desired number of zeros
matches the sparsity.  If there are more zeros than the desired sparsity,
zeros will be randomly chosen to match the target sparsity
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks_from_abs_threshold">
              <span class="sig-name descname">
               <span class="pre">
                create_sparsity_masks_from_abs_threshold
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 tensors
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 threshold
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 float
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_masks_from_abs_threshold">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_masks_from_abs_threshold" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    tensors
                   </strong>
                   – list of tensors to calculate a masks based on their contained
values
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    threshold
                   </strong>
                   – a threshold at which to mask abs(values) if they are
less than it or equal
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 list of masks (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tensors abs(values) &lt;= threshold are masked,
all others are unmasked
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              load_mask_creator
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               obj
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Iterable
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
             <span class="pre">
              sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#load_mask_creator">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                obj
               </strong>
               – Formatted string or block shape iterable specifying SparsityMaskCreator
object to return
              </p>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               SparsityMaskCreator object created from obj
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.mask_pruning">
          <span id="sparseml-pytorch-optim-mask-pruning-module">
          </span>
          <h2>
           sparseml.pytorch.optim.mask_pruning module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.mask_pruning" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Code related to applying a mask onto a parameter to impose kernel sparsity,
aka model pruning
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.mask_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModuleParamPruningMask
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               param_names
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'weight'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               store_init
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               store_unmasked
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               track_grad_mom
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               mask_creator
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </a>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               unstructured
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layer_names
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               global_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             Mask to apply kernel sparsity (model pruning) to a specific parameter in a layer
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  layers
                 </strong>
                 – the layers containing the parameters to mask
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  param_names
                 </strong>
                 – the names of the parameter to mask in each layer. If only
one name is given, that name will be applied to all layers that this object
masks. default is weight
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  store_init
                 </strong>
                 – store the init weights in a separate variable that can be
used and referenced later
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  store_unmasked
                 </strong>
                 – store the unmasked weights in a separate variable that
can be used and referenced later
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  track_grad_mom
                 </strong>
                 – store the gradient updates to the parameter with a
momentum variable must be in the range [0.0, 1.0), if set to 0.0 then will
only keep most recent
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  mask_creator
                 </strong>
                 – object to define sparisty mask creation,
default is unstructured mask
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  layer_names
                 </strong>
                 – the name of the layers the parameters to mask are located in
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  global_sparsity
                 </strong>
                 – set True to enable global pruning. if True, when creating
sparsity masks for a target sparsity sparsity masks will be created such that
the average sparsity across all given layers is the target sparsity with the
lowest global values masked. If False, each layer will be masked to the target
sparsity ranking values within each individual tensor. Default is False
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply">
              <span class="sig-name descname">
               <span class="pre">
                apply
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 param_idx
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 int
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.apply">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               apply the current mask to the params tensor (zero out the desired values)
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  param_idx
                 </strong>
                 – index of parameter to apply mask to. if not set, then masks
will be applied to all parameters with available masks
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                enabled
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if the parameter is currently being masked, False otherwise
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.global_sparsity">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                global_sparsity
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.global_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if global pruning is enabled, False otherwise
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_names">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                layer_names
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                Optional[List[str]]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_names" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the names of the layers the parameter to mask is located in
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layers">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                layers
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.nn.modules.module.Module]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the layers containing the parameters to mask
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                mask_creator
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               SparsityMaskCreator object used to generate masks
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.names">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                names
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[str]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.names" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the full names of the sparsity masks in the following format:
&lt;LAYER&gt;.&lt;PARAM&gt;.sparsity_mask
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_masks">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                param_masks
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the current masks applied to each of the parameters
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_names">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                param_names
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[str]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_names" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the names of the parameters to mask in the layers
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_data">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                params_data
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[torch.Tensor]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_data" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the current tensors in each of the parameters
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_grad">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                params_grad
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[Optional[torch.Tensor]]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_grad" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the current gradient values for each parameter
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_init">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                params_init
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[Optional[torch.Tensor]]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_init" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the initial values of the parameters before being masked
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_unmasked">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                params_unmasked
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[Optional[torch.Tensor]]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.params_unmasked" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the unmasked values of the parameters
(stores the last unmasked value before masking)
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset">
              <span class="sig-name descname">
               <span class="pre">
                reset
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.reset">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               resets the current stored tensors such that they will be on the same device
and have the initial data
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data">
              <span class="sig-name descname">
               <span class="pre">
                set_param_data
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 value
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 param_idx
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_data">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    value
                   </strong>
                   – the value to set as the current tensor for the parameter,
if enabled the mask will be applied
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    param_idx
                   </strong>
                   – index of the parameter in this object to set the data of
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks">
              <span class="sig-name descname">
               <span class="pre">
                set_param_masks
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 masks
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  masks
                 </strong>
                 – the masks to set and apply as the current param tensors,
if enabled mask is applied immediately
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_abs_threshold">
              <span class="sig-name descname">
               <span class="pre">
                set_param_masks_from_abs_threshold
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 threshold
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 float
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 torch.Tensor
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_abs_threshold">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_abs_threshold" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Convenience function to set the parameter masks such that if
abs(value) &lt;= threshold the it a value is masked to 0
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  threshold
                 </strong>
                 – the threshold at which all values will be masked to 0
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                set_param_masks_from_sparsity
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 sparsity
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_sparsity">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Convenience function to set the parameter masks such that each masks have an
amount of masked values such that the percentage equals the sparsity amount
given. Masks the absolute smallest values up until sparsity is reached.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  sparsity
                 </strong>
                 – the decimal sparsity to set the param mask to
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_weights">
              <span class="sig-name descname">
               <span class="pre">
                set_param_masks_from_weights
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_masks_from_weights">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_masks_from_weights" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Convenience function to set the parameter masks such that the
mask is 1 if a parameter value is non zero and 0 otherwise,
unless otherwise defined by this object’s mask_creator.
              </p>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                store_init
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               store the init weights in a separate variable that can be used and
referenced later
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                store_unmasked
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               store the unmasked weights in a separate variable that can be used and
referenced later
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                track_grad_mom
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                float
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               store the gradient updates to the parameter with a momentum variable
must be in the range [0.0, 1.0), if set to 0.0 then will only
keep most recent
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier">
          <span id="sparseml-pytorch-optim-modifier-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Contains base code related to modifiers: objects that modify some aspect
of the training process for a model.
For example, learning rate schedules or kernel sparsity (weight pruning)
are implemented as modifiers.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              Modifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="o">
              <span class="pre">
               **
              </span>
             </span>
             <span class="n">
              <span class="pre">
               kwargs
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseModifier" title="sparseml.optim.modifier.BaseModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.modifier.BaseModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             The base pytorch modifier implementation,
all modifiers must inherit from this class.
It defines common things needed for the lifecycle and implementation of a modifier.
            </p>
            <div class="line-block">
             <div class="line">
              Lifecycle:
             </div>
             <div class="line-block">
              <div class="line">
               - initialize
              </div>
              <div class="line">
               - initialize_loggers
              </div>
              <div class="line">
               <br/>
              </div>
              <div class="line">
               training loop:
              </div>
              <div class="line-block">
               <div class="line">
                - update
               </div>
               <div class="line">
                - log_update
               </div>
               <div class="line">
                - loss_update
               </div>
               <div class="line">
                - optimizer_pre_step
               </div>
               <div class="line">
                - optimizer_post_step
               </div>
               <div class="line">
                <br/>
               </div>
              </div>
              <div class="line">
               - finalize
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers that can be used by the modifier instance
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  kwargs
                 </strong>
                 – standard key word args, used to support multi inheritance
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.apply">
              <span class="sig-name descname">
               <span class="pre">
                apply
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 inf
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 finalize
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.apply">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.apply" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Apply the modifier for a given model/module (one shot application).
Calls into initialize(module, epoch, loggers,
               <a href="#id1">
                <span class="problematic" id="id2">
                 **
                </span>
               </a>
               kwargs) and then
finalize(module,
               <a href="#id3">
                <span class="problematic" id="id4">
                 **
                </span>
               </a>
               kwargs) immediately after if finalize=True.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – the epoch to apply the modifier at, defaults to math.inf (end)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    finalize
                   </strong>
                   – True to invoke finalize after initialize, False otherwise.
If training after one shot, set finalize=False to keep modifiers applied.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers (passed to initialize and finalize).
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.finalize">
              <span class="sig-name descname">
               <span class="pre">
                finalize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 reset_loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.finalize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.finalize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles any finalization of the modifier for the given model/module.
Applies any remaining logic and cleans up any hooks or attachments to the model.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – The model/module to finalize the modifier for.
Marked optional so state can still be cleaned up on delete,
but generally should always be passed in.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    reset_loggers
                   </strong>
                   – True to remove any currently attached loggers (default),
False to keep the loggers attached.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles any initialization of the modifier for the given model/module.
epoch and steps_per_epoch can optionally be passed in to initialize the modifier
and module at a specific point in the training process.
If loggers is not None, will additionally call initialize_loggers.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.initialize_loggers">
              <span class="sig-name descname">
               <span class="pre">
                initialize_loggers
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Union
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 None
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.initialize_loggers">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.initialize_loggers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles initializing and setting up the loggers for the modifier.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  loggers
                 </strong>
                 – the loggers to setup this modifier with for logging important
info and milestones to
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.load_list">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                load_list
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 yaml_str
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 str
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.load_list">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.load_list" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  yaml_str
                 </strong>
                 – a string representation of the yaml syntax to
load modifiers from
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the loaded modifiers list
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.load_obj">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                load_obj
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 yaml_str
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 str
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.load_obj">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.load_obj" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  yaml_str
                 </strong>
                 – a string representation of the yaml syntax to
load a modifier from
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the loaded modifier object
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.load_state_dict">
              <span class="sig-name descname">
               <span class="pre">
                load_state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 state_dict
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Dict
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ,
                 </span>
                </span>
                <span class="pre">
                 Dict
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 strict
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.load_state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.load_state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Loads the given state dict into this modifier
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    state_dict
                   </strong>
                   – dictionary object as generated by this object’s state_dict
function
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    strict
                   </strong>
                   – True to raise a KeyError for any missing or extra information in
the state dict, False to ignore
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Raises
               </dt>
               <dd class="field-even">
                <p>
                 <strong>
                  IndexError
                 </strong>
                 – If any keys in the state dict do not correspond to a valid
index for this manager and strict=True
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.log_update">
              <span class="sig-name descname">
               <span class="pre">
                log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles logging updates for the modifier for better tracking and visualization.
Should be overwritten for logging.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.loggers">
              <span class="sig-name descname">
               <span class="pre">
                loggers
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loggers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.loggers_initialized">
              <span class="sig-name descname">
               <span class="pre">
                loggers_initialized
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loggers_initialized" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.loss_update">
              <span class="sig-name descname">
               <span class="pre">
                loss_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.loss_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loss_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Optional call that can be made on the optimizer to update the modifiers
once the loss has been calculated.
Called independent of if the modifier is currently active or not.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    loss
                   </strong>
                   – The calculated loss tensor
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the modified loss tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.optimizer_post_step">
              <span class="sig-name descname">
               <span class="pre">
                optimizer_post_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.optimizer_post_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.optimizer_post_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called after the optimizer step happens and weights have updated.
Called independent of if the modifier is currently active or not.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.optimizer_pre_step">
              <span class="sig-name descname">
               <span class="pre">
                optimizer_pre_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.optimizer_pre_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.optimizer_pre_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called before the optimizer step happens
(after backward has been called, before optimizer.step).
Called independent of if the modifier is currently active or not.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.state_dict">
              <span class="sig-name descname">
               <span class="pre">
                state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Returns
               </dt>
               <dd class="field-odd">
                <p>
                 PyTorch state dictionary to store any variables from this modifier
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.Modifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModifierProp
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               serializable
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               restrict_initialized
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               restrict_enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               restrict_extras
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               no_serialize_val
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               func_get
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               func_set
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               doc
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.modifier.BaseProp
               </span>
              </code>
             </a>
            </p>
            <p>
             Property used to decorate a modifier.
Use for creating getters and setters in a modifier.
Handles making sure props cannot be changed after a certain point;
ex after initialized.
Also, marks the properties so they can be easily collected and serialized later.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  serializable
                 </strong>
                 – True if the property should be serialized (ex in yaml),
False otherwise. Default True
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  restrict_initialized
                 </strong>
                 – True to keep the property from being set after
initialized, False otherwise. Default True
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  restrict_enabled
                 </strong>
                 – True to keep the property from being set after enabled,
False otherwise. Default False
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  restrict_extras
                 </strong>
                 – extra attributes to check, if any are truthy then keep
from being set. Default None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  no_serialize_val
                 </strong>
                 – If prop is equal to this value, will not serialize the prop
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  func_get
                 </strong>
                 – The function getter
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  func_set
                 </strong>
                 – The function setter
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  doc
                 </strong>
                 – The docs function
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp.getter">
              <span class="sig-name descname">
               <span class="pre">
                getter
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 func_get
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Callable
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">
               <span class="pre">
                sparseml.optim.modifier.BaseProp
               </span>
              </a>
              <a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.getter">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.getter" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Create a ModifierProp based off the current instance with the getter function
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  func_get
                 </strong>
                 – the getter function
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the recreated instance with the new getter function
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp.no_serialize_val">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                no_serialize_val
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                Any
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.no_serialize_val" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               a value that if the prop is equal to, will not serialize the prop
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp.restrictions">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                restrictions
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                List[str]
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.restrictions" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The attributes to check for restricting when the attribute can be set
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp.serializable">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                serializable
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                bool
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.serializable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if the property should be serialized (ex in yaml), False otherwise
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ModifierProp.setter">
              <span class="sig-name descname">
               <span class="pre">
                setter
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 func_set
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Callable
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">
               <span class="pre">
                sparseml.optim.modifier.BaseProp
               </span>
              </a>
              <a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.setter">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.setter" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Create a ModifierProp based off the current instance with the setter function
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  func_set
                 </strong>
                 – the setter function
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the recreated instance with the new setter function
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.PyTorchModifierYAML">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              PyTorchModifierYAML
             </span>
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#PyTorchModifierYAML">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier.PyTorchModifierYAML" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.ModifierYAML" title="sparseml.optim.modifier.ModifierYAML">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.modifier.ModifierYAML
               </span>
              </code>
             </a>
            </p>
            <p>
             A decorator to handle making a pytorch modifier class YAML ready.
IE it can be loaded in through the yaml plugin easily.
            </p>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ScheduledModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_start
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_end
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_comparator
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="o">
              <span class="pre">
               **
              </span>
             </span>
             <span class="n">
              <span class="pre">
               kwargs
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.Modifier
               </span>
              </code>
             </a>
             ,
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseScheduled" title="sparseml.optim.modifier.BaseScheduled">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.modifier.BaseScheduled
               </span>
              </code>
             </a>
            </p>
            <p>
             The base scheduled modifier implementation,
all scheduled modifiers must inherit from this class.
The difference for this and a Modifier is that these have start and end epochs.
It defines common things needed for the lifecycle and implementation of a
scheduled modifier.
            </p>
            <div class="line-block">
             <div class="line">
              Lifecycle:
             </div>
             <div class="line-block">
              <div class="line">
               - initialize
              </div>
              <div class="line">
               - initialize_loggers
              </div>
              <div class="line">
               <br/>
              </div>
              <div class="line">
               training loop:
              </div>
              <div class="line-block">
               <div class="line">
                - update_ready
               </div>
               <div class="line-block">
                <div class="line">
                 - scheduled_update
                </div>
                <div class="line-block">
                 <div class="line">
                  - update
                 </div>
                </div>
               </div>
               <div class="line">
                - scheduled_log_update
               </div>
               <div class="line-block">
                <div class="line">
                 - log_update
                </div>
               </div>
               <div class="line">
                - loss_update
               </div>
               <div class="line">
                - optimizer_pre_step
               </div>
               <div class="line">
                - optimizer_post_step
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers that can be used by the modifier instance
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers that can be used by the modifier instance
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_start
                 </strong>
                 – The minimum acceptable value for start_epoch, default -1
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_end
                 </strong>
                 – The minimum acceptable value for end_epoch, default 0
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_comparator
                 </strong>
                 – integer value representing how the end_epoch should be
compared to start_epoch.
if == None, then end_epoch can only be set to what its initial value was.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  kwargs
                 </strong>
                 – standard key word args, used to support multi inheritance
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.end_pending">
              <span class="sig-name descname">
               <span class="pre">
                end_pending
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.end_pending">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.end_pending" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Base implementation compares current epoch with the end epoch and
that it has been started.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 True if the modifier is ready to stop modifying, false otherwise
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.ended">
              <span class="sig-name descname">
               <span class="pre">
                ended
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.ended" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.log_update">
              <span class="sig-name descname">
               <span class="pre">
                log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles logging updates for the modifier for better tracking and visualization.
Should be overridden for logging but not called directly,
use scheduled_log_update instead.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_log_update">
              <span class="sig-name descname">
               <span class="pre">
                scheduled_log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.scheduled_log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles checking if a log update should happen.
IE, is the modifier currently in the range of its start and end epochs.
No restrictions are placed on it by update_ready in the event that the modifier
should log constantly or outside of an update being ready.
General use case is checking if logs should happen by comparing
cached values with updated values.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_update">
              <span class="sig-name descname">
               <span class="pre">
                scheduled_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.scheduled_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called by the system and calls into update() method
Tracks state and should not be overridden!!
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.start_pending">
              <span class="sig-name descname">
               <span class="pre">
                start_pending
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.start_pending">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.start_pending" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Base implementation compares current epoch with the start epoch.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 True if the modifier is ready to begin modifying, false otherwise
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.started">
              <span class="sig-name descname">
               <span class="pre">
                started
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.started" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledModifier.update_ready">
              <span class="sig-name descname">
               <span class="pre">
                update_ready
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.update_ready">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.update_ready" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Base implementation checks if start_pending() or end_pending().
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 True if the modifier is pending an update and update() should be called
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ScheduledUpdateModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_start
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_end
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_comparator
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="o">
              <span class="pre">
               **
              </span>
             </span>
             <span class="n">
              <span class="pre">
               kwargs
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
             ,
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseUpdate" title="sparseml.optim.modifier.BaseUpdate">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.modifier.BaseUpdate
               </span>
              </code>
             </a>
            </p>
            <p>
             The base scheduled update modifier implementation,
all scheduled update modifiers must inherit from this class.
The difference for this and a ScheduledModifier is that these have a certain
interval that they update within the start and end ranges.
It defines common things needed for the lifecycle and implementation of a scheduled
update modifier.
            </p>
            <div class="line-block">
             <div class="line">
              Lifecycle:
             </div>
             <div class="line-block">
              <div class="line">
               - initialize
              </div>
              <div class="line">
               - initialize_loggers
              </div>
              <div class="line">
               <br/>
              </div>
              <div class="line">
               training loop:
              </div>
              <div class="line-block">
               <div class="line">
                - update_ready
               </div>
               <div class="line-block">
                <div class="line">
                 - scheduled_update
                </div>
                <div class="line-block">
                 <div class="line">
                  - update
                 </div>
                </div>
               </div>
               <div class="line">
                - loss_update
               </div>
               <div class="line">
                - optimizer_pre_step
               </div>
               <div class="line">
                - optimizer_post_step
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers that can be used by the modifier instance
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers that can be used by the modifier instance
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_start
                 </strong>
                 – The minimum acceptable value for start_epoch, default -1
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_end
                 </strong>
                 – The minimum acceptable value for end_epoch, default 0
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_comparator
                 </strong>
                 – integer value representing how the end_epoch should be
compared to start_epoch.
if == None, then end_epoch can only be set to what its initial value was.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_frequency
                 </strong>
                 – The minimum acceptable value for update_frequency, default -1
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  kwargs
                 </strong>
                 – standard key word args, used to support multi inheritance
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update_ready">
              <span class="sig-name descname">
               <span class="pre">
                update_ready
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier.update_ready">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update_ready" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Calls base implementation to check if start_pending() or end_pending().
Additionally checks if an update is ready based on the frequency and current’
epoch vs last epoch updated.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 True if the modifier is pending an update and update() should be called
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_as">
          <span id="sparseml-pytorch-optim-modifier-as-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_as module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_as" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifiers for increasing / enforcing activation sparsity on models while training.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ASRegModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               alpha
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layer_normalized
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               reg_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'l1'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               reg_tens
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'inp'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Add a regularizer over the inputs or outputs to given layers
(activation regularization).
This promotes larger activation sparsity values.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !ASRegModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                layers:
               </div>
               <div class="line-block">
                <div class="line">
                 - layer1
                </div>
                <div class="line">
                 -layer2
                </div>
               </div>
               <div class="line">
                alpha: 0.00001
               </div>
               <div class="line">
                layer_normalized: True
               </div>
               <div class="line">
                reg_func: l1
               </div>
               <div class="line">
                reg_tens: inp
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  layers
                 </strong>
                 – str or list of str for the layers to apply the AS modifier to
can also use the token __ALL__ to specify all layers
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  alpha
                 </strong>
                 – the weight to use for the regularization,
ie cost = loss + alpha * reg
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  layer_normalized
                 </strong>
                 – True to normalize the values by 1 / L where L
is the number of layers
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  reg_func
                 </strong>
                 – the regularization function to apply to the activations,
one of: l1, l2, relu, hs
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  reg_tens
                 </strong>
                 – the regularization tensor to apply a function to,
one of: inp, out
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.alpha">
              <span class="sig-name descname">
               <span class="pre">
                alpha
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.alpha" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the weight to use for the regularization, ie cost = loss + alpha * reg
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.finalize">
              <span class="sig-name descname">
               <span class="pre">
                finalize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 reset_loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.finalize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.finalize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Clean up any state for tracking activation sparsity
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – The model/module to finalize the modifier for.
Marked optional so state can still be cleaned up on delete,
but generally should always be passed in.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    reset_loggers
                   </strong>
                   – True to remove any currently attached loggers (default),
False to keep the loggers attached.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Grabs the layers to control the activation sparsity for
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.layer_normalized">
              <span class="sig-name descname">
               <span class="pre">
                layer_normalized
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.layer_normalized" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True to normalize the values by 1 / L where L is the number of layers
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.layers">
              <span class="sig-name descname">
               <span class="pre">
                layers
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.layers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               str or list of str for the layers to apply the AS modifier to
can also use the token __ALL__ to specify all layers
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.loss_update">
              <span class="sig-name descname">
               <span class="pre">
                loss_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.loss_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.loss_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Modify the loss to include the norms for the outputs of the layers
being modified.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    loss
                   </strong>
                   – The calculated loss tensor
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the modified loss tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.optimizer_post_step">
              <span class="sig-name descname">
               <span class="pre">
                optimizer_post_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.optimizer_post_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.optimizer_post_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               be sure to clear out the values after the update step has been taken
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_func">
              <span class="sig-name descname">
               <span class="pre">
                reg_func
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_func" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the regularization function to apply to the activations,
one of: l1, l2, relu, hs
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_tens">
              <span class="sig-name descname">
               <span class="pre">
                reg_tens
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_tens" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the regularization tensor to apply a function to,
one of: inp, out
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Update the loss tracking for each layer that is being modified on start and stop
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_as.ASRegModifier.validate">
              <span class="sig-name descname">
               <span class="pre">
                validate
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.validate">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.validate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Validate the values of the params for the current instance are valid
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_epoch">
          <span id="sparseml-pytorch-optim-modifier-epoch-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_epoch module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_epoch" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifiers related to controlling the training epochs while training a model
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_epoch.EpochRangeModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_epoch.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              EpochRangeModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_epoch.html#EpochRangeModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_epoch.EpochRangeModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Simple modifier to set the range of epochs for running in a scheduled optimizer
(ie to set min and max epochs within a range without hacking other modifiers).
            </p>
            <p>
             Note, that if other modifiers exceed the range of this one for min or max epochs,
this modifier will not have an effect.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !EpochRangeModifier:
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0
               </div>
               <div class="line">
                end_epoch: 90
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_lr">
          <span id="sparseml-pytorch-optim-modifier-lr-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_lr module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_lr" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifiers for changing the learning rate while training according to
certain update formulas or patterns.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_lr.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              LearningRateModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               lr_class
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               lr_kwargs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Dict
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_lr
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               constant_logging
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledUpdateModifier
               </span>
              </code>
             </a>
             ,
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.LearningRate" title="sparseml.optim.learning_rate.LearningRate">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.learning_rate.LearningRate
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifier to set the learning rate to specific values at certain points in the
training process between set epochs.
Any time an update point is reached, the LR is updated for the parameters
in the optimizer.
Builds on top of the builtin LR schedulers in PyTorch.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !LearningRateModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                lr_class: ExponentialLR
               </div>
               <div class="line">
                lr_kwargs:
               </div>
               <div class="line-block">
                <div class="line">
                 gamma: 0.95
                </div>
               </div>
               <div class="line">
                init_lr: 0.01
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
               <div class="line">
                constant_logging: True
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  lr_class
                 </strong>
                 – The name of the lr scheduler class to use:
[StepLR, MultiStepLR, ExponentialLR, CosineAnnealingWarmRestarts]
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  lr_kwargs
                 </strong>
                 – The dictionary of keyword arguments to pass to the constructor
for the lr_class
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  init_lr
                 </strong>
                 – The initial learning rate to use once this modifier starts
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
(set to -1.0 so it starts immediately)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at,
(set to -1.0 so it doesn’t end)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  update_frequency
                 </strong>
                 – unused and should not be set
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  constant_logging
                 </strong>
                 – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.constant_logging">
              <span class="sig-name descname">
               <span class="pre">
                constant_logging
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.constant_logging" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True to constantly log on every step,
False to only log on an LR change, default True
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.log_update">
              <span class="sig-name descname">
               <span class="pre">
                log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Check whether to log an update for the learning rate of the modifier
If constant logging is enabled, then will always log
Otherwise checks for a change in the LR before logging
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Calls into the lr scheduler to step given the epoch
Additionally will first set the lr to the init_lr if not set yet
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.validate">
              <span class="sig-name descname">
               <span class="pre">
                validate
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.validate">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.validate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Validate the values of the params for the current instance are valid
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_lr.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              SetLearningRateModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               learning_rate
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               constant_logging
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
             ,
             <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.SetLearningRate" title="sparseml.optim.learning_rate.SetLearningRate">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.optim.learning_rate.SetLearningRate
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifier to set the learning rate to a specific value at a certain point in the
training process.
Once that point is reached,
will update the optimizer’s params with the learning rate.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !SetLearningRateModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                learning_rate: 0.001
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
               <div class="line">
                constant_logging: True
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  learning_rate
                 </strong>
                 – The learning rate to use once this modifier starts
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
(set to -1.0 so it starts immediately)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – unused and should not be set
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  constant_logging
                 </strong>
                 – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.applied_learning_rate">
              <span class="sig-name descname">
               <span class="pre">
                applied_learning_rate
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.applied_learning_rate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.constant_logging">
              <span class="sig-name descname">
               <span class="pre">
                constant_logging
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.constant_logging" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True to constantly log on every step,
False to only log on an LR change, default True
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.log_update">
              <span class="sig-name descname">
               <span class="pre">
                log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier.log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Check whether to log an update for the learning rate of the modifier
If constant logging is enabled, then will always log
Otherwise checks for a change in the LR before logging
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Check whether to update the learning rate for the optimizer or not
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_params">
          <span id="sparseml-pytorch-optim-modifier-params-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_params module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_params" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifier for changing the state of a modules params while training according to
certain update formulas or patterns.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_params.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              GradualParamModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_val
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Any
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               final_val
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Any
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inter_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'linear'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params_strict
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledUpdateModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifier to set the param values for a given list of parameter regex patterns
from a start value through an end value and using an interpolation function
for updates in between.
To set all parameters in the given module, set to the ALL_TOKEN string: __ALL__
            </p>
            <div class="line-block">
             <div class="line">
              Sample YAML:
             </div>
             <div class="line-block">
              <div class="line">
               !GradualParamModifier
              </div>
              <div class="line-block">
               <div class="line">
                params: [“re:.*bias”]
               </div>
               <div class="line">
                init_val: [0.0, 0.0, …]
               </div>
               <div class="line">
                final_val: [1.0, 1.0, …]
               </div>
               <div class="line">
                inter_func: linear
               </div>
               <div class="line">
                params_strict: False
               </div>
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                update_frequency: 1.0
               </div>
              </div>
             </div>
            </div>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.final_val">
              <span class="sig-name descname">
               <span class="pre">
                final_val
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.final_val" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The final value to set for the given param in the given layers at
end_epoch
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.init_val">
              <span class="sig-name descname">
               <span class="pre">
                init_val
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.init_val" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The initial value to set for the given param in the given layers
at start_epoch
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Grab the layers params to control the values for within the given module
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.inter_func">
              <span class="sig-name descname">
               <span class="pre">
                inter_func
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.inter_func" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the type of interpolation function to use:
[linear, cubic, inverse_cubic]; default is linear
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.params">
              <span class="sig-name descname">
               <span class="pre">
                params
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.params" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.params_strict">
              <span class="sig-name descname">
               <span class="pre">
                params_strict
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.params_strict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if every regex pattern in params must match at least
one parameter name in the module
False if missing params are ok – will not raise an err
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Updates the modules layers params to the interpolated value based on given
settings and current epoch.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.validate">
              <span class="sig-name descname">
               <span class="pre">
                validate
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.validate">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.validate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Validate the values of the params for the current instance are valid
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_params.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              SetParamModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               val
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Any
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params_strict
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifier to set the param values for a given list of parameter name regex patterns.
To set all parameters in the given module, set to the ALL_TOKEN string: __ALL__
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !SetParamModifier:
              </div>
              <div class="line-block">
               <div class="line">
                params: [“re:.*bias”]
               </div>
               <div class="line">
                val: [0.1, 0.1, …]
               </div>
               <div class="line">
                params_strict: False
               </div>
               <div class="line">
                start_epoch: 0
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  val
                 </strong>
                 – The value to set for the given param in the given layers at start_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params_strict
                 </strong>
                 – True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
(set to -1.0 so it starts immediately)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – unused and should not be passed
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Grab the layers params to control the values for within the given module
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier.params">
              <span class="sig-name descname">
               <span class="pre">
                params
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.params" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier.params_strict">
              <span class="sig-name descname">
               <span class="pre">
                params_strict
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.params_strict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               If start_pending(), updates the modules layers params to the
value based on given settings.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.SetParamModifier.val">
              <span class="sig-name descname">
               <span class="pre">
                val
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.val" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The value to set for the given param in the given layers at start_epoch
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_params.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              TrainableParamsModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               trainable
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params_strict
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifier to control the params for a given list of parameter regex patterns.
If end_epoch is supplied and greater than 0, then it will revert to the trainable
settings before the modifier.
To set all params in the given layers, set to the ALL_TOKEN string: __ALL__
To set all layers in the given module, set to the ALL_TOKEN string: __ALL__
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !TrainableParamsModifier:
              </div>
              <div class="line-block">
               <div class="line">
                params: [“conv_net.conv1.weight”]
               </div>
               <div class="line">
                trainable: True
               </div>
               <div class="line">
                params_strict: False
               </div>
               <div class="line">
                start_epoch: 0
               </div>
               <div class="line">
                end_epoch: 10
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  trainable
                 </strong>
                 – True if the param(s) should be made trainable,
False to make them non-trainable
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params_strict
                 </strong>
                 – True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
(set to -1.0 so it starts immediately)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at (set to -1.0 so it never ends),
if &gt; 0 then will revert to the original value for the params after this epoch
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Grab the layers params to control trainable or not for within the given module
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params">
              <span class="sig-name descname">
               <span class="pre">
                params
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params_strict">
              <span class="sig-name descname">
               <span class="pre">
                params_strict
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params_strict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if every regex pattern in params must match at least
one parameter name in the module
False if missing params are ok and will not raise an err
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.trainable">
              <span class="sig-name descname">
               <span class="pre">
                trainable
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.trainable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True if the param(s) should be made trainable,
False to make them non-trainable
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               If start_pending(), updates the modules layers params to be trainable or
not depending on given settings.
If end_pending(), updates the modules layers params to their original
trainable state.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_pruning">
          <span id="sparseml-pytorch-optim-modifier-pruning-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_pruning module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_pruning" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifiers for inducing / enforcing kernel sparsity (model pruning)
on models while pruning.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ConstantPruningModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               sparseml.pytorch.optim.modifier_pruning._PruningParamsModifier
              </span>
             </code>
            </p>
            <p>
             Holds the sparsity level and shape for a given parameter(s) constant while training.
Useful for transfer learning use cases.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !ConstantPruningModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                params: [‘re:.*weight’]
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  update_frequency
                 </strong>
                 – Ignored for this modifier
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. __ALL_PRUNABLE__ will match to all ConvNd
and Linear layers’ weights
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.from_sparse_model">
              <em class="property">
               <span class="pre">
                static
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                from_sparse_model
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 model
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.from_sparse_model">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.from_sparse_model" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Create constant ks modifiers for all prunable params in the given model
(conv, linear) that have been artificially sparsified (sparsity &gt; 40%).
Useful for transfer learning from a pruned model.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  model
                 </strong>
                 – the model to create constant ks modifiers for
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the list of created constant ks modifiers
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              GMPruningModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               final_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               leave_enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inter_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'cubic'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               mask_type
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'unstructured'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               global_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               sparseml.pytorch.optim.modifier_pruning._PruningParamsModifier
              </span>
             </code>
            </p>
            <p>
             Gradually applies kernel sparsity to a given parameter or parameters from
init_sparsity until final_sparsity is reached over a given amount of time
and applied with an interpolated function for each step taken.
            </p>
            <p>
             Applies based on magnitude pruning unless otherwise specified by mask_type.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !GMPruningModifier
              </div>
              <div class="line-block">
               <div class="line">
                init_sparsity: 0.05
               </div>
               <div class="line">
                final_sparsity: 0.8
               </div>
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                update_frequency: 1.0
               </div>
               <div class="line">
                params: [“re:.*weight”]
               </div>
               <div class="line">
                leave_enabled: True
               </div>
               <div class="line">
                inter_func: cubic
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
               <div class="line">
                mask_type: unstructured
               </div>
               <div class="line">
                global_sparsity: False
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  init_sparsity
                 </strong>
                 – the initial sparsity for the param to start with at
start_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  final_sparsity
                 </strong>
                 – the final sparsity for the param to end with at end_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  update_frequency
                 </strong>
                 – The number of epochs or fraction of epochs to update at
between start and end
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. __ALL_PRUNABLE__ will match to all ConvNd
and Linear layers’ weights
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  leave_enabled
                 </strong>
                 – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inter_func
                 </strong>
                 – the type of interpolation function to use:
[linear, cubic, inverse_cubic]
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  mask_type
                 </strong>
                 – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameters in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  global_sparsity
                 </strong>
                 – set True to enable global pruning. if False, pruning will
be layer-wise. Default is False
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.applied_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                applied_sparsity
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.applied_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.final_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                final_sparsity
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.final_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the final sparsity for the param to end with at end_epoch
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.global_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                global_sparsity
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.global_sparsity">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.global_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Returns
               </dt>
               <dd class="field-odd">
                <p>
                 True if global pruning is enabled, False otherwise
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.init_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                init_sparsity
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.init_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the initial sparsity for the param to start with at start_epoch
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.inter_func">
              <span class="sig-name descname">
               <span class="pre">
                inter_func
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.inter_func" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the type of interpolation function to use:
[linear, cubic, inverse_cubic]
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.leave_enabled">
              <span class="sig-name descname">
               <span class="pre">
                leave_enabled
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.leave_enabled" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True to continue masking the weights after end_epoch,
False to stop masking. Note, if set as False, sparsity will not be enforced
and the model will likely deviate from the sparse solution
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.mask_type">
              <span class="sig-name descname">
               <span class="pre">
                mask_type
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.mask_type" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the SparsityMaskCreator object used
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.validate">
              <span class="sig-name descname">
               <span class="pre">
                validate
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.validate">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.validate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Validate the values of the params for the current instance are valid
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GlobalMagnitudePruningModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              GlobalMagnitudePruningModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               final_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL_PRUNABLE__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               leave_enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inter_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'cubic'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               mask_type
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'unstructured'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GlobalMagnitudePruningModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GlobalMagnitudePruningModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier" title="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier_pruning.GMPruningModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Gradually applies kernel sparsity to a given parameter or parameters from
init_sparsity until final_sparsity is reached over a given amount of time
and applied with an interpolated function for each step taken.
            </p>
            <p>
             Uses magnitude pruning over the global scope of all given parameters
to gradually mask parameter values. Pruning is unstructured by default,
structure can be specified by mask_type.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !MagnitudePruningModifier
              </div>
              <div class="line-block">
               <div class="line">
                init_sparsity: 0.05
               </div>
               <div class="line">
                final_sparsity: 0.8
               </div>
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                update_frequency: 1.0
               </div>
               <div class="line">
                params: __ALL_PRUNABLE__
               </div>
               <div class="line">
                leave_enabled: True
               </div>
               <div class="line">
                inter_func: cubic
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
               <div class="line">
                mask_type: unstructured
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  init_sparsity
                 </strong>
                 – the initial sparsity for the param to start with at
start_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  final_sparsity
                 </strong>
                 – the final sparsity for the param to end with at end_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  update_frequency
                 </strong>
                 – The number of epochs or fraction of epochs to update at
between start and end
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. __ALL_PRUNABLE__ will match to all ConvNd
and Linear layers’ weights. Defualt is __ALL_PRUNABLE__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  leave_enabled
                 </strong>
                 – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inter_func
                 </strong>
                 – the type of interpolation function to use:
[linear, cubic, inverse_cubic]
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  mask_type
                 </strong>
                 – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameters in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.GlobalMagnitudePruningModifier.global_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                global_sparsity
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GlobalMagnitudePruningModifier.global_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.MagnitudePruningModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              MagnitudePruningModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               final_sparsity
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               update_frequency
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               params
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               leave_enabled
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               inter_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'cubic'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               mask_type
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
               <span class="pre">
                sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               'unstructured'
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#MagnitudePruningModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.MagnitudePruningModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier" title="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier_pruning.GMPruningModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Gradually applies kernel sparsity to a given parameter or parameters from
init_sparsity until final_sparsity is reached over a given amount of time
and applied with an interpolated function for each step taken.
            </p>
            <p>
             Uses magnitude pruning to gradually mask parameter values. Pruning is
unstructured by default, structure can be specified by mask_type.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !MagnitudePruningModifier
              </div>
              <div class="line-block">
               <div class="line">
                init_sparsity: 0.05
               </div>
               <div class="line">
                final_sparsity: 0.8
               </div>
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                end_epoch: 10.0
               </div>
               <div class="line">
                update_frequency: 1.0
               </div>
               <div class="line">
                params: [“re:.*weight”]
               </div>
               <div class="line">
                leave_enabled: True
               </div>
               <div class="line">
                inter_func: cubic
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
               <div class="line">
                mask_type: unstructured
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  init_sparsity
                 </strong>
                 – the initial sparsity for the param to start with at
start_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  final_sparsity
                 </strong>
                 – the final sparsity for the param to end with at end_epoch
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – The epoch to end the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  update_frequency
                 </strong>
                 – The number of epochs or fraction of epochs to update at
between start and end
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  params
                 </strong>
                 – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters. __ALL_PRUNABLE__ will match to all ConvNd
and Linear layers’ weights
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  leave_enabled
                 </strong>
                 – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  inter_func
                 </strong>
                 – the type of interpolation function to use:
[linear, cubic, inverse_cubic]
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  mask_type
                 </strong>
                 – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameters in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_pruning.MagnitudePruningModifier.global_sparsity">
              <span class="sig-name descname">
               <span class="pre">
                global_sparsity
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.MagnitudePruningModifier.global_sparsity" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_quantization">
          <span id="sparseml-pytorch-optim-modifier-quantization-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_quantization module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_quantization" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifier for models through quantization aware training.
          </p>
          <p>
           PyTorch version must support quantization (&gt;=1.2, ONNX export support introduced in 1.7)
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_quantization.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              QuantizationModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               submodules
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model_fuse_fn_name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               disable_quantization_observer_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               None
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               freeze_bn_stats_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               None
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               model_fuse_fn_kwargs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Enables quantization aware training (QAT) for a given module or its submodules
After the start epoch, the specified module(s)’ forward pass will emulate
quantized execution and the modifier will be enabled until training is completed.
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !QuantizationModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                submodules: [‘blocks.0’, ‘blocks.2’]
               </div>
               <div class="line">
                model_fuse_fn_name: ‘fuse_module’
               </div>
               <div class="line">
                disable_quantization_observer_epoch: 2.0
               </div>
               <div class="line">
                freeze_bn_stats_epoch: 3.0
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  submodules
                 </strong>
                 – List of submodule names to perform QAT on. Leave None to quantize
entire model. Default is None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  model_fuse_fn_name
                 </strong>
                 – Name of model function to fuse the model in place prior
to performing QAT.  Set as ‘no_fuse’ to skip module fusing. Leave None to use
the default function
                 <cite>
                  sparseml.pytorch.utils.fuse_module_conv_bn_relus
                 </cite>
                 .
Default is None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  disable_quantization_observer_epoch
                 </strong>
                 – Epoch to disable updates to the module’s
quantization observers. After this point, quantized weights and zero points will
not be updated. Leave None to not disable observers during QAT. Default is None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  freeze_bn_stats_epoch
                 </strong>
                 – Epoch to stop the tracking of batch norm stats. Leave
None to not stop tracking batch norm stats during QAT. Default is None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – Disabled, setting to anything other than -1 will raise an
exception. For compatibility with YAML serialization only.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  model_fuse_fn_kwargs
                 </strong>
                 – dictionary of keyword argument values to be passed
to the model fusing function
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.disable_quantization_observer_epoch">
              <span class="sig-name descname">
               <span class="pre">
                disable_quantization_observer_epoch
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.disable_quantization_observer_epoch" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Epoch to disable updates to the module’s
quantization observers. After this point, quantized weights and zero points will
not be updated. When None, observers never disabled during QAT
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.finalize">
              <span class="sig-name descname">
               <span class="pre">
                finalize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 reset_loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 True
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.finalize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.finalize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Cleans up any state
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – The model/module to finalize the modifier for.
Marked optional so state can still be cleaned up on delete,
but generally should always be passed in.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    reset_loggers
                   </strong>
                   – True to remove any currently attached loggers (default),
False to keep the loggers attached.
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.freeze_bn_stats_epoch">
              <span class="sig-name descname">
               <span class="pre">
                freeze_bn_stats_epoch
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.freeze_bn_stats_epoch" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Epoch to stop the tracking of batch norm stats. When
None, batch norm stats are track for all of training
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.initialize">
              <span class="sig-name descname">
               <span class="pre">
                initialize
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loggers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 Optional
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
                 <span class="pre">
                  sparseml.pytorch.utils.logger.BaseLogger
                 </span>
                </a>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="o">
                <span class="pre">
                 **
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 kwargs
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.initialize">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.initialize" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Grab the module / submodule to perform QAT on
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – the PyTorch model/module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – The epoch to initialize the modifier and module at.
Defaults to 0 (start of the training process)
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    loggers
                   </strong>
                   – Optional list of loggers to log the modification process to
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    kwargs
                   </strong>
                   – Optional kwargs to support specific arguments
for individual modifiers.
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.model_fuse_fn_name">
              <span class="sig-name descname">
               <span class="pre">
                model_fuse_fn_name
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.model_fuse_fn_name" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Name of model function to fuse the model in place prior
to performing QAT. None to uses the default function
               <cite>
                sparseml.pytorch.utils.fuse_module_conv_bn_relus
               </cite>
               .
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.submodules">
              <span class="sig-name descname">
               <span class="pre">
                submodules
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.submodules" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               List of submodule names to perform QAT on. None quantizes the entire
model
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               If start_pending(), fuses the model, sets the model quantization config,
calls torch.quantization.prepare_qat on the model to begin QAT
If end_pending(), updates the modules layers params to their original
trainable state.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update_ready">
              <span class="sig-name descname">
               <span class="pre">
                update_ready
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               bool
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.update_ready">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update_ready" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 True if the modifier is pending an update and update() should be called
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.modifier_regularizer">
          <span id="sparseml-pytorch-optim-modifier-regularizer-module">
          </span>
          <h2>
           sparseml.pytorch.optim.modifier_regularizer module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_regularizer" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Modifier for changing parameters for regularization
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.modifier_regularizer.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              SetWeightDecayModifier
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               weight_decay
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               param_groups
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               end_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               -
              </span>
              <span class="pre">
               1.0
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               log_types
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__ALL__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               constant_logging
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                sparseml.pytorch.optim.modifier.ScheduledModifier
               </span>
              </code>
             </a>
            </p>
            <p>
             Modifies the weight decay (L2 penalty) applied to with an optimizer during training
            </p>
            <div class="line-block">
             <div class="line">
              Sample yaml:
             </div>
             <div class="line-block">
              <div class="line">
               !SetWeightDecayModifier
              </div>
              <div class="line-block">
               <div class="line">
                start_epoch: 0.0
               </div>
               <div class="line">
                weight_decay: 0.0
               </div>
               <div class="line">
                param_groups: [0]
               </div>
               <div class="line">
                log_types: __ALL__
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  weight_decay
                 </strong>
                 – weight decay (L2 penalty) value to set for the given optimizer
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start_epoch
                 </strong>
                 – The epoch to start the modifier at
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  param_groups
                 </strong>
                 – The indices of param groups in the optimizer to be modified.
If None, all param groups will be modified. Default is None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  end_epoch
                 </strong>
                 – unused and should not be set
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  log_types
                 </strong>
                 – The loggers to allow the learning rate to be logged to,
default is __ALL__
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  constant_logging
                 </strong>
                 – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.constant_logging">
              <span class="sig-name descname">
               <span class="pre">
                constant_logging
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.constant_logging" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               True to constantly log on every step,
False to only log on an LR change, default True
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.log_update">
              <span class="sig-name descname">
               <span class="pre">
                log_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier.log_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.log_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Check whether to log an update for the weight decay of the modifier
If constant logging is enabled, then will always log
Otherwise only logs after this modifier makes a change to the weight decay
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.param_groups">
              <span class="sig-name descname">
               <span class="pre">
                param_groups
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.param_groups" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The indices of param groups in the optimizer to be modified.
If None, all param groups will be modified.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.update">
              <span class="sig-name descname">
               <span class="pre">
                update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 module
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.nn.modules.module.Module
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 optimizer
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.optim.optimizer.Optimizer
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 steps_per_epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier.update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               If start_pending(), updates the optimizers weight decay according to the
parameters of this modifier
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    module
                   </strong>
                   – module to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    optimizer
                   </strong>
                   – optimizer to modify
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – current epoch and progress within the current epoch
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    steps_per_epoch
                   </strong>
                   – number of steps taken within each epoch
(calculate batch number using this and epoch)
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py attribute">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.weight_decay">
              <span class="sig-name descname">
               <span class="pre">
                weight_decay
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.weight_decay" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               weight decay (L2 penalty) value to set for the given optimizer
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.optimizer">
          <span id="sparseml-pytorch-optim-optimizer-module">
          </span>
          <h2>
           sparseml.pytorch.optim.optimizer module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.optimizer" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Optimizer wrapper for enforcing Modifiers on the training process of a Module.
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.optimizer.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ScheduledOptimizer
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               optimizer
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.optim.optimizer.Optimizer
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               manager
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <a class="reference internal" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="sparseml.pytorch.optim.manager.ScheduledModifierManager">
               <span class="pre">
                sparseml.pytorch.optim.manager.ScheduledModifierManager
               </span>
              </a>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               steps_per_epoch
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loggers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
               <span class="pre">
                sparseml.pytorch.utils.logger.BaseLogger
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               torch.optim.optimizer.Optimizer
              </span>
             </code>
            </p>
            <p>
             An optimizer wrapper to handle applying modifiers according to their schedule
to both the passed in optimizer and the module.
            </p>
            <p>
             Overrides the step() function so that this method can call before and after on the
modifiers to apply appropriate modifications to both the optimizer and the module.
            </p>
            <p>
             The epoch_start and epoch_end are based on how many steps have been taken
along with the steps_per_epoch.
            </p>
            <div class="line-block">
             <div class="line">
              Lifecycle:
             </div>
             <div class="line-block">
              <div class="line">
               - training cycle
              </div>
              <div class="line-block">
               <div class="line">
                - zero_grad
               </div>
               <div class="line">
                - loss_update
               </div>
               <div class="line-block">
                <div class="line">
                 - modifiers.loss_update
                </div>
               </div>
               <div class="line">
                - step
               </div>
               <div class="line-block">
                <div class="line">
                 - modifiers.update
                </div>
                <div class="line">
                 - modifiers.optimizer_pre_step
                </div>
                <div class="line">
                 - optimizer.step
                </div>
                <div class="line">
                 - modifiers.optimizers_post_step
                </div>
               </div>
              </div>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – module to modify
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  optimizer
                 </strong>
                 – optimizer to modify
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  manager
                 </strong>
                 – the manager or list of managers used to apply modifications
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  steps_per_epoch
                 </strong>
                 – the number of steps or batches in each epoch,
not strictly required and can be set to -1.
used to calculate decimals within the epoch,
when not using can result in irregularities
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loggers
                 </strong>
                 – loggers to log important info to within the modifiers;
ex tensorboard or to the console
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step">
              <span class="sig-name descname">
               <span class="pre">
                adjust_current_step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 epoch
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 step
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 int
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.adjust_current_step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Adjust the current step for the manager’s schedule to the given epoch and step.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    epoch
                   </strong>
                   – the epoch to set the current global step to match
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    step
                   </strong>
                   – the step (batch) within the epoch to set the
current global step to match
                  </p>
                 </li>
                </ul>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                learning_rate
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                float
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               convenience function to get the first learning rate for any of
the param groups in the optimizer
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict">
              <span class="sig-name descname">
               <span class="pre">
                load_manager_state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 state_dict
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.load_manager_state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update">
              <span class="sig-name descname">
               <span class="pre">
                loss_update
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 torch.Tensor
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               torch.Tensor
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.loss_update">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Optional call to update modifiers based on the calculated loss.
Not needed unless one or more of the modifier is using the loss
to make a modification or is modifying the loss itself.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  loss
                 </strong>
                 – the calculated loss after running a forward pass and loss_fn
                </p>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 the modified loss tensor
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                manager
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                sparseml.pytorch.optim.manager.ScheduledModifierManager
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               The ScheduledModifierManager for this optimizer
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict">
              <span class="sig-name descname">
               <span class="pre">
                manager_state_dict
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.manager_state_dict">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step">
              <span class="sig-name descname">
               <span class="pre">
                step
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 closure
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 None
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.step">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Called to perform a step on the optimizer activation normal.
Updates the current epoch based on the step count.
Calls into modifiers before the step happens.
Calls into modifiers after the step happens.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <p>
                 <strong>
                  closure
                 </strong>
                 – optional closure passed into the contained optimizer
for the step
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.sensitivity_as">
          <span id="sparseml-pytorch-optim-sensitivity-as-module">
          </span>
          <h2>
           sparseml.pytorch.optim.sensitivity_as module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_as" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Sensitivity analysis implementations for increasing activation sparsity by using FATReLU
          </p>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ASLayerTracker
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layer
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               track_input
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               track_output
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               input_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               None
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_func
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               None
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             An implementation for tracking activation sparsity properties for a module.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  layer
                 </strong>
                 – the module to track activation sparsity for
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  track_input
                 </strong>
                 – track the input sparsity for the module
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  track_output
                 </strong>
                 – track the output sparsity for the module
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  input_func
                 </strong>
                 – the function to call on input to the layer
and receives the input tensor
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  output_func
                 </strong>
                 – the function to call on output to the layer
and receives the output tensor
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear">
              <span class="sig-name descname">
               <span class="pre">
                clear
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.clear">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Clear out current results for the model
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable">
              <span class="sig-name descname">
               <span class="pre">
                disable
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.disable">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Disable the forward hooks for the layer
              </p>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable">
              <span class="sig-name descname">
               <span class="pre">
                enable
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <span class="sig-paren">
               )
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.enable">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Enable the forward hooks to the layer
              </p>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                tracked_input
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the current tracked input results
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                tracked_output
               </span>
              </span>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the current tracked output results
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              LayerBoostResults
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               name
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               threshold
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               boosted_as
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.Tensor
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               boosted_loss
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults">
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunResults
               </span>
              </a>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               baseline_as
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.Tensor
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               baseline_loss
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults">
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunResults
               </span>
              </a>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#LayerBoostResults">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             Results for a specific threshold set in a FATReLU layer.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  name
                 </strong>
                 – the name of the layer the results are for
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  threshold
                 </strong>
                 – the threshold used in the FATReLU layer
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  boosted_as
                 </strong>
                 – the measured activation sparsity after threshold is applied
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  boosted_loss
                 </strong>
                 – the measured loss after threshold is applied
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  baseline_as
                 </strong>
                 – the measured activation sparsity before threshold is applied
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  baseline_loss
                 </strong>
                 – the measured loss before threshold is applied
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                baseline_as
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the measured activation sparsity before threshold is applied
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                baseline_loss
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunResults
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the measured loss before threshold is applied
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                boosted_as
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                torch.Tensor
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the measured activation sparsity after threshold is applied
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                boosted_loss
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunResults
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the measured loss after threshold is applied
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                name
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                str
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the name of the layer the results are for
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
            <dl class="py property">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold">
              <em class="property">
               <span class="pre">
                property
               </span>
              </em>
              <span class="sig-name descname">
               <span class="pre">
                threshold
               </span>
              </span>
              <em class="property">
               <span class="pre">
                :
               </span>
               <span class="pre">
                float
               </span>
              </em>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               the threshold used in the FATReLU layer
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Type
               </dt>
               <dd class="field-odd">
                <p>
                 return
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py class">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster">
            <em class="property">
             <span class="pre">
              class
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_as.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              ModuleASOneShootBooster
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dataset
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.utils.data.dataset.Dataset
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               batch_size
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loss
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">
               <span class="pre">
                sparseml.pytorch.utils.loss.LossWrapper
               </span>
              </a>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               data_loader_kwargs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Dict
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Bases:
             <code class="xref py py-class docutils literal notranslate">
              <span class="pre">
               object
              </span>
             </code>
            </p>
            <p>
             Implementation class for boosting the activation sparsity in a given module
using FATReLUs.
Programmatically goes through and figures out the best thresholds to limit loss
based on provided parameters.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to boost
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 – the device to run the analysis on; ex [cpu, cuda, cuda:1]
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  dataset
                 </strong>
                 – the dataset used to evaluate the boosting on
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  batch_size
                 </strong>
                 – the batch size to run through the module in test mode
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loss
                 </strong>
                 – the loss function to use for calculations
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  data_loader_kwargs
                 </strong>
                 – any keyword arguments to supply to a the
DataLoader constructor
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <dl class="py method">
             <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers">
              <span class="sig-name descname">
               <span class="pre">
                run_layers
               </span>
              </span>
              <span class="sig-paren">
               (
              </span>
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 layers
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 List
                </span>
                <span class="p">
                 <span class="pre">
                  [
                 </span>
                </span>
                <span class="pre">
                 str
                </span>
                <span class="p">
                 <span class="pre">
                  ]
                 </span>
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 max_target_metric_loss
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 metric_key
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 str
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 metric_increases
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 bool
                </span>
               </span>
              </em>
              ,
              <em class="sig-param">
               <span class="n">
                <span class="pre">
                 precision
                </span>
               </span>
               <span class="p">
                <span class="pre">
                 :
                </span>
               </span>
               <span class="n">
                <span class="pre">
                 float
                </span>
               </span>
               <span class="o">
                <span class="pre">
                 =
                </span>
               </span>
               <span class="default_value">
                <span class="pre">
                 0.001
                </span>
               </span>
              </em>
              <span class="sig-paren">
               )
              </span>
              →
              <span class="pre">
               Dict
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               str
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <a class="reference internal" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults">
               <span class="pre">
                sparseml.pytorch.optim.sensitivity_as.LayerBoostResults
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster.run_layers">
               <span class="viewcode-link">
                <span class="pre">
                 [source]
                </span>
               </span>
              </a>
              <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers" title="Permalink to this definition">
               ¶
              </a>
             </dt>
             <dd>
              <p>
               Run the booster for the specified layers.
              </p>
              <dl class="field-list simple">
               <dt class="field-odd">
                Parameters
               </dt>
               <dd class="field-odd">
                <ul class="simple">
                 <li>
                  <p>
                   <strong>
                    layers
                   </strong>
                   – names of the layers to run boosting on
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    max_target_metric_loss
                   </strong>
                   – the max loss in the target metric that
can happen while boosting
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    metric_key
                   </strong>
                   – the name of the metric to evaluate while boosting;
ex: [__loss__, top1acc, top5acc]. Must exist in the LossWrapper
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    metric_increases
                   </strong>
                   – True if the metric increases for worse loss such as in
a CrossEntropyLoss, False if the metric decreases for worse such as in
accuracy
                  </p>
                 </li>
                 <li>
                  <p>
                   <strong>
                    precision
                   </strong>
                   – the precision to check the results to. Larger values here will
give less precise results but won’t take as long
                  </p>
                 </li>
                </ul>
               </dd>
               <dt class="field-even">
                Returns
               </dt>
               <dd class="field-even">
                <p>
                 The results for the boosting
                </p>
               </dd>
              </dl>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.sensitivity_lr">
          <span id="sparseml-pytorch-optim-sensitivity-lr-module">
          </span>
          <h2>
           sparseml.pytorch.optim.sensitivity_lr module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_lr" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Sensitivity analysis implementations for learning rate on Modules against loss funcs.
          </p>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_lr.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              default_exponential_check_lrs
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               init_lr
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               1e-06
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               final_lr
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               0.5
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               lr_mult
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               float
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               1.1
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <span class="pre">
             Tuple
            </span>
            <span class="p">
             <span class="pre">
              [
             </span>
            </span>
            <span class="pre">
             float
            </span>
            <span class="p">
             <span class="pre">
              ,
             </span>
            </span>
            <span class="p">
             <span class="pre">
              ...
             </span>
            </span>
            <span class="p">
             <span class="pre">
              ]
             </span>
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#default_exponential_check_lrs">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Get the default learning rates to check between init_lr and final_lr.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  init_lr
                 </strong>
                 – the initial learning rate in the returned list
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  final_lr
                 </strong>
                 – the final learning rate in the returned list
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  lr_mult
                 </strong>
                 – the multiplier increase for each step between
init_lr and final_lr
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the list of created lrs that increase exponentially between
init_lr and final_lr according to lr_mult
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_lr.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              lr_loss_sensitivity
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               data
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.utils.data.dataloader.DataLoader
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loss
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">
               <span class="pre">
                sparseml.pytorch.utils.loss.LossWrapper
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               optim
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.optim.optimizer.Optimizer
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               steps_per_measurement
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               check_lrs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Tuple
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ...
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               (1e-06,
              </span>
              <span class="pre">
               1.1e-06,
              </span>
              <span class="pre">
               1.21e-06,
              </span>
              <span class="pre">
               1.3310000000000003e-06,
              </span>
              <span class="pre">
               1.4641000000000003e-06,
              </span>
              <span class="pre">
               1.6105100000000006e-06,
              </span>
              <span class="pre">
               1.7715610000000007e-06,
              </span>
              <span class="pre">
               1.948717100000001e-06,
              </span>
              <span class="pre">
               2.1435888100000012e-06,
              </span>
              <span class="pre">
               2.3579476910000015e-06,
              </span>
              <span class="pre">
               2.5937424601000017e-06,
              </span>
              <span class="pre">
               2.853116706110002e-06,
              </span>
              <span class="pre">
               3.1384283767210024e-06,
              </span>
              <span class="pre">
               3.452271214393103e-06,
              </span>
              <span class="pre">
               3.7974983358324136e-06,
              </span>
              <span class="pre">
               4.177248169415655e-06,
              </span>
              <span class="pre">
               4.594972986357221e-06,
              </span>
              <span class="pre">
               5.0544702849929435e-06,
              </span>
              <span class="pre">
               5.559917313492238e-06,
              </span>
              <span class="pre">
               6.115909044841462e-06,
              </span>
              <span class="pre">
               6.727499949325609e-06,
              </span>
              <span class="pre">
               7.40024994425817e-06,
              </span>
              <span class="pre">
               8.140274938683989e-06,
              </span>
              <span class="pre">
               8.954302432552388e-06,
              </span>
              <span class="pre">
               9.849732675807628e-06,
              </span>
              <span class="pre">
               1.0834705943388392e-05,
              </span>
              <span class="pre">
               1.1918176537727232e-05,
              </span>
              <span class="pre">
               1.3109994191499957e-05,
              </span>
              <span class="pre">
               1.4420993610649954e-05,
              </span>
              <span class="pre">
               1.586309297171495e-05,
              </span>
              <span class="pre">
               1.7449402268886447e-05,
              </span>
              <span class="pre">
               1.9194342495775094e-05,
              </span>
              <span class="pre">
               2.1113776745352607e-05,
              </span>
              <span class="pre">
               2.322515441988787e-05,
              </span>
              <span class="pre">
               2.554766986187666e-05,
              </span>
              <span class="pre">
               2.8102436848064327e-05,
              </span>
              <span class="pre">
               3.091268053287076e-05,
              </span>
              <span class="pre">
               3.4003948586157844e-05,
              </span>
              <span class="pre">
               3.7404343444773634e-05,
              </span>
              <span class="pre">
               4.1144777789251e-05,
              </span>
              <span class="pre">
               4.52592555681761e-05,
              </span>
              <span class="pre">
               4.978518112499371e-05,
              </span>
              <span class="pre">
               5.4763699237493086e-05,
              </span>
              <span class="pre">
               6.02400691612424e-05,
              </span>
              <span class="pre">
               6.626407607736664e-05,
              </span>
              <span class="pre">
               7.289048368510331e-05,
              </span>
              <span class="pre">
               8.017953205361364e-05,
              </span>
              <span class="pre">
               8.819748525897502e-05,
              </span>
              <span class="pre">
               9.701723378487253e-05,
              </span>
              <span class="pre">
               0.00010671895716335979,
              </span>
              <span class="pre">
               0.00011739085287969578,
              </span>
              <span class="pre">
               0.00012912993816766537,
              </span>
              <span class="pre">
               0.00014204293198443192,
              </span>
              <span class="pre">
               0.00015624722518287512,
              </span>
              <span class="pre">
               0.00017187194770116264,
              </span>
              <span class="pre">
               0.00018905914247127894,
              </span>
              <span class="pre">
               0.00020796505671840686,
              </span>
              <span class="pre">
               0.00022876156239024756,
              </span>
              <span class="pre">
               0.00025163771862927233,
              </span>
              <span class="pre">
               0.0002768014904921996,
              </span>
              <span class="pre">
               0.0003044816395414196,
              </span>
              <span class="pre">
               0.00033492980349556157,
              </span>
              <span class="pre">
               0.00036842278384511775,
              </span>
              <span class="pre">
               0.0004052650622296296,
              </span>
              <span class="pre">
               0.0004457915684525926,
              </span>
              <span class="pre">
               0.0004903707252978519,
              </span>
              <span class="pre">
               0.0005394077978276372,
              </span>
              <span class="pre">
               0.000593348577610401,
              </span>
              <span class="pre">
               0.0006526834353714411,
              </span>
              <span class="pre">
               0.0007179517789085853,
              </span>
              <span class="pre">
               0.0007897469567994438,
              </span>
              <span class="pre">
               0.0008687216524793883,
              </span>
              <span class="pre">
               0.0009555938177273272,
              </span>
              <span class="pre">
               0.00105115319950006,
              </span>
              <span class="pre">
               0.001156268519450066,
              </span>
              <span class="pre">
               0.0012718953713950728,
              </span>
              <span class="pre">
               0.0013990849085345801,
              </span>
              <span class="pre">
               0.0015389933993880383,
              </span>
              <span class="pre">
               0.0016928927393268422,
              </span>
              <span class="pre">
               0.0018621820132595267,
              </span>
              <span class="pre">
               0.0020484002145854797,
              </span>
              <span class="pre">
               0.0022532402360440277,
              </span>
              <span class="pre">
               0.0024785642596484307,
              </span>
              <span class="pre">
               0.002726420685613274,
              </span>
              <span class="pre">
               0.0029990627541746015,
              </span>
              <span class="pre">
               0.003298969029592062,
              </span>
              <span class="pre">
               0.0036288659325512686,
              </span>
              <span class="pre">
               0.003991752525806396,
              </span>
              <span class="pre">
               0.0043909277783870364,
              </span>
              <span class="pre">
               0.004830020556225741,
              </span>
              <span class="pre">
               0.005313022611848316,
              </span>
              <span class="pre">
               0.005844324873033148,
              </span>
              <span class="pre">
               0.006428757360336463,
              </span>
              <span class="pre">
               0.00707163309637011,
              </span>
              <span class="pre">
               0.007778796406007121,
              </span>
              <span class="pre">
               0.008556676046607835,
              </span>
              <span class="pre">
               0.009412343651268619,
              </span>
              <span class="pre">
               0.010353578016395481,
              </span>
              <span class="pre">
               0.01138893581803503,
              </span>
              <span class="pre">
               0.012527829399838533,
              </span>
              <span class="pre">
               0.013780612339822387,
              </span>
              <span class="pre">
               0.015158673573804626,
              </span>
              <span class="pre">
               0.01667454093118509,
              </span>
              <span class="pre">
               0.0183419950243036,
              </span>
              <span class="pre">
               0.020176194526733963,
              </span>
              <span class="pre">
               0.02219381397940736,
              </span>
              <span class="pre">
               0.0244131953773481,
              </span>
              <span class="pre">
               0.02685451491508291,
              </span>
              <span class="pre">
               0.029539966406591206,
              </span>
              <span class="pre">
               0.03249396304725033,
              </span>
              <span class="pre">
               0.03574335935197537,
              </span>
              <span class="pre">
               0.03931769528717291,
              </span>
              <span class="pre">
               0.043249464815890204,
              </span>
              <span class="pre">
               0.047574411297479226,
              </span>
              <span class="pre">
               0.052331852427227155,
              </span>
              <span class="pre">
               0.05756503766994987,
              </span>
              <span class="pre">
               0.06332154143694486,
              </span>
              <span class="pre">
               0.06965369558063936,
              </span>
              <span class="pre">
               0.0766190651387033,
              </span>
              <span class="pre">
               0.08428097165257363,
              </span>
              <span class="pre">
               0.092709068817831,
              </span>
              <span class="pre">
               0.10197997569961412,
              </span>
              <span class="pre">
               0.11217797326957554,
              </span>
              <span class="pre">
               0.1233957705965331,
              </span>
              <span class="pre">
               0.13573534765618642,
              </span>
              <span class="pre">
               0.14930888242180507,
              </span>
              <span class="pre">
               0.1642397706639856,
              </span>
              <span class="pre">
               0.18066374773038418,
              </span>
              <span class="pre">
               0.19873012250342262,
              </span>
              <span class="pre">
               0.2186031347537649,
              </span>
              <span class="pre">
               0.2404634482291414,
              </span>
              <span class="pre">
               0.2645097930520556,
              </span>
              <span class="pre">
               0.29096077235726114,
              </span>
              <span class="pre">
               0.3200568495929873,
              </span>
              <span class="pre">
               0.3520625345522861,
              </span>
              <span class="pre">
               0.38726878800751474,
              </span>
              <span class="pre">
               0.4259956668082662,
              </span>
              <span class="pre">
               0.4685952334890929,
              </span>
              <span class="pre">
               0.5154547568380022,
              </span>
              <span class="pre">
               0.5)
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loss_key
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__loss__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               trainer_run_funcs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs">
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunFuncs
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               trainer_loggers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
               <span class="pre">
                sparseml.pytorch.utils.logger.BaseLogger
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               show_progress
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.LRLossSensitivityAnalysis" title="sparseml.optim.sensitivity.LRLossSensitivityAnalysis">
             <span class="pre">
              sparseml.optim.sensitivity.LRLossSensitivityAnalysis
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#lr_loss_sensitivity">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Implementation for handling running sensitivity analysis for
learning rates on modules.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to run the learning rate sensitivity analysis over,
it is expected to already be on the correct device
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  data
                 </strong>
                 – the data to run through the module for calculating
the sensitivity analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loss
                 </strong>
                 – the loss function to use for the sensitivity analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  optim
                 </strong>
                 – the optimizer to run the sensitivity analysis with
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 – the device to run the analysis on; ex: cpu, cuda.
module must already be on that device, this is used to place then data
on that same device.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  steps_per_measurement
                 </strong>
                 – the number of batches to run through for
the analysis at each LR
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  check_lrs
                 </strong>
                 – the learning rates to check for analysis
(will sort them small to large before running)
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loss_key
                 </strong>
                 – the key for the loss function to track in the returned dict
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  trainer_run_funcs
                 </strong>
                 – override functions for ModuleTrainer class
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  trainer_loggers
                 </strong>
                 – loggers to log data to while running the analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  show_progress
                 </strong>
                 – track progress of the runs if True
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               a list of tuples containing the analyzed learning rate at 0
and the ModuleRunResults in 1, ModuleRunResults being a collection
of all the batch results run through the module at that LR
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim.sensitivity_pruning">
          <span id="sparseml-pytorch-optim-sensitivity-pruning-module">
          </span>
          <h2>
           sparseml.pytorch.optim.sensitivity_pruning module
           <a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_pruning" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Sensitivity analysis implementations for kernel sparsity on Modules against loss funcs.
          </p>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              model_prunability_magnitude
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#model_prunability_magnitude">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Calculate the approximate sensitivity for an overall model.
Range of the values are not scaled to anything, so must be taken in context
with other known models.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                module
               </strong>
               – the model to calculate the sensitivity for
              </p>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the approximated sensitivity
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              pruning_loss_sens_magnitude
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               sparsity_levels
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Tuple
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               float
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ...
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               (0.0,
              </span>
              <span class="pre">
               0.01,
              </span>
              <span class="pre">
               0.02,
              </span>
              <span class="pre">
               0.03,
              </span>
              <span class="pre">
               0.04,
              </span>
              <span class="pre">
               0.05,
              </span>
              <span class="pre">
               0.06,
              </span>
              <span class="pre">
               0.07,
              </span>
              <span class="pre">
               0.08,
              </span>
              <span class="pre">
               0.09,
              </span>
              <span class="pre">
               0.1,
              </span>
              <span class="pre">
               0.11,
              </span>
              <span class="pre">
               0.12,
              </span>
              <span class="pre">
               0.13,
              </span>
              <span class="pre">
               0.14,
              </span>
              <span class="pre">
               0.15,
              </span>
              <span class="pre">
               0.16,
              </span>
              <span class="pre">
               0.17,
              </span>
              <span class="pre">
               0.18,
              </span>
              <span class="pre">
               0.19,
              </span>
              <span class="pre">
               0.2,
              </span>
              <span class="pre">
               0.21,
              </span>
              <span class="pre">
               0.22,
              </span>
              <span class="pre">
               0.23,
              </span>
              <span class="pre">
               0.24,
              </span>
              <span class="pre">
               0.25,
              </span>
              <span class="pre">
               0.26,
              </span>
              <span class="pre">
               0.27,
              </span>
              <span class="pre">
               0.28,
              </span>
              <span class="pre">
               0.29,
              </span>
              <span class="pre">
               0.3,
              </span>
              <span class="pre">
               0.31,
              </span>
              <span class="pre">
               0.32,
              </span>
              <span class="pre">
               0.33,
              </span>
              <span class="pre">
               0.34,
              </span>
              <span class="pre">
               0.35,
              </span>
              <span class="pre">
               0.36,
              </span>
              <span class="pre">
               0.37,
              </span>
              <span class="pre">
               0.38,
              </span>
              <span class="pre">
               0.39,
              </span>
              <span class="pre">
               0.4,
              </span>
              <span class="pre">
               0.41,
              </span>
              <span class="pre">
               0.42,
              </span>
              <span class="pre">
               0.43,
              </span>
              <span class="pre">
               0.44,
              </span>
              <span class="pre">
               0.45,
              </span>
              <span class="pre">
               0.46,
              </span>
              <span class="pre">
               0.47,
              </span>
              <span class="pre">
               0.48,
              </span>
              <span class="pre">
               0.49,
              </span>
              <span class="pre">
               0.5,
              </span>
              <span class="pre">
               0.51,
              </span>
              <span class="pre">
               0.52,
              </span>
              <span class="pre">
               0.53,
              </span>
              <span class="pre">
               0.54,
              </span>
              <span class="pre">
               0.55,
              </span>
              <span class="pre">
               0.56,
              </span>
              <span class="pre">
               0.57,
              </span>
              <span class="pre">
               0.58,
              </span>
              <span class="pre">
               0.59,
              </span>
              <span class="pre">
               0.6,
              </span>
              <span class="pre">
               0.61,
              </span>
              <span class="pre">
               0.62,
              </span>
              <span class="pre">
               0.63,
              </span>
              <span class="pre">
               0.64,
              </span>
              <span class="pre">
               0.65,
              </span>
              <span class="pre">
               0.66,
              </span>
              <span class="pre">
               0.67,
              </span>
              <span class="pre">
               0.68,
              </span>
              <span class="pre">
               0.69,
              </span>
              <span class="pre">
               0.7,
              </span>
              <span class="pre">
               0.71,
              </span>
              <span class="pre">
               0.72,
              </span>
              <span class="pre">
               0.73,
              </span>
              <span class="pre">
               0.74,
              </span>
              <span class="pre">
               0.75,
              </span>
              <span class="pre">
               0.76,
              </span>
              <span class="pre">
               0.77,
              </span>
              <span class="pre">
               0.78,
              </span>
              <span class="pre">
               0.79,
              </span>
              <span class="pre">
               0.8,
              </span>
              <span class="pre">
               0.81,
              </span>
              <span class="pre">
               0.82,
              </span>
              <span class="pre">
               0.83,
              </span>
              <span class="pre">
               0.84,
              </span>
              <span class="pre">
               0.85,
              </span>
              <span class="pre">
               0.86,
              </span>
              <span class="pre">
               0.87,
              </span>
              <span class="pre">
               0.88,
              </span>
              <span class="pre">
               0.89,
              </span>
              <span class="pre">
               0.9,
              </span>
              <span class="pre">
               0.91,
              </span>
              <span class="pre">
               0.92,
              </span>
              <span class="pre">
               0.93,
              </span>
              <span class="pre">
               0.94,
              </span>
              <span class="pre">
               0.95,
              </span>
              <span class="pre">
               0.96,
              </span>
              <span class="pre">
               0.97,
              </span>
              <span class="pre">
               0.98,
              </span>
              <span class="pre">
               0.99)
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">
             <span class="pre">
              sparseml.optim.sensitivity.PruningLossSensitivityAnalysis
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_magnitude">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Approximated kernel sparsity (pruning) loss analysis for a given model.
Returns the results for each prunable param (conv, linear) in the model.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the model to calculate the sparse sensitivity analysis for
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  sparsity_levels
                 </strong>
                 – the sparsity levels to calculate the loss for for each param
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the analysis results for the model
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot">
            <span class="sig-prename descclassname">
             <span class="pre">
              sparseml.pytorch.optim.sensitivity_pruning.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              pruning_loss_sens_one_shot
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               module
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.nn.modules.module.Module
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               data
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               torch.utils.data.dataloader.DataLoader
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loss
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Union
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">
               <span class="pre">
                sparseml.pytorch.utils.loss.LossWrapper
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Callable
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               Any
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ,
               </span>
              </span>
              <span class="pre">
               torch.Tensor
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               steps_per_measurement
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               int
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               sparsity_levels
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               int
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               (0.0,
              </span>
              <span class="pre">
               0.2,
              </span>
              <span class="pre">
               0.4,
              </span>
              <span class="pre">
               0.6,
              </span>
              <span class="pre">
               0.7,
              </span>
              <span class="pre">
               0.8,
              </span>
              <span class="pre">
               0.85,
              </span>
              <span class="pre">
               0.9,
              </span>
              <span class="pre">
               0.95,
              </span>
              <span class="pre">
               0.99)
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               loss_key
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               str
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               '__loss__'
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               tester_run_funcs
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs">
               <span class="pre">
                sparseml.pytorch.utils.module.ModuleRunFuncs
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               tester_loggers
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               Optional
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <span class="pre">
               List
              </span>
              <span class="p">
               <span class="pre">
                [
               </span>
              </span>
              <a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.BaseLogger" title="sparseml.pytorch.utils.logger.BaseLogger">
               <span class="pre">
                sparseml.pytorch.utils.logger.BaseLogger
               </span>
              </a>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
              <span class="p">
               <span class="pre">
                ]
               </span>
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               show_progress
              </span>
             </span>
             <span class="p">
              <span class="pre">
               :
              </span>
             </span>
             <span class="n">
              <span class="pre">
               bool
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               True
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            →
            <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">
             <span class="pre">
              sparseml.optim.sensitivity.PruningLossSensitivityAnalysis
             </span>
            </a>
            <a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_one_shot">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Run a one shot sensitivity analysis for kernel sparsity.
It does not retrain, and instead puts the model to eval mode.
Moves layer by layer to calculate the sensitivity analysis for each and
resets the previously run layers.
Note, by default it caches the data.
This means it is not parallel for data loading and the first run can take longer.
Subsequent sparsity checks for layers and levels will be much faster.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  module
                 </strong>
                 – the module to run the kernel sparsity sensitivity analysis over
will extract all prunable layers out
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  data
                 </strong>
                 – the data to run through the module for calculating the sensitivity
analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loss
                 </strong>
                 – the loss function to use for the sensitivity analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 – the device to run the analysis on; ex: cpu, cuda
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  steps_per_measurement
                 </strong>
                 – the number of samples or items to take for each
measurement at each sparsity lev
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  sparsity_levels
                 </strong>
                 – the sparsity levels to check for each layer to calculate
sensitivity
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  loss_key
                 </strong>
                 – the key for the loss function to track in the returned dict
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  tester_run_funcs
                 </strong>
                 – override functions to use in the ModuleTester that runs
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  tester_loggers
                 </strong>
                 – loggers to log data to while running the analysis
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  show_progress
                 </strong>
                 – track progress of the runs if True
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Returns
             </dt>
             <dd class="field-even">
              <p>
               the sensitivity results for every layer that is prunable
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="module-sparseml.pytorch.optim">
          <span id="module-contents">
          </span>
          <h2>
           Module contents
           <a class="headerlink" href="#module-sparseml.pytorch.optim" title="Permalink to this headline">
            ¶
           </a>
          </h2>
          <p>
           Recalibration code for the PyTorch framework.
Handles things like model pruning and increasing activation sparsity.
          </p>
         </div>
        </div>
       </div>
      </div>
      <footer>
       <div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
        <a accesskey="n" class="btn btn-neutral float-right" href="sparseml.pytorch.sparsification.html" rel="next" title="sparseml.pytorch.sparsification package">
         Next
         <span aria-hidden="true" class="fa fa-arrow-circle-right">
         </span>
        </a>
        <a accesskey="p" class="btn btn-neutral float-left" href="sparseml.pytorch.nn.html" rel="prev" title="sparseml.pytorch.nn package">
         <span aria-hidden="true" class="fa fa-arrow-circle-left">
         </span>
         Previous
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the "License").
        </p>
       </div>
       Built with
       <a href="https://www.sphinx-doc.org/">
        Sphinx
       </a>
       using a
       <a href="https://github.com/readthedocs/sphinx_rtd_theme">
        theme
       </a>
       provided by
       <a href="https://readthedocs.org">
        Read the Docs
       </a>
       .
      </footer>
     </div>
    </div>
   </section>
  </div>
  <div aria-label="versions" class="rst-versions" data-toggle="rst-versions" role="note">
   <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book">
     Other Versions
    </span>
    v: v0.3.1
    <span class="fa fa-caret-down">
    </span>
   </span>
   <div class="rst-other-versions">
    <dl>
     <dt>
      Tags
     </dt>
     <dd>
      <a href="../v0.3.0/api/sparseml.pytorch.optim.html">
       v0.3.0
      </a>
     </dd>
     <dd>
      <a href="sparseml.pytorch.optim.html">
       v0.3.1
      </a>
     </dd>
    </dl>
    <dl>
     <dt>
      Branches
     </dt>
     <dd>
      <a href="../main/api/sparseml.pytorch.optim.html">
       main
      </a>
     </dd>
    </dl>
   </div>
  </div>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <!-- Theme Analytics -->
  <script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
  </script>
 </body>
</html>