

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>sparseml.pytorch.optim package &mdash; SparseML 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sparseml.pytorch.optim.quantization package" href="sparseml.pytorch.optim.quantization.html" />
    <link rel="prev" title="sparseml.pytorch.nn package" href="sparseml.pytorch.nn.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> SparseML
          

          
            
            <img src="../_static/icon-sparseml.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quicktour.html">Quick Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes.html">Sparsification Recipes</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="sparseml.html">sparseml package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sparseml.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="sparseml.keras.html">sparseml.keras package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.onnx.html">sparseml.onnx package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.optim.html">sparseml.optim package</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="sparseml.pytorch.html">sparseml.pytorch package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="sparseml.pytorch.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="sparseml.pytorch.html#module-sparseml.pytorch">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.tensorflow_v1.html">sparseml.tensorflow_v1 package</a></li>
<li class="toctree-l3"><a class="reference internal" href="sparseml.utils.html">sparseml.utils package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml.log">sparseml.log module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.html#module-sparseml">Module contents</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Help</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/issues">Bugs, Feature Requests</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/neuralmagic/sparseml/discussions">Support, General Q&amp;A</a></li>
<li class="toctree-l1"><a class="reference external" href="https://docs.neuralmagic.com">Neural Magic Docs</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SparseML</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="sparseml.html">sparseml package</a> &raquo;</li>
        
          <li><a href="sparseml.pytorch.html">sparseml.pytorch package</a> &raquo;</li>
        
      <li>sparseml.pytorch.optim package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api/sparseml.pytorch.optim.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="sparseml-pytorch-optim-package">
<h1>sparseml.pytorch.optim package<a class="headerlink" href="#sparseml-pytorch-optim-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sparseml.pytorch.optim.quantization.html">sparseml.pytorch.optim.quantization package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sparseml.pytorch.optim.quantization.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.pytorch.optim.quantization.html#module-sparseml.pytorch.optim.quantization.helpers">sparseml.pytorch.optim.quantization.helpers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.pytorch.optim.quantization.html#module-sparseml.pytorch.optim.quantization.quantize_qat_export">sparseml.pytorch.optim.quantization.quantize_qat_export module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sparseml.pytorch.optim.quantization.html#module-sparseml.pytorch.optim.quantization">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_as">
<span id="sparseml-pytorch-optim-analyzer-as-module"></span><h2>sparseml.pytorch.optim.analyzer_as module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_as" title="Permalink to this headline">¶</a></h2>
<p>Code related to analyzing activation sparsity within PyTorch neural networks.
More information can be found in the paper
<a class="reference external" href="https://arxiv.org/abs/1705.01626">here</a>.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.analyzer_as.</code><code class="sig-name descname">ASResultType</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ASResultType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Result type to track for activation sparsity.</p>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample">
<code class="sig-name descname">inputs_sample</code><em class="property"> = 'inputs_sample'</em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity">
<code class="sig-name descname">inputs_sparsity</code><em class="property"> = 'inputs_sparsity'</em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.inputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample">
<code class="sig-name descname">outputs_sample</code><em class="property"> = 'outputs_sample'</em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity">
<code class="sig-name descname">outputs_sparsity</code><em class="property"> = 'outputs_sparsity'</em><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ASResultType.outputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.analyzer_as.</code><code class="sig-name descname">ModuleASAnalyzer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>Tuple<span class="p">[</span>int<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">track_inputs_sparsity</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">track_outputs_sparsity</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">inputs_sample_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">outputs_sample_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation used to monitor the activation sparsity with a module.
Generally used to monitor an individual layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – The module to analyze activation sparsity for</p></li>
<li><p><strong>dim</strong> – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel</p></li>
<li><p><strong>track_inputs_sparsity</strong> – True to track the input sparsity to the module,
False otherwise</p></li>
<li><p><strong>track_outputs_sparsity</strong> – True to track the output sparsity to the module,
False otherwise</p></li>
<li><p><strong>inputs_sample_size</strong> – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>outputs_sample_size</strong> – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers">
<em class="property">static </em><code class="sig-name descname">analyze_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">layers</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>Tuple<span class="p">[</span>int<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">track_inputs_sparsity</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">track_outputs_sparsity</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">inputs_sample_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">outputs_sample_size</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.analyze_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.analyze_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to analyze multiple layers activation sparsity in</p></li>
<li><p><strong>layers</strong> – the names of the layers to analyze (from module.named_modules())</p></li>
<li><p><strong>dim</strong> – Any dims within the tensor such as across batch,
channel, etc. Ex: 0 for batch, 1 for channel, [0, 1] for batch and channel</p></li>
<li><p><strong>track_inputs_sparsity</strong> – True to track the input sparsity to the module,
False otherwise</p></li>
<li><p><strong>track_outputs_sparsity</strong> – True to track the output sparsity to the module,
False otherwise</p></li>
<li><p><strong>inputs_sample_size</strong> – The number of samples to grab from the input tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>outputs_sample_size</strong> – The number of samples to grab from the output tensor
on each forward pass. If &lt;= 0, then will not sample any values.</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of the created analyzers, matches the ordering in layers</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">specific_result_type</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.clear" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim">
<em class="property">property </em><code class="sig-name descname">dim</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable">
<code class="sig-name descname">disable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.disable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.disable" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable">
<code class="sig-name descname">enable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.enable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enable" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled">
<em class="property">property </em><code class="sig-name descname">enabled</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.enabled" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample">
<em class="property">property </em><code class="sig-name descname">inputs_sample</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max">
<em class="property">property </em><code class="sig-name descname">inputs_sample_max</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean">
<em class="property">property </em><code class="sig-name descname">inputs_sample_mean</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min">
<em class="property">property </em><code class="sig-name descname">inputs_sample_min</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size">
<em class="property">property </em><code class="sig-name descname">inputs_sample_size</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std">
<em class="property">property </em><code class="sig-name descname">inputs_sample_std</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sample_std" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity">
<em class="property">property </em><code class="sig-name descname">inputs_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max">
<em class="property">property </em><code class="sig-name descname">inputs_sparsity_max</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean">
<em class="property">property </em><code class="sig-name descname">inputs_sparsity_mean</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min">
<em class="property">property </em><code class="sig-name descname">inputs_sparsity_min</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std">
<em class="property">property </em><code class="sig-name descname">inputs_sparsity_std</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.inputs_sparsity_std" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module">
<em class="property">property </em><code class="sig-name descname">module</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.module" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample">
<em class="property">property </em><code class="sig-name descname">outputs_sample</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max">
<em class="property">property </em><code class="sig-name descname">outputs_sample_max</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean">
<em class="property">property </em><code class="sig-name descname">outputs_sample_mean</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min">
<em class="property">property </em><code class="sig-name descname">outputs_sample_min</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size">
<em class="property">property </em><code class="sig-name descname">outputs_sample_size</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std">
<em class="property">property </em><code class="sig-name descname">outputs_sample_std</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sample_std" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity">
<em class="property">property </em><code class="sig-name descname">outputs_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max">
<em class="property">property </em><code class="sig-name descname">outputs_sparsity_max</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean">
<em class="property">property </em><code class="sig-name descname">outputs_sparsity_mean</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min">
<em class="property">property </em><code class="sig-name descname">outputs_sparsity_min</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std">
<em class="property">property </em><code class="sig-name descname">outputs_sparsity_std</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.outputs_sparsity_std" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results">
<code class="sig-name descname">results</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">result_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a></span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max">
<code class="sig-name descname">results_max</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">result_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_max"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_max" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean">
<code class="sig-name descname">results_mean</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">result_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_mean"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_mean" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min">
<code class="sig-name descname">results_min</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">result_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_min"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_min" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std">
<code class="sig-name descname">results_std</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">result_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.analyzer_as.ASResultType" title="sparseml.pytorch.optim.analyzer_as.ASResultType">sparseml.pytorch.optim.analyzer_as.ASResultType</a></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_as.html#ModuleASAnalyzer.results_std"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.results_std" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity">
<em class="property">property </em><code class="sig-name descname">track_inputs_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_inputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity">
<em class="property">property </em><code class="sig-name descname">track_outputs_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_as.ModuleASAnalyzer.track_outputs_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_module">
<span id="sparseml-pytorch-optim-analyzer-module-module"></span><h2>sparseml.pytorch.optim.analyzer_module module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_module" title="Permalink to this headline">¶</a></h2>
<p>Code related to monitoring, analyzing, and reporting info for Modules in PyTorch.
Records things like FLOPS, input and output shapes, kernel shapes, etc.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.analyzer_module.</code><code class="sig-name descname">ModuleAnalyzer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation for monitoring the execution profile and graph of
a Module in PyTorch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to analyze</p></li>
<li><p><strong>enabled</strong> – True to enable the hooks for analyzing and actively track,
False to disable and not track</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled">
<em class="property">property </em><code class="sig-name descname">enabled</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.enabled" title="Permalink to this definition">¶</a></dt>
<dd><p>True if enabled and the hooks for analyzing are active, False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs">
<code class="sig-name descname">ks_layer_descs</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc">sparseml.optim.analyzer.AnalyzedLayerDesc</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.ks_layer_descs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.ks_layer_descs" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the descriptions for all layers in the module that support kernel sparsity
(model pruning). Ex: all convolutions and linear layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a list of descriptions for all layers in the module that support ks</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc">
<code class="sig-name descname">layer_desc</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.analyzer.AnalyzedLayerDesc" title="sparseml.optim.analyzer.AnalyzedLayerDesc">sparseml.optim.analyzer.AnalyzedLayerDesc</a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_module.html#ModuleAnalyzer.layer_desc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.layer_desc" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a specific layer’s description within the Module.
Set to None to get the overall Module’s description.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> – name of the layer to get a description for,
None for an overall description</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analyzed layer description for the given name</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module">
<em class="property">property </em><code class="sig-name descname">module</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_module.ModuleAnalyzer.module" title="Permalink to this definition">¶</a></dt>
<dd><p>The module that is being actively analyzed</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.analyzer_pruning">
<span id="sparseml-pytorch-optim-analyzer-pruning-module"></span><h2>sparseml.pytorch.optim.analyzer_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.analyzer_pruning" title="Permalink to this headline">¶</a></h2>
<p>Code related to monitoring, analyzing, and reporting the kernel sparsity
(model pruning) for a model’s layers and params.
More info on kernel sparsity can be found <cite>here &lt;https://arxiv.org/abs/1902.09574&gt;</cite> __.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.analyzer_pruning.</code><code class="sig-name descname">ModulePruningAnalyzer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">param_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'weight'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An analyzer implementation monitoring the kernel sparsity of a given
param in a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module containing the param to analyze the sparsity for</p></li>
<li><p><strong>name</strong> – name of the layer, used for tracking</p></li>
<li><p><strong>param_name</strong> – name of the parameter to analyze the sparsity for,
defaults to weight</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers">
<em class="property">static </em><code class="sig-name descname">analyze_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">layers</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">param_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'weight'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.analyze_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.analyze_layers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to create multiple analyzers for</p></li>
<li><p><strong>layers</strong> – the names of the layers to create analyzer for that are
in the module</p></li>
<li><p><strong>param_name</strong> – the name of the param to monitor within each layer</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of analyzers, one for each layer passed in and in the same order</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module">
<em class="property">property </em><code class="sig-name descname">module</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.module" title="Permalink to this definition">¶</a></dt>
<dd><p>the module containing the param to analyze the sparsity for</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.name" title="Permalink to this definition">¶</a></dt>
<dd><p>name of the layer, used for tracking</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param">
<em class="property">property </em><code class="sig-name descname">param</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param" title="Permalink to this definition">¶</a></dt>
<dd><p>the parameter that is being monitored for kernel sparsity</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name">
<em class="property">property </em><code class="sig-name descname">param_name</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_name" title="Permalink to this definition">¶</a></dt>
<dd><p>name of the parameter to analyze the sparsity for, defaults to weight</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity">
<em class="property">property </em><code class="sig-name descname">param_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>the sparsity of the contained parameter (how many zeros are in it)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim">
<code class="sig-name descname">param_sparsity_dim</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>int<span class="p">, </span>Tuple<span class="p">[</span>int<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/analyzer_pruning.html#ModulePruningAnalyzer.param_sparsity_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.param_sparsity_dim" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dim</strong> – a dimension(s) to calculate the sparsity over, ex over channels</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sparsity of the contained parameter structured according
to the dim passed in</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag">
<em class="property">property </em><code class="sig-name descname">tag</code><a class="headerlink" href="#sparseml.pytorch.optim.analyzer_pruning.ModulePruningAnalyzer.tag" title="Permalink to this definition">¶</a></dt>
<dd><p>combines the layer name and param name in to a single string
separated by a period</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.manager">
<span id="sparseml-pytorch-optim-manager-module"></span><h2>sparseml.pytorch.optim.manager module<a class="headerlink" href="#module-sparseml.pytorch.optim.manager" title="Permalink to this headline">¶</a></h2>
<p>Contains base code related to modifier managers: modifier managers handle
grouping modifiers and running them together.
Also handles loading modifiers from yaml files</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.manager.</code><code class="sig-name descname">ScheduledModifierManager</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">modifiers</span><span class="p">:</span> <span class="n">List<span class="p">[</span><a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">sparseml.pytorch.optim.modifier.ScheduledModifier</a><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.manager.BaseManager" title="sparseml.optim.manager.BaseManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.manager.BaseManager</span></code></a>, <a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.Modifier</span></code></a></p>
<p>The base modifier manager, handles managing multiple ScheduledModifers.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- initialize</div>
<div class="line">- initialize_loggers</div>
<div class="line"><br /></div>
<div class="line">training loop:</div>
<div class="line-block">
<div class="line">- update_ready</div>
<div class="line-block">
<div class="line">- scheduled_update</div>
<div class="line-block">
<div class="line">- update</div>
</div>
</div>
<div class="line">- scheduled_log_update</div>
<div class="line-block">
<div class="line">- log_update</div>
</div>
<div class="line">- loss_update</div>
<div class="line">- optimizer_pre_step</div>
<div class="line">- optimizer_post_step</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>modifiers</strong> – the modifiers to wrap</p>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml">
<em class="property">static </em><code class="sig-name descname">from_yaml</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">file_path</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>sparsezoo.objects.optimization_recipe.OptimizationRecipe<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">add_modifiers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier">sparseml.pytorch.optim.modifier.Modifier</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.from_yaml"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.from_yaml" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function used to create the manager of multiple modifiers from a
recipe file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> – the path to the recipe file to load the modifier from, or
a SparseZoo model stub to load a recipe for a model stored in SparseZoo.
SparseZoo stubs should be preceded by ‘zoo:’, and can contain an optional
‘?recipe_type=&lt;type&gt;’ parameter. Can also be a SparseZoo OptimizationRecipe
object. i.e. ‘/path/to/local/recipe.yaml’, ‘zoo:model/stub/path’,
‘zoo:model/stub/path?recipe_type=transfer’</p></li>
<li><p><strong>add_modifiers</strong> – additional modifiers that should be added to the
returned manager alongside the ones loaded from the recipe file</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ScheduledModifierManager() created from the recipe file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles initializing and setting up the contained modifiers
Called once on construction of the scheduled optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers">
<code class="sig-name descname">initialize_loggers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loggers</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger">sparseml.pytorch.utils.logger.PyTorchLogger</a><span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.initialize_loggers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.initialize_loggers" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles initializing and setting up the loggers for the contained modifiers
Called once on construction of the scheduled optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loggers</strong> – the loggers to setup this modifier with for logging important
info and milestones to</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Dict<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the given state dict into this object’s modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> – dictionary object as generated by this object’s state_dict
function</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If any keys in the state dict do not correspond to a valid
index in this manager’s modifier list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update">
<code class="sig-name descname">loss_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.loss_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.loss_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Optional call that can be made on the optimizer to update the contained
modifiers once loss has been calculated</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The calculated loss tensor</p></li>
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step">
<code class="sig-name descname">optimizer_post_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_post_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_post_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after the optimizer step happens and weights have updated
Calls into the contained modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step">
<code class="sig-name descname">optimizer_pre_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.optimizer_pre_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.optimizer_pre_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before the optimizer step happens (after backward has been called,
before optimizer.step)
Calls into the contained modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>Dict<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary to store any state variables from this Manager’s Modifiers.
Only Modifiers with a state_dict function will be included. The mapping
is modifier_idx -&gt; modifier.state_dict(). If no modifiers have a state
dict, an empty dictionary is returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.manager.ScheduledModifierManager.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">log_updates</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#ScheduledModifierManager.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles updating the contained modifiers’ states, module, or optimizer
Only calls scheduled_update on the each modifier if modifier.update_ready()</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
<li><p><strong>log_updates</strong> – True to log the updates for each modifier to the loggers,
False to skip logging</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.manager.load_manager">
<code class="sig-prename descclassname">sparseml.pytorch.optim.manager.</code><code class="sig-name descname">load_manager</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">path</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">manager</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="sparseml.pytorch.optim.manager.ScheduledModifierManager">sparseml.pytorch.optim.manager.ScheduledModifierManager</a></span></em>, <em class="sig-param"><span class="n">map_location</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'cpu'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/manager.html#load_manager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.manager.load_manager" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the state dict into a ScheduledModifierManager from a given file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – the path to the pth file to load the state dict from</p></li>
<li><p><strong>manager</strong> – the optimizer to load the state dict into</p></li>
<li><p><strong>map_location</strong> – the location to map the values to when loading</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.mask_creator_pruning">
<span id="sparseml-pytorch-optim-mask-creator-pruning-module"></span><h2>sparseml.pytorch.optim.mask_creator_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.mask_creator_pruning" title="Permalink to this headline">¶</a></h2>
<p>Classes for defining sparsity masks based on model parameters.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">BlockPruningMaskCreator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">block_shape</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">grouping_fn_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups the input tensor into blocks of
shape block_shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_shape</strong> – The shape in and out channel should take in blocks.  Should be
a list of exactly two integers that divide the input tensors evenly on the
channel dimensions.  -1 for a dimension blocks across the entire dimension</p></li>
<li><p><strong>grouping_fn_name</strong> – The name of the torch grouping function to reduce
dimensions by</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor">
<code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#BlockPruningMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.BlockPruningMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The absolute mean values of the tensor grouped by blocks of
shape self._block_shape</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">DimensionSparsityMaskCreator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dim</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">grouping_fn_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'mean'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator</span></code></a></p>
<p>Structured sparsity mask creator that groups sparsity blocks by the given
dimension(s)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> – The index or list of indices of dimensions to group the mask by or
the type of dims to prune ([‘channel’, ‘filter’])</p></li>
<li><p><strong>grouping_fn_name</strong> – The name of the torch grouping function to reduce
dimensions by</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor">
<code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#DimensionSparsityMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.DimensionSparsityMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to transform</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The absolute mean values of the tensor grouped by the
dimension(s) in self._dim</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">GroupedPruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator</span></code></a></p>
<p>Abstract class for a sparsity mask creator that structures masks according to
grouping functions.  Subclasses should implement group_tensor and
_map_mask_to_tensor</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask">
<code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tens such that the desired number of zeros
matches the sparsity and all values mapped to the same group have the
same value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask_from_abs_threshold">
<code class="sig-name descname">create_sparsity_mask_from_abs_threshold</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_mask_from_abs_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask_from_abs_threshold" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained
values</p></li>
<li><p><strong>threshold</strong> – a threshold of group_tensor values to determine cutoff
for sparsification</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask derived from the tensor and the grouped threshold</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask_from_tensor">
<code class="sig-name descname">create_sparsity_mask_from_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.create_sparsity_mask_from_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.create_sparsity_mask_from_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – the tensor to calculate a mask based on its values</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask derived from the values of tensor grouped by the group_tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_grouping_fn">
<em class="property">static </em><code class="sig-name descname">get_grouping_fn</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">grouping_fn_name</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; Callable<span class="p">[</span><span class="p">[</span>torch.Tensor<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.get_grouping_fn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.get_grouping_fn" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grouping_fn_name</strong> – name of grouping function to get torch function for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>torch function for grouping_fn_name if available,
raises error otherwise</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor">
<em class="property">abstract </em><code class="sig-name descname">group_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#GroupedPruningMaskCreator.group_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.GroupedPruningMaskCreator.group_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – The tensor to reduce in groups</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The grouped tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">PruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Base abstract class for a sparsity mask creator.
Subclasses should define all methods for creating masks</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask">
<em class="property">abstract </em><code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tens such that the desired number of zeros
matches the sparsity.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask_from_abs_threshold">
<em class="property">abstract </em><code class="sig-name descname">create_sparsity_mask_from_abs_threshold</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_mask_from_abs_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask_from_abs_threshold" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained
values</p></li>
<li><p><strong>threshold</strong> – a threshold to determine cutoff for sparsification</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask derived from the tensor and the threshold</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask_from_tensor">
<code class="sig-name descname">create_sparsity_mask_from_tensor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#PruningMaskCreator.create_sparsity_mask_from_tensor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator.create_sparsity_mask_from_tensor" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> – the tensor to calculate a mask based on its values</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask derived from the values of tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">UnstructuredPruningMaskCreator</code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</span></code></a></p>
<p>Class for creating unstructured sparsity masks.
Masks will be created using unstructured sparsity by pruning weights ranked
by their magnitude.</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask">
<code class="sig-name descname">create_sparsity_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained values</p></li>
<li><p><strong>sparsity</strong> – the desired sparsity to reach within the mask
(decimal fraction of zeros)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tens such that the desired number of zeros
matches the sparsity. removes the abs lowest values if there are more zeros
in the tens than desired sparsity, then will randomly choose the zeros</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask_from_abs_threshold">
<code class="sig-name descname">create_sparsity_mask_from_abs_threshold</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tensor</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#UnstructuredPruningMaskCreator.create_sparsity_mask_from_abs_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.UnstructuredPruningMaskCreator.create_sparsity_mask_from_abs_threshold" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – the tensor to calculate a mask from based on the contained values</p></li>
<li><p><strong>threshold</strong> – a threshold at which to mask abs(values) if they are
less than it or equal</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a mask (0.0 for values that are masked, 1.0 for values that are
unmasked) calculated from the tens abs(values) &lt;= threshold are masked,
all others are unmasked</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator">
<code class="sig-prename descclassname">sparseml.pytorch.optim.mask_creator_pruning.</code><code class="sig-name descname">load_mask_creator</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">obj</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>Iterable<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_creator_pruning.html#load_mask_creator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_creator_pruning.load_mask_creator" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> – Formatted string or block shape iterable specifying SparsityMaskCreator
object to return</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>SparsityMaskCreator object created from obj</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.mask_pruning">
<span id="sparseml-pytorch-optim-mask-pruning-module"></span><h2>sparseml.pytorch.optim.mask_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.mask_pruning" title="Permalink to this headline">¶</a></h2>
<p>Code related to applying a mask onto a parameter to impose kernel sparsity,
aka model pruning</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.mask_pruning.</code><code class="sig-name descname">ModuleParamPruningMask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">param_name</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'weight'</span></em>, <em class="sig-param"><span class="n">store_init</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">store_unmasked</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">track_grad_mom</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">mask_creator</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</a></span> <span class="o">=</span> <span class="default_value">unstructured</span></em>, <em class="sig-param"><span class="n">layer_name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Mask to apply kernel sparsity (model pruning) to a specific parameter in a layer</p>
<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>apply the current mask to the params tensor (zero out the desired values)</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled">
<em class="property">property </em><code class="sig-name descname">enabled</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.enabled" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the parameter is currently being masked, False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer">
<em class="property">property </em><code class="sig-name descname">layer</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer" title="Permalink to this definition">¶</a></dt>
<dd><p>the layer containing the parameter to mask</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_name">
<em class="property">property </em><code class="sig-name descname">layer_name</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.layer_name" title="Permalink to this definition">¶</a></dt>
<dd><p>the name of the layer the parameter to mask is located in</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator">
<em class="property">property </em><code class="sig-name descname">mask_creator</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.mask_creator" title="Permalink to this definition">¶</a></dt>
<dd><p>SparsityMaskCreator object used to generate masks</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.name" title="Permalink to this definition">¶</a></dt>
<dd><p>the full name of this sparsity mask in the following format:
&lt;LAYER&gt;.&lt;PARAM&gt;.sparsity_mask</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_data">
<em class="property">property </em><code class="sig-name descname">param_data</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_data" title="Permalink to this definition">¶</a></dt>
<dd><p>the current tensor in the parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_grad">
<em class="property">property </em><code class="sig-name descname">param_grad</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>the current gradient values for the parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_init">
<em class="property">property </em><code class="sig-name descname">param_init</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_init" title="Permalink to this definition">¶</a></dt>
<dd><p>the initial value of the parameter before being masked</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_mask">
<em class="property">property </em><code class="sig-name descname">param_mask</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_mask" title="Permalink to this definition">¶</a></dt>
<dd><p>the current mask applied to the parameter</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_name">
<em class="property">property </em><code class="sig-name descname">param_name</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_name" title="Permalink to this definition">¶</a></dt>
<dd><p>the name of the parameter to mask in the layer, default is weight</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_unmasked">
<em class="property">property </em><code class="sig-name descname">param_unmasked</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.param_unmasked" title="Permalink to this definition">¶</a></dt>
<dd><p>the unmasked value of the parameter
(stores the last unmasked value before masking)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset">
<code class="sig-name descname">reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.reset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.reset" title="Permalink to this definition">¶</a></dt>
<dd><p>resets the current stored tensors such that they will be on the same device
and have the proper data</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data">
<code class="sig-name descname">set_param_data</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_data" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> – the value to set as the current tensor for the parameter,
if enabled the mask will be applied</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask">
<code class="sig-name descname">set_param_mask</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>value</strong> – the mask to set and apply as the current tensor,
if enabled mask is applied immediately</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_abs_threshold">
<code class="sig-name descname">set_param_mask_from_abs_threshold</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_mask_from_abs_threshold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_abs_threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function to set the parameter mask such that if
abs(value) &lt;= threshold the it is masked to 0</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>threshold</strong> – the threshold at which all values will be masked to 0</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_sparsity">
<code class="sig-name descname">set_param_mask_from_sparsity</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sparsity</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_mask_from_sparsity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function to set the parameter mask such that it has a specific
amount of masked values such that the percentage equals the sparsity amount
given. Masks the absolute smallest values up until sparsity is reached.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparsity</strong> – the decimal sparsity to set the param mask to</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_weights">
<code class="sig-name descname">set_param_mask_from_weights</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/mask_pruning.html#ModuleParamPruningMask.set_param_mask_from_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.set_param_mask_from_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function to set the parameter mask such that the
mask is 1 if the parameter value is non zero and 0 otherwise,
unless otherwise defined by this object’s mask_creator.</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init">
<em class="property">property </em><code class="sig-name descname">store_init</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_init" title="Permalink to this definition">¶</a></dt>
<dd><p>store the init weights in a separate variable that can be used and
referenced later</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked">
<em class="property">property </em><code class="sig-name descname">store_unmasked</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.store_unmasked" title="Permalink to this definition">¶</a></dt>
<dd><p>store the unmasked weights in a separate variable that can be used and
referenced later</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom">
<em class="property">property </em><code class="sig-name descname">track_grad_mom</code><a class="headerlink" href="#sparseml.pytorch.optim.mask_pruning.ModuleParamPruningMask.track_grad_mom" title="Permalink to this definition">¶</a></dt>
<dd><p>store the gradient updates to the parameter with a momentum variable
must be in the range [0.0, 1.0), if set to 0.0 then will only
keep most recent</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier">
<span id="sparseml-pytorch-optim-modifier-module"></span><h2>sparseml.pytorch.optim.modifier module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier" title="Permalink to this headline">¶</a></h2>
<p>Contains base code related to modifiers: objects that modify some aspect
of the training process for a model.
For example, learning rate schedules or kernel sparsity (weight pruning)
are implemented as modifiers.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier.Modifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier.</code><code class="sig-name descname">Modifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseModifier" title="sparseml.optim.modifier.BaseModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseModifier</span></code></a></p>
<p>The base pytorch modifier implementation,
all modifiers must inherit from this class.
It defines common things needed for the lifecycle and implementation of a modifier.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- initialize</div>
<div class="line">- initialize_loggers</div>
<div class="line"><br /></div>
<div class="line">training loop:</div>
<div class="line-block">
<div class="line">- update</div>
<div class="line">- log_update</div>
<div class="line">- loss_update</div>
<div class="line">- optimizer_pre_step</div>
<div class="line">- optimizer_post_step</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – The loggers that can be used by the modifier instance</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles initializing and setting up the modifier.
Called once on construction of the scheduled optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.initialize_loggers">
<code class="sig-name descname">initialize_loggers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loggers</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>List<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger">sparseml.pytorch.utils.logger.PyTorchLogger</a><span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.initialize_loggers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.initialize_loggers" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loggers</strong> – the loggers to setup this modifier with for logging important
info and milestones to</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.load_list">
<em class="property">static </em><code class="sig-name descname">load_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">yaml_str</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.load_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.load_list" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>yaml_str</strong> – a string representation of the yaml syntax to
load modifiers from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the loaded modifiers list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.load_obj">
<em class="property">static </em><code class="sig-name descname">load_obj</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">yaml_str</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.load_obj"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.load_obj" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>yaml_str</strong> – a string representation of the yaml syntax to
load a modifier from</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the loaded modifier object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles logging updates for the modifier for better tracking and visualization.
Should be overwritten for logging.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier.Modifier.loggers">
<code class="sig-name descname">loggers</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loggers" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier.Modifier.loggers_initialized">
<code class="sig-name descname">loggers_initialized</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loggers_initialized" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.loss_update">
<code class="sig-name descname">loss_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.loss_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.loss_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Optional call that can be made on the optimizer to update the modifiers
once the loss has been calculated.
Called independent of if the modifier is currently active or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The calculated loss tensor</p></li>
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.optimizer_post_step">
<code class="sig-name descname">optimizer_post_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.optimizer_post_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.optimizer_post_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after the optimizer step happens and weights have updated.
Called independent of if the modifier is currently active or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.optimizer_pre_step">
<code class="sig-name descname">optimizer_pre_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.optimizer_pre_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.optimizer_pre_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before the optimizer step happens
(after backward has been called, before optimizer.step).
Called independent of if the modifier is currently active or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.Modifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#Modifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.Modifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier.</code><code class="sig-name descname">ModifierProp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">serializable</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">restrict_initialized</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">restrict_enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">restrict_extras</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">no_serialize_val</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Any<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">func_get</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">func_set</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">doc</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseProp</span></code></a></p>
<p>Property used to decorate a modifier.
Use for creating getters and setters in a modifier.
Handles making sure props cannot be changed after a certain point;
ex after initialized.
Also, marks the properties so they can be easily collected and serialized later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>serializable</strong> – True if the property should be serialized (ex in yaml),
False otherwise. Default True</p></li>
<li><p><strong>restrict_initialized</strong> – True to keep the property from being set after
initialized, False otherwise. Default True</p></li>
<li><p><strong>restrict_enabled</strong> – True to keep the property from being set after enabled,
False otherwise. Default False</p></li>
<li><p><strong>restrict_extras</strong> – extra attributes to check, if any are truthy then keep
from being set. Default None</p></li>
<li><p><strong>no_serialize_val</strong> – If prop is equal to this value, will not serialize the prop</p></li>
<li><p><strong>func_get</strong> – The function getter</p></li>
<li><p><strong>func_set</strong> – The function setter</p></li>
<li><p><strong>doc</strong> – The docs function</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp.getter">
<code class="sig-name descname">getter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func_get</span><span class="p">:</span> <span class="n">Callable</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">sparseml.optim.modifier.BaseProp</a><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.getter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.getter" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a ModifierProp based off the current instance with the getter function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func_get</strong> – the getter function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the recreated instance with the new getter function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp.no_serialize_val">
<em class="property">property </em><code class="sig-name descname">no_serialize_val</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.no_serialize_val" title="Permalink to this definition">¶</a></dt>
<dd><p>a value that if the prop is equal to, will not serialize the prop</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp.restrictions">
<em class="property">property </em><code class="sig-name descname">restrictions</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.restrictions" title="Permalink to this definition">¶</a></dt>
<dd><p>The attributes to check for restricting when the attribute can be set</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp.serializable">
<em class="property">property </em><code class="sig-name descname">serializable</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.serializable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the property should be serialized (ex in yaml), False otherwise</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ModifierProp.setter">
<code class="sig-name descname">setter</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">func_set</span><span class="p">:</span> <span class="n">Callable</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseProp" title="sparseml.optim.modifier.BaseProp">sparseml.optim.modifier.BaseProp</a><a class="reference internal" href="../_modules/sparseml/optim/modifier.html#ModifierProp.setter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ModifierProp.setter" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a ModifierProp based off the current instance with the setter function</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>func_set</strong> – the setter function</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the recreated instance with the new setter function</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier.PyTorchModifierYAML">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier.</code><code class="sig-name descname">PyTorchModifierYAML</code><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#PyTorchModifierYAML"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.PyTorchModifierYAML" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.ModifierYAML" title="sparseml.optim.modifier.ModifierYAML"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.ModifierYAML</span></code></a></p>
<p>A decorator to handle making a pytorch modifier class YAML ready.
IE it can be loaded in through the yaml plugin easily.</p>
</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier.</code><code class="sig-name descname">ScheduledModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_start</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_end</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_comparator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.Modifier" title="sparseml.pytorch.optim.modifier.Modifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.Modifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseScheduled" title="sparseml.optim.modifier.BaseScheduled"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseScheduled</span></code></a></p>
<p>The base scheduled modifier implementation,
all scheduled modifiers must inherit from this class.
The difference for this and a Modifier is that these have start and end epochs.
It defines common things needed for the lifecycle and implementation of a
scheduled modifier.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- initialize</div>
<div class="line">- initialize_loggers</div>
<div class="line"><br /></div>
<div class="line">training loop:</div>
<div class="line-block">
<div class="line">- update_ready</div>
<div class="line-block">
<div class="line">- scheduled_update</div>
<div class="line-block">
<div class="line">- update</div>
</div>
</div>
<div class="line">- scheduled_log_update</div>
<div class="line-block">
<div class="line">- log_update</div>
</div>
<div class="line">- loss_update</div>
<div class="line">- optimizer_pre_step</div>
<div class="line">- optimizer_post_step</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – The loggers that can be used by the modifier instance</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>log_types</strong> – The loggers that can be used by the modifier instance</p></li>
<li><p><strong>min_start</strong> – The minimum acceptable value for start_epoch, default -1</p></li>
<li><p><strong>min_end</strong> – The minimum acceptable value for end_epoch, default 0</p></li>
<li><p><strong>end_comparator</strong> – integer value representing how the end_epoch should be
compared to start_epoch.
if == None, then end_epoch can only be set to what its initial value was.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.end_pending">
<code class="sig-name descname">end_pending</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.end_pending"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.end_pending" title="Permalink to this definition">¶</a></dt>
<dd><p>Base implementation compares current epoch with the end epoch and
that it has been started.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the modifier is ready to stop modifying, false otherwise</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.ended">
<code class="sig-name descname">ended</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.ended" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles logging updates for the modifier for better tracking and visualization.
Should be overridden for logging but not called directly,
use scheduled_log_update instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_log_update">
<code class="sig-name descname">scheduled_log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.scheduled_log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles checking if a log update should happen.
IE, is the modifier currently in the range of its start and end epochs.
No restrictions are placed on it by update_ready in the event that the modifier
should log constantly or outside of an update being ready.
General use case is checking if logs should happen by comparing
cached values with updated values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_update">
<code class="sig-name descname">scheduled_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.scheduled_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.scheduled_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by the system and calls into update() method
Tracks state and should not be overridden!!</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.start_pending">
<code class="sig-name descname">start_pending</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.start_pending"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.start_pending" title="Permalink to this definition">¶</a></dt>
<dd><p>Base implementation compares current epoch with the start epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the modifier is ready to begin modifying, false otherwise</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.started">
<code class="sig-name descname">started</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.started" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledModifier.update_ready">
<code class="sig-name descname">update_ready</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledModifier.update_ready"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledModifier.update_ready" title="Permalink to this definition">¶</a></dt>
<dd><p>Base implementation checks if start_pending() or end_pending().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the modifier is pending an update and update() should be called</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier.</code><code class="sig-name descname">ScheduledUpdateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_start</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_end</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_comparator</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">min_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.modifier.BaseUpdate" title="sparseml.optim.modifier.BaseUpdate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.modifier.BaseUpdate</span></code></a></p>
<p>The base scheduled update modifier implementation,
all scheduled update modifiers must inherit from this class.
The difference for this and a ScheduledModifier is that these have a certain
interval that they update within the start and end ranges.
It defines common things needed for the lifecycle and implementation of a scheduled
update modifier.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- initialize</div>
<div class="line">- initialize_loggers</div>
<div class="line"><br /></div>
<div class="line">training loop:</div>
<div class="line-block">
<div class="line">- update_ready</div>
<div class="line-block">
<div class="line">- scheduled_update</div>
<div class="line-block">
<div class="line">- update</div>
</div>
</div>
<div class="line">- loss_update</div>
<div class="line">- optimizer_pre_step</div>
<div class="line">- optimizer_post_step</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>log_types</strong> – The loggers that can be used by the modifier instance</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>log_types</strong> – The loggers that can be used by the modifier instance</p></li>
<li><p><strong>min_start</strong> – The minimum acceptable value for start_epoch, default -1</p></li>
<li><p><strong>min_end</strong> – The minimum acceptable value for end_epoch, default 0</p></li>
<li><p><strong>end_comparator</strong> – integer value representing how the end_epoch should be
compared to start_epoch.
if == None, then end_epoch can only be set to what its initial value was.
if == -1, then end_epoch can be less than, equal, or greater than start_epoch.
if == 0, then end_epoch can be equal to or greater than start_epoch.
if == 1, then end_epoch can only be greater than start_epoch.</p></li>
<li><p><strong>min_frequency</strong> – The minimum acceptable value for update_frequency, default -1</p></li>
<li><p><strong>kwargs</strong> – standard key word args, used to support multi inheritance</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles updating the modifier’s state, module, or optimizer.
Called when update_ready() returns True.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update_ready">
<code class="sig-name descname">update_ready</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier.html#ScheduledUpdateModifier.update_ready"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier.update_ready" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls base implementation to check if start_pending() or end_pending().
Additionally checks if an update is ready based on the frequency and current’
epoch vs last epoch updated.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the modifier is pending an update and update() should be called</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_as">
<span id="sparseml-pytorch-optim-modifier-as-module"></span><h2>sparseml.pytorch.optim.modifier_as module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_as" title="Permalink to this headline">¶</a></h2>
<p>Modifiers for increasing / enforcing activation sparsity on models while training.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_as.</code><code class="sig-name descname">ASRegModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layers</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">alpha</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">layer_normalized</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">reg_func</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'l1'</span></em>, <em class="sig-param"><span class="n">reg_tens</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'inp'</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Add a regularizer over the inputs or outputs to given layers
(activation regularization).
This promotes larger activation sparsity values.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!ASRegModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">layers:</div>
<div class="line-block">
<div class="line">- layer1</div>
<div class="line">-layer2</div>
</div>
<div class="line">alpha: 0.00001</div>
<div class="line">layer_normalized: True</div>
<div class="line">reg_func: l1</div>
<div class="line">reg_tens: inp</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – str or list of str for the layers to apply the AS modifier to
can also use the token __ALL__ to specify all layers</p></li>
<li><p><strong>alpha</strong> – the weight to use for the regularization,
ie cost = loss + alpha * reg</p></li>
<li><p><strong>layer_normalized</strong> – True to normalize the values by 1 / L where L
is the number of layers</p></li>
<li><p><strong>reg_func</strong> – the regularization function to apply to the activations,
one of: l1, l2, relu, hs</p></li>
<li><p><strong>reg_tens</strong> – the regularization tensor to apply a function to,
one of: inp, out</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.alpha">
<code class="sig-name descname">alpha</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.alpha" title="Permalink to this definition">¶</a></dt>
<dd><p>the weight to use for the regularization, ie cost = loss + alpha * reg</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the layer’s to control activation sparsity for</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.layer_normalized">
<code class="sig-name descname">layer_normalized</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.layer_normalized" title="Permalink to this definition">¶</a></dt>
<dd><p>True to normalize the values by 1 / L where L is the number of layers</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.layers">
<code class="sig-name descname">layers</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.layers" title="Permalink to this definition">¶</a></dt>
<dd><p>str or list of str for the layers to apply the AS modifier to
can also use the token __ALL__ to specify all layers</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.loss_update">
<code class="sig-name descname">loss_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.loss_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.loss_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Modify the loss to include the norms for the outputs of the layers
being modified.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> – The calculated loss tensor</p></li>
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.optimizer_post_step">
<code class="sig-name descname">optimizer_post_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.optimizer_post_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.optimizer_post_step" title="Permalink to this definition">¶</a></dt>
<dd><p>be sure to clear out the values after the update step has been taken</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_func">
<code class="sig-name descname">reg_func</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_func" title="Permalink to this definition">¶</a></dt>
<dd><p>the regularization function to apply to the activations,
one of: l1, l2, relu, hs</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_tens">
<code class="sig-name descname">reg_tens</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.reg_tens" title="Permalink to this definition">¶</a></dt>
<dd><p>the regularization tensor to apply a function to,
one of: inp, out</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the loss tracking for each layer that is being modified on start and stop</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_as.ASRegModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_as.html#ASRegModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_as.ASRegModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_epoch">
<span id="sparseml-pytorch-optim-modifier-epoch-module"></span><h2>sparseml.pytorch.optim.modifier_epoch module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_epoch" title="Permalink to this headline">¶</a></h2>
<p>Modifiers related to controlling the training epochs while training a model</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_epoch.EpochRangeModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_epoch.</code><code class="sig-name descname">EpochRangeModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_epoch.html#EpochRangeModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_epoch.EpochRangeModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Simple modifier to set the range of epochs for running in a scheduled optimizer
(ie to set min and max epochs within a range without hacking other modifiers).</p>
<p>Note, that if other modifiers exceed the range of this one for min or max epochs,
this modifier will not have an effect.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!EpochRangeModifier:</div>
<div class="line-block">
<div class="line">start_epoch: 0</div>
<div class="line">end_epoch: 90</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_lr">
<span id="sparseml-pytorch-optim-modifier-lr-module"></span><h2>sparseml.pytorch.optim.modifier_lr module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_lr" title="Permalink to this headline">¶</a></h2>
<p>Modifiers for changing the learning rate while training according to
certain update formulas or patterns.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_lr.</code><code class="sig-name descname">LearningRateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr_class</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">lr_kwargs</span><span class="p">:</span> <span class="n">Dict</span></em>, <em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em>, <em class="sig-param"><span class="n">constant_logging</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledUpdateModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.LearningRate" title="sparseml.optim.learning_rate.LearningRate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.learning_rate.LearningRate</span></code></a></p>
<p>Modifier to set the learning rate to specific values at certain points in the
training process between set epochs.
Any time an update point is reached, the LR is updated for the parameters
in the optimizer.
Builds on top of the builtin LR schedulers in PyTorch.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!LearningRateModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">lr_class: ExponentialLR</div>
<div class="line">lr_kwargs:</div>
<div class="line-block">
<div class="line">gamma: 0.95</div>
</div>
<div class="line">init_lr: 0.01</div>
<div class="line">log_types: __ALL__</div>
<div class="line">constant_logging: True</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr_class</strong> – The name of the lr scheduler class to use:
[StepLR, MultiStepLR, ExponentialLR, CosineAnnealingWarmRestarts]</p></li>
<li><p><strong>lr_kwargs</strong> – The dictionary of keyword arguments to pass to the constructor
for the lr_class</p></li>
<li><p><strong>init_lr</strong> – The initial learning rate to use once this modifier starts</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at
(set to -1.0 so it starts immediately)</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at,
(set to -1.0 so it doesn’t end)</p></li>
<li><p><strong>update_frequency</strong> – unused and should not be set</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
<li><p><strong>constant_logging</strong> – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.constant_logging">
<code class="sig-name descname">constant_logging</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.constant_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>True to constantly log on every step,
False to only log on an LR change, default True</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to log an update for the learning rate of the modifier
If constant logging is enabled, then will always log
Otherwise checks for a change in the LR before logging</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Calls into the lr scheduler to step given the epoch
Additionally will first set the lr to the init_lr if not set yet</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_lr.LearningRateModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#LearningRateModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.LearningRateModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_lr.</code><code class="sig-name descname">SetLearningRateModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em>, <em class="sig-param"><span class="n">constant_logging</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a>, <a class="reference internal" href="sparseml.optim.html#sparseml.optim.learning_rate.SetLearningRate" title="sparseml.optim.learning_rate.SetLearningRate"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.optim.learning_rate.SetLearningRate</span></code></a></p>
<p>Modifier to set the learning rate to a specific value at a certain point in the
training process.
Once that point is reached,
will update the optimizer’s params with the learning rate.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!SetLearningRateModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">learning_rate: 0.001</div>
<div class="line">log_types: __ALL__</div>
<div class="line">constant_logging: True</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The learning rate to use once this modifier starts</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at
(set to -1.0 so it starts immediately)</p></li>
<li><p><strong>end_epoch</strong> – unused and should not be set</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
<li><p><strong>constant_logging</strong> – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.applied_learning_rate">
<code class="sig-name descname">applied_learning_rate</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.applied_learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.constant_logging">
<code class="sig-name descname">constant_logging</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.constant_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>True to constantly log on every step,
False to only log on an LR change, default True</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to log an update for the learning rate of the modifier
If constant logging is enabled, then will always log
Otherwise checks for a change in the LR before logging</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_lr.html#SetLearningRateModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_lr.SetLearningRateModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to update the learning rate for the optimizer or not</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_params">
<span id="sparseml-pytorch-optim-modifier-params-module"></span><h2>sparseml.pytorch.optim.modifier_params module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_params" title="Permalink to this headline">¶</a></h2>
<p>Modifier for changing the state of a modules params while training according to
certain update formulas or patterns.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_params.</code><code class="sig-name descname">GradualParamModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">init_val</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">final_val</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">inter_func</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'linear'</span></em>, <em class="sig-param"><span class="n">params_strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledUpdateModifier</span></code></a></p>
<p>Modifier to set the param values for a given list of parameter regex patterns
from a start value through an end value and using an interpolation function
for updates in between.
To set all parameters in the given module, set to the ALL_TOKEN string: __ALL__</p>
<div class="line-block">
<div class="line">Sample YAML:</div>
<div class="line-block">
<div class="line">!GradualParamModifier</div>
<div class="line-block">
<div class="line">params: [“re:.*bias”]</div>
<div class="line">init_val: [0.0, 0.0, …]</div>
<div class="line">final_val: [1.0, 1.0, …]</div>
<div class="line">inter_func: linear</div>
<div class="line">params_strict: False</div>
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">update_frequency: 1.0</div>
</div>
</div>
</div>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.final_val">
<code class="sig-name descname">final_val</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.final_val" title="Permalink to this definition">¶</a></dt>
<dd><p>The final value to set for the given param in the given layers at
end_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.init_val">
<code class="sig-name descname">init_val</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.init_val" title="Permalink to this definition">¶</a></dt>
<dd><p>The initial value to set for the given param in the given layers
at start_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the layers params to control the values for within the given module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.inter_func">
<code class="sig-name descname">inter_func</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.inter_func" title="Permalink to this definition">¶</a></dt>
<dd><p>the type of interpolation function to use:
[linear, cubic, inverse_cubic]; default is linear</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.params_strict">
<code class="sig-name descname">params_strict</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.params_strict" title="Permalink to this definition">¶</a></dt>
<dd><p>True if every regex pattern in params must match at least
one parameter name in the module
False if missing params are ok – will not raise an err</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the modules layers params to the interpolated value based on given
settings and current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.GradualParamModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#GradualParamModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.GradualParamModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_params.</code><code class="sig-name descname">SetParamModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">val</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="n">params_strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Modifier to set the param values for a given list of parameter name regex patterns.
To set all parameters in the given module, set to the ALL_TOKEN string: __ALL__</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!SetParamModifier:</div>
<div class="line-block">
<div class="line">params: [“re:.*bias”]</div>
<div class="line">val: [0.1, 0.1, …]</div>
<div class="line">params_strict: False</div>
<div class="line">start_epoch: 0</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p></li>
<li><p><strong>val</strong> – The value to set for the given param in the given layers at start_epoch</p></li>
<li><p><strong>params_strict</strong> – True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at
(set to -1.0 so it starts immediately)</p></li>
<li><p><strong>end_epoch</strong> – unused and should not be passed</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the layers params to control the values for within the given module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier.params_strict">
<code class="sig-name descname">params_strict</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.params_strict" title="Permalink to this definition">¶</a></dt>
<dd><p>True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#SetParamModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>If start_pending(), updates the modules layers params to the
value based on given settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.SetParamModifier.val">
<code class="sig-name descname">val</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.SetParamModifier.val" title="Permalink to this definition">¶</a></dt>
<dd><p>The value to set for the given param in the given layers at start_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_params.</code><code class="sig-name descname">TrainableParamsModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">trainable</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">params_strict</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Modifier to control the params for a given list of parameter regex patterns.
If end_epoch is supplied and greater than 0, then it will revert to the trainable
settings before the modifier.
To set all params in the given layers, set to the ALL_TOKEN string: __ALL__
To set all layers in the given module, set to the ALL_TOKEN string: __ALL__</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!TrainableParamsModifier:</div>
<div class="line-block">
<div class="line">params: [“conv_net.conv1.weight”]</div>
<div class="line">trainable: True</div>
<div class="line">params_strict: False</div>
<div class="line">start_epoch: 0</div>
<div class="line">end_epoch: 10</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p></li>
<li><p><strong>trainable</strong> – True if the param(s) should be made trainable,
False to make them non-trainable</p></li>
<li><p><strong>params_strict</strong> – True if every regex pattern in params must match at least
one parameter name in the module,
False if missing params are ok and will not raise an err</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at
(set to -1.0 so it starts immediately)</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at (set to -1.0 so it never ends),
if &gt; 0 then will revert to the original value for the params after this epoch</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the layers params to control trainable or not for within the given module</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params_strict">
<code class="sig-name descname">params_strict</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.params_strict" title="Permalink to this definition">¶</a></dt>
<dd><p>True if every regex pattern in params must match at least
one parameter name in the module
False if missing params are ok and will not raise an err</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.trainable">
<code class="sig-name descname">trainable</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.trainable" title="Permalink to this definition">¶</a></dt>
<dd><p>True if the param(s) should be made trainable,
False to make them non-trainable</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_params.html#TrainableParamsModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_params.TrainableParamsModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>If start_pending(), updates the modules layers params to be trainable or
not depending on given settings.
If end_pending(), updates the modules layers params to their original
trainable state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_pruning">
<span id="sparseml-pytorch-optim-modifier-pruning-module"></span><h2>sparseml.pytorch.optim.modifier_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_pruning" title="Permalink to this headline">¶</a></h2>
<p>Modifiers for inducing / enforcing kernel sparsity (model pruning)
on models while pruning.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_pruning.</code><code class="sig-name descname">ConstantPruningModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Holds the sparsity level and shape for a given parameter(s) constant while training.
Useful for transfer learning use cases.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!ConstantPruningModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">params: [‘re:.*weight’]</div>
<div class="line">log_types: __ALL__</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>params</strong> – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.from_sparse_model">
<em class="property">static </em><code class="sig-name descname">from_sparse_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span><a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier">sparseml.pytorch.optim.modifier.ScheduledModifier</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.from_sparse_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.from_sparse_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Create constant ks modifiers for all prunable params in the given model
(conv, linear) that have been artificially sparsified (sparsity &gt; 40%).
Useful for transfer learning from a pruned model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> – the model to create constant ks modifiers for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the list of created constant ks modifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the params to control kernel sparsity for.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the given state dict into this object’s modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> – dictionary object as generated by this object’s state_dict
function</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If any keys in the state dict do not correspond to a valid
parameter in this modifier or if this modifier has not been initialized</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to log an update for the learning rate of the modifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.optimizer_post_step">
<code class="sig-name descname">optimizer_post_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.optimizer_post_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.optimizer_post_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Reapply the mask after the optimizer step in case the optimizer
has momentum that may have moved weights from 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary to store the masks currently created by this object. The
mapping is param_name -&gt; mask</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#ConstantPruningModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.ConstantPruningModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update to enable and disable the mask when chosen.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_pruning.</code><code class="sig-name descname">GMPruningModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">init_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">final_sparsity</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">update_frequency</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">params</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">leave_enabled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">inter_func</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'cubic'</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em>, <em class="sig-param"><span class="n">mask_type</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>int<span class="p">]</span><span class="p">, </span><a class="reference internal" href="#sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator" title="sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator">sparseml.pytorch.optim.mask_creator_pruning.PruningMaskCreator</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'unstructured'</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledUpdateModifier" title="sparseml.pytorch.optim.modifier.ScheduledUpdateModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledUpdateModifier</span></code></a></p>
<p>Gradually applies kernel sparsity to a given parameter or parameters from
init_sparsity until final_sparsity is reached over a given amount of time
and applied with an interpolated function for each step taken.</p>
<p>Applies based on magnitude pruning unless otherwise specified by mask_type.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!GMPruningModifier</div>
<div class="line-block">
<div class="line">init_sparsity: 0.05</div>
<div class="line">final_sparsity: 0.8</div>
<div class="line">start_epoch: 0.0</div>
<div class="line">end_epoch: 10.0</div>
<div class="line">update_frequency: 1.0</div>
<div class="line">params: [“re:.*weight”]</div>
<div class="line">leave_enabled: True</div>
<div class="line">inter_func: cubic</div>
<div class="line">log_types: __ALL__</div>
<div class="line">mask_type: unstructured</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_sparsity</strong> – the initial sparsity for the param to start with at
start_epoch</p></li>
<li><p><strong>final_sparsity</strong> – the final sparsity for the param to end with at end_epoch</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>end_epoch</strong> – The epoch to end the modifier at</p></li>
<li><p><strong>update_frequency</strong> – The number of epochs or fraction of epochs to update at
between start and end</p></li>
<li><p><strong>params</strong> – A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p></li>
<li><p><strong>leave_enabled</strong> – True to continue masking the weights after end_epoch,
False to stop masking. Should be set to False if exporting the result
immediately after or doing some other prune</p></li>
<li><p><strong>inter_func</strong> – the type of interpolation function to use:
[linear, cubic, inverse_cubic]</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
<li><p><strong>mask_type</strong> – String to define type of sparsity (options: [‘unstructured’,
‘channel’, ‘filter’]), List to define block shape of a parameters in and out
channels, or a SparsityMaskCreator object. default is ‘unstructured’</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.applied_sparsity">
<code class="sig-name descname">applied_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.applied_sparsity" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.final_sparsity">
<code class="sig-name descname">final_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.final_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>the final sparsity for the param to end with at end_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.init_sparsity">
<code class="sig-name descname">init_sparsity</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.init_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>the initial sparsity for the param to start with at start_epoch</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the params to control kernel sparsity for</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.inter_func">
<code class="sig-name descname">inter_func</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.inter_func" title="Permalink to this definition">¶</a></dt>
<dd><p>the type of interpolation function to use:
[linear, cubic, inverse_cubic]</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.leave_enabled">
<code class="sig-name descname">leave_enabled</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.leave_enabled" title="Permalink to this definition">¶</a></dt>
<dd><p>True to continue masking the weights after end_epoch,
False to stop masking. Note, if set as False, sparsity will not be enforced
and the model will likely deviate from the sparse solution</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the given state dict into this object’s modifiers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> – dictionary object as generated by this object’s state_dict
function</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – If any keys in the state dict do not correspond to a valid
parameter in this modifier or if this modifier has not been initialized</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to log an update for the learning rate of the modifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.mask_type">
<code class="sig-name descname">mask_type</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.mask_type" title="Permalink to this definition">¶</a></dt>
<dd><p>the SparsityMaskCreator object used</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.optimizer_post_step">
<code class="sig-name descname">optimizer_post_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.optimizer_post_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.optimizer_post_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Reapply the mask after the optimizer step in case the optimizer has momentum
that may have moved weights from 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.params">
<code class="sig-name descname">params</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.params" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of full parameter names or regex patterns of names to apply
pruning to.  Regex patterns must be specified with the prefix ‘re:’. __ALL__
will match to all parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span>torch.Tensor<span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Dictionary to store the masks currently created by this object. The
mapping is param_name -&gt; mask</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the sparsity mask for the selected parameters.
If start, enables the masks.
If end, disables the masks if leave_enabled is False.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_pruning.html#GMPruningModifier.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_pruning.GMPruningModifier.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Validate the values of the params for the current instance are valid</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_quantization">
<span id="sparseml-pytorch-optim-modifier-quantization-module"></span><h2>sparseml.pytorch.optim.modifier_quantization module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_quantization" title="Permalink to this headline">¶</a></h2>
<p>Modifier for models through quantization aware training.</p>
<p>PyTorch version must support quantization (&gt;=1.2, ONNX export support introduced in 1.7)</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_quantization.</code><code class="sig-name descname">QuantizationModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">submodules</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">model_fuse_fn_name</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">disable_quantization_observer_epoch</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">freeze_bn_stats_epoch</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">model_fuse_fn_kwargs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Enables quantization aware training (QAT) for a given module or its submodules
After the start epoch, the specified module(s)’ forward pass will emulate
quantized execution and the modifier will be enabled until training is completed.</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!QuantizationModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">submodules: [‘blocks.0’, ‘blocks.2’]</div>
<div class="line">model_fuse_fn_name: ‘fuse_module’</div>
<div class="line">disable_quantization_observer_epoch: 2.0</div>
<div class="line">freeze_bn_stats_epoch: 3.0</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>submodules</strong> – List of submodule names to perform QAT on. Leave None to quantize
entire model. Default is None</p></li>
<li><p><strong>model_fuse_fn_name</strong> – Name of model function to fuse the model in place prior
to performing QAT.  Set as ‘no_fuse’ to skip module fusing. Leave None to use
the default function <cite>sparseml.pytorch.utils.fuse_module_conv_bn_relus</cite>.
Default is None</p></li>
<li><p><strong>disable_quantization_observer_epoch</strong> – Epoch to disable updates to the module’s
quantization observers. After this point, quantized weights and zero points will
not be updated. Leave None to not disable observers during QAT. Default is None</p></li>
<li><p><strong>freeze_bn_stats_epoch</strong> – Epoch to stop the tracking of batch norm stats. Leave
None to not stop tracking batch norm stats during QAT. Default is None</p></li>
<li><p><strong>end_epoch</strong> – Disabled, setting to anything other than -1 will raise an
exception. For compatibility with YAML serialization only.</p></li>
<li><p><strong>model_fuse_fn_kwargs</strong> – dictionary of keyword argument values to be passed
to the model fusing function</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.disable_quantization_observer_epoch">
<code class="sig-name descname">disable_quantization_observer_epoch</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.disable_quantization_observer_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>Epoch to disable updates to the module’s
quantization observers. After this point, quantized weights and zero points will
not be updated. When None, observers never disabled during QAT</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.freeze_bn_stats_epoch">
<code class="sig-name descname">freeze_bn_stats_epoch</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.freeze_bn_stats_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>Epoch to stop the tracking of batch norm stats. When
None, batch norm stats are track for all of training</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.initialize">
<code class="sig-name descname">initialize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.initialize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.initialize" title="Permalink to this definition">¶</a></dt>
<dd><p>Grab the module / submodule to perform QAT on</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.model_fuse_fn_name">
<code class="sig-name descname">model_fuse_fn_name</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.model_fuse_fn_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Name of model function to fuse the model in place prior
to performing QAT. None to uses the default function
<cite>sparseml.pytorch.utils.fuse_module_conv_bn_relus</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.submodules">
<code class="sig-name descname">submodules</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.submodules" title="Permalink to this definition">¶</a></dt>
<dd><p>List of submodule names to perform QAT on. None quantizes the entire
model</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>If start_pending(), fuses the model, sets the model quantization config,
calls torch.quantization.prepare_qat on the model to begin QAT
If end_pending(), updates the modules layers params to their original
trainable state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update_ready">
<code class="sig-name descname">update_ready</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_quantization.html#QuantizationModifier.update_ready"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_quantization.QuantizationModifier.update_ready" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the modifier is pending an update and update() should be called</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.modifier_regularizer">
<span id="sparseml-pytorch-optim-modifier-regularizer-module"></span><h2>sparseml.pytorch.optim.modifier_regularizer module<a class="headerlink" href="#module-sparseml.pytorch.optim.modifier_regularizer" title="Permalink to this headline">¶</a></h2>
<p>Modifier for changing parameters for regularization</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.modifier_regularizer.</code><code class="sig-name descname">SetWeightDecayModifier</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weight_decay</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">start_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">param_groups</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">end_epoch</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">- 1.0</span></em>, <em class="sig-param"><span class="n">log_types</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>List<span class="p">[</span>str<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'__ALL__'</span></em>, <em class="sig-param"><span class="n">constant_logging</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sparseml.pytorch.optim.modifier.ScheduledModifier" title="sparseml.pytorch.optim.modifier.ScheduledModifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sparseml.pytorch.optim.modifier.ScheduledModifier</span></code></a></p>
<p>Modifies the weight decay (L2 penalty) applied to with an optimizer during training</p>
<div class="line-block">
<div class="line">Sample yaml:</div>
<div class="line-block">
<div class="line">!SetWeightDecayModifier</div>
<div class="line-block">
<div class="line">start_epoch: 0.0</div>
<div class="line">weight_decay: 0.0</div>
<div class="line">param_groups: [0]</div>
<div class="line">log_types: __ALL__</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight_decay</strong> – weight decay (L2 penalty) value to set for the given optimizer</p></li>
<li><p><strong>start_epoch</strong> – The epoch to start the modifier at</p></li>
<li><p><strong>param_groups</strong> – The indices of param groups in the optimizer to be modified.
If None, all param groups will be modified. Default is None</p></li>
<li><p><strong>end_epoch</strong> – unused and should not be set</p></li>
<li><p><strong>log_types</strong> – The loggers to allow the learning rate to be logged to,
default is __ALL__</p></li>
<li><p><strong>constant_logging</strong> – True to constantly log on every step,
False to only log on an LR change and min once per epoch, default False</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.constant_logging">
<code class="sig-name descname">constant_logging</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.constant_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>True to constantly log on every step,
False to only log on an LR change, default True</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.log_update">
<code class="sig-name descname">log_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier.log_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.log_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether to log an update for the weight decay of the modifier
If constant logging is enabled, then will always log
Otherwise only logs after this modifier makes a change to the weight decay</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.param_groups">
<code class="sig-name descname">param_groups</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.param_groups" title="Permalink to this definition">¶</a></dt>
<dd><p>The indices of param groups in the optimizer to be modified.
If None, all param groups will be modified.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/modifier_regularizer.html#SetWeightDecayModifier.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.update" title="Permalink to this definition">¶</a></dt>
<dd><p>If start_pending(), updates the optimizers weight decay according to the
parameters of this modifier</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>epoch</strong> – current epoch and progress within the current epoch</p></li>
<li><p><strong>steps_per_epoch</strong> – number of steps taken within each epoch
(calculate batch number using this and epoch)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.weight_decay">
<code class="sig-name descname">weight_decay</code><a class="headerlink" href="#sparseml.pytorch.optim.modifier_regularizer.SetWeightDecayModifier.weight_decay" title="Permalink to this definition">¶</a></dt>
<dd><p>weight decay (L2 penalty) value to set for the given optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.optimizer">
<span id="sparseml-pytorch-optim-optimizer-module"></span><h2>sparseml.pytorch.optim.optimizer module<a class="headerlink" href="#module-sparseml.pytorch.optim.optimizer" title="Permalink to this headline">¶</a></h2>
<p>Optimizer wrapper for enforcing Modifiers on the training process of a Module.</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.optimizer.</code><code class="sig-name descname">ScheduledOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizer</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">manager</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#sparseml.pytorch.optim.manager.ScheduledModifierManager" title="sparseml.pytorch.optim.manager.ScheduledModifierManager">sparseml.pytorch.optim.manager.ScheduledModifierManager</a></span></em>, <em class="sig-param"><span class="n">steps_per_epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">loggers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger">sparseml.pytorch.utils.logger.PyTorchLogger</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<p>An optimizer wrapper to handle applying modifiers according to their schedule
to both the passed in optimizer and the module.</p>
<p>Overrides the step() function so that this method can call before and after on the
modifiers to apply appropriate modifications to both the optimizer and the module.</p>
<p>The epoch_start and epoch_end are based on how many steps have been taken
along with the steps_per_epoch.</p>
<div class="line-block">
<div class="line">Lifecycle:</div>
<div class="line-block">
<div class="line">- training cycle</div>
<div class="line-block">
<div class="line">- zero_grad</div>
<div class="line">- loss_update</div>
<div class="line-block">
<div class="line">- modifiers.loss_update</div>
</div>
<div class="line">- step</div>
<div class="line-block">
<div class="line">- modifiers.update</div>
<div class="line">- modifiers.optimizer_pre_step</div>
<div class="line">- optimizer.step</div>
<div class="line">- modifiers.optimizers_post_step</div>
</div>
</div>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – module to modify</p></li>
<li><p><strong>optimizer</strong> – optimizer to modify</p></li>
<li><p><strong>manager</strong> – the manager or list of managers used to apply modifications</p></li>
<li><p><strong>steps_per_epoch</strong> – the number of steps or batches in each epoch,
not strictly required and can be set to -1.
used to calculate decimals within the epoch,
when not using can result in irregularities</p></li>
<li><p><strong>loggers</strong> – loggers to log important info to within the modifiers;
ex tensorboard or to the console</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.add_param_group">
<code class="sig-name descname">add_param_group</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">param_group</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.add_param_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.add_param_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a param group to the <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code> s <cite>param_groups</cite>.</p>
<p>This can be useful when fine tuning a pre-trained network as frozen layers can be made
trainable and added to the <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code> as training progresses.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_group</strong> (<em>dict</em>) – Specifies what Tensors should be optimized along with group</p></li>
<li><p><strong>optimization options.</strong> (<em>specific</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step">
<code class="sig-name descname">adjust_current_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.adjust_current_step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.adjust_current_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Adjust the current step for the manager’s schedule to the given epoch and step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – the epoch to set the current global step to match</p></li>
<li><p><strong>step</strong> – the step (batch) within the epoch to set the
current global step to match</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>convenience function to get the first learning rate for any of
the param groups in the optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict">
<code class="sig-name descname">load_manager_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.load_manager_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_manager_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">state_dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the optimizer state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<em>dict</em>) – optimizer state. Should be an object returned
from a call to <a class="reference internal" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.state_dict" title="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update">
<code class="sig-name descname">loss_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">torch.Tensor</span></em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.loss_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.loss_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Optional call to update modifiers based on the calculated loss.
Not needed unless one or more of the modifier is using the loss
to make a modification or is modifying the loss itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> – the calculated loss after running a forward pass and loss_fn</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the modified loss tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager">
<em class="property">property </em><code class="sig-name descname">manager</code><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager" title="Permalink to this definition">¶</a></dt>
<dd><p>The ScheduledModifierManager for this optimizer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict">
<code class="sig-name descname">manager_state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.manager_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.manager_state_dict" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.param_groups">
<em class="property">property </em><code class="sig-name descname">param_groups</code><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.param_groups" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state of the optimizer as a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
<p>It contains two entries:</p>
<ul class="simple">
<li><dl class="simple">
<dt>state - a dict holding current optimization state. Its content</dt><dd><p>differs between optimizer classes.</p>
</dd>
</dl>
</li>
<li><p>param_groups - a dict containing all parameter groups</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">closure</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform a step on the optimizer activation normal.
Updates the current epoch based on the step count.
Calls into modifiers before the step happens.
Calls into modifiers after the step happens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> – optional closure passed into the contained optimizer
for the step</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.optimizer.ScheduledOptimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/optimizer.html#ScheduledOptimizer.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.optimizer.ScheduledOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the gradients of all optimized <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s to zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
This is will in general have lower memory footprint, and can modestly improve performance.
However, it changes certain behaviors. For example:
1. When the user tries to access a gradient and perform manual ops on it,
a None attribute or a Tensor full of 0s will behave differently.
2. If the user requests <code class="docutils literal notranslate"><span class="pre">zero_grad(set_to_none=True)</span></code> followed by a backward pass, <code class="docutils literal notranslate"><span class="pre">.grad</span></code>s
are guaranteed to be None for params that did not receive a gradient.
3. <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> optimizers have a different behavior if the gradient is 0 or None
(in one case it does the step with a gradient of 0 and in the other it skips
the step altogether).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_as">
<span id="sparseml-pytorch-optim-sensitivity-as-module"></span><h2>sparseml.pytorch.optim.sensitivity_as module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_as" title="Permalink to this headline">¶</a></h2>
<p>Sensitivity analysis implementations for increasing activation sparsity by using FATReLU</p>
<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_as.</code><code class="sig-name descname">ASLayerTracker</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">track_input</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">track_output</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">input_func</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">output_func</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>None<span class="p">, </span>Callable<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>An implementation for tracking activation sparsity properties for a module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> – the module to track activation sparsity for</p></li>
<li><p><strong>track_input</strong> – track the input sparsity for the module</p></li>
<li><p><strong>track_output</strong> – track the output sparsity for the module</p></li>
<li><p><strong>input_func</strong> – the function to call on input to the layer
and receives the input tensor</p></li>
<li><p><strong>output_func</strong> – the function to call on output to the layer
and receives the output tensor</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear">
<code class="sig-name descname">clear</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.clear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.clear" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear out current results for the model</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable">
<code class="sig-name descname">disable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.disable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.disable" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable the forward hooks for the layer</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable">
<code class="sig-name descname">enable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ASLayerTracker.enable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.enable" title="Permalink to this definition">¶</a></dt>
<dd><p>Enable the forward hooks to the layer</p>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input">
<em class="property">property </em><code class="sig-name descname">tracked_input</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_input" title="Permalink to this definition">¶</a></dt>
<dd><p>the current tracked input results</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output">
<em class="property">property </em><code class="sig-name descname">tracked_output</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ASLayerTracker.tracked_output" title="Permalink to this definition">¶</a></dt>
<dd><p>the current tracked output results</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_as.</code><code class="sig-name descname">LayerBoostResults</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">threshold</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">boosted_as</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">boosted_loss</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults">sparseml.pytorch.utils.module.ModuleRunResults</a></span></em>, <em class="sig-param"><span class="n">baseline_as</span><span class="p">:</span> <span class="n">torch.Tensor</span></em>, <em class="sig-param"><span class="n">baseline_loss</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunResults" title="sparseml.pytorch.utils.module.ModuleRunResults">sparseml.pytorch.utils.module.ModuleRunResults</a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#LayerBoostResults"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Results for a specific threshold set in a FATReLU layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – the name of the layer the results are for</p></li>
<li><p><strong>threshold</strong> – the threshold used in the FATReLU layer</p></li>
<li><p><strong>boosted_as</strong> – the measured activation sparsity after threshold is applied</p></li>
<li><p><strong>boosted_loss</strong> – the measured loss after threshold is applied</p></li>
<li><p><strong>baseline_as</strong> – the measured activation sparsity before threshold is applied</p></li>
<li><p><strong>baseline_loss</strong> – the measured loss before threshold is applied</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as">
<em class="property">property </em><code class="sig-name descname">baseline_as</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_as" title="Permalink to this definition">¶</a></dt>
<dd><p>the measured activation sparsity before threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss">
<em class="property">property </em><code class="sig-name descname">baseline_loss</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.baseline_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>the measured loss before threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as">
<em class="property">property </em><code class="sig-name descname">boosted_as</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_as" title="Permalink to this definition">¶</a></dt>
<dd><p>the measured activation sparsity after threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss">
<em class="property">property </em><code class="sig-name descname">boosted_loss</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.boosted_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>the measured loss after threshold is applied</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.name" title="Permalink to this definition">¶</a></dt>
<dd><p>the name of the layer the results are for</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold">
<em class="property">property </em><code class="sig-name descname">threshold</code><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>the threshold used in the FATReLU layer</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>return</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster">
<em class="property">class </em><code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_as.</code><code class="sig-name descname">ModuleASOneShootBooster</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">dataset</span><span class="p">:</span> <span class="n">torch.utils.data.dataset.Dataset</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n"><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">sparseml.pytorch.utils.loss.LossWrapper</a></span></em>, <em class="sig-param"><span class="n">data_loader_kwargs</span><span class="p">:</span> <span class="n">Dict</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implementation class for boosting the activation sparsity in a given module
using FATReLUs.
Programmatically goes through and figures out the best thresholds to limit loss
based on provided parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to boost</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex [cpu, cuda, cuda:1]</p></li>
<li><p><strong>dataset</strong> – the dataset used to evaluate the boosting on</p></li>
<li><p><strong>batch_size</strong> – the batch size to run through the module in test mode</p></li>
<li><p><strong>loss</strong> – the loss function to use for calculations</p></li>
<li><p><strong>data_loader_kwargs</strong> – any keyword arguments to supply to a the
DataLoader constructor</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers">
<code class="sig-name descname">run_layers</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layers</span><span class="p">:</span> <span class="n">List<span class="p">[</span>str<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">max_target_metric_loss</span><span class="p">:</span> <span class="n">float</span></em>, <em class="sig-param"><span class="n">metric_key</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">metric_increases</span><span class="p">:</span> <span class="n">bool</span></em>, <em class="sig-param"><span class="n">precision</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.001</span></em><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>str<span class="p">, </span><a class="reference internal" href="#sparseml.pytorch.optim.sensitivity_as.LayerBoostResults" title="sparseml.pytorch.optim.sensitivity_as.LayerBoostResults">sparseml.pytorch.optim.sensitivity_as.LayerBoostResults</a><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_as.html#ModuleASOneShootBooster.run_layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_as.ModuleASOneShootBooster.run_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Run the booster for the specified layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layers</strong> – names of the layers to run boosting on</p></li>
<li><p><strong>max_target_metric_loss</strong> – the max loss in the target metric that
can happen while boosting</p></li>
<li><p><strong>metric_key</strong> – the name of the metric to evaluate while boosting;
ex: [__loss__, top1acc, top5acc]. Must exist in the LossWrapper</p></li>
<li><p><strong>metric_increases</strong> – True if the metric increases for worse loss such as in
a CrossEntropyLoss, False if the metric decreases for worse such as in
accuracy</p></li>
<li><p><strong>precision</strong> – the precision to check the results to. Larger values here will
give less precise results but won’t take as long</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The results for the boosting</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_lr">
<span id="sparseml-pytorch-optim-sensitivity-lr-module"></span><h2>sparseml.pytorch.optim.sensitivity_lr module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_lr" title="Permalink to this headline">¶</a></h2>
<p>Sensitivity analysis implementations for learning rate on Modules against loss funcs.</p>
<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs">
<code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_lr.</code><code class="sig-name descname">default_exponential_check_lrs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">init_lr</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1e-06</span></em>, <em class="sig-param"><span class="n">final_lr</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">lr_mult</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.1</span></em><span class="sig-paren">)</span> &#x2192; Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#default_exponential_check_lrs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.default_exponential_check_lrs" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the default learning rates to check between init_lr and final_lr.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_lr</strong> – the initial learning rate in the returned list</p></li>
<li><p><strong>final_lr</strong> – the final learning rate in the returned list</p></li>
<li><p><strong>lr_mult</strong> – the multiplier increase for each step between
init_lr and final_lr</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the list of created lrs that increase exponentially between
init_lr and final_lr according to lr_mult</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity">
<code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_lr.</code><code class="sig-name descname">lr_loss_sensitivity</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">sparseml.pytorch.utils.loss.LossWrapper</a><span class="p">, </span>Callable<span class="p">[</span><span class="p">[</span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">optim</span><span class="p">:</span> <span class="n">torch.optim.optimizer.Optimizer</span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">steps_per_measurement</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">check_lrs</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(1e-06, 1.1e-06, 1.21e-06, 1.3310000000000003e-06, 1.4641000000000003e-06, 1.6105100000000006e-06, 1.7715610000000007e-06, 1.948717100000001e-06, 2.1435888100000012e-06, 2.3579476910000015e-06, 2.5937424601000017e-06, 2.853116706110002e-06, 3.1384283767210024e-06, 3.452271214393103e-06, 3.7974983358324136e-06, 4.177248169415655e-06, 4.594972986357221e-06, 5.0544702849929435e-06, 5.559917313492238e-06, 6.115909044841462e-06, 6.727499949325609e-06, 7.40024994425817e-06, 8.140274938683989e-06, 8.954302432552388e-06, 9.849732675807628e-06, 1.0834705943388392e-05, 1.1918176537727232e-05, 1.3109994191499957e-05, 1.4420993610649954e-05, 1.586309297171495e-05, 1.7449402268886447e-05, 1.9194342495775094e-05, 2.1113776745352607e-05, 2.322515441988787e-05, 2.554766986187666e-05, 2.8102436848064327e-05, 3.091268053287076e-05, 3.4003948586157844e-05, 3.7404343444773634e-05, 4.1144777789251e-05, 4.52592555681761e-05, 4.978518112499371e-05, 5.4763699237493086e-05, 6.02400691612424e-05, 6.626407607736664e-05, 7.289048368510331e-05, 8.017953205361364e-05, 8.819748525897502e-05, 9.701723378487253e-05, 0.00010671895716335979, 0.00011739085287969578, 0.00012912993816766537, 0.00014204293198443192, 0.00015624722518287512, 0.00017187194770116264, 0.00018905914247127894, 0.00020796505671840686, 0.00022876156239024756, 0.00025163771862927233, 0.0002768014904921996, 0.0003044816395414196, 0.00033492980349556157, 0.00036842278384511775, 0.0004052650622296296, 0.0004457915684525926, 0.0004903707252978519, 0.0005394077978276372, 0.000593348577610401, 0.0006526834353714411, 0.0007179517789085853, 0.0007897469567994438, 0.0008687216524793883, 0.0009555938177273272, 0.00105115319950006, 0.001156268519450066, 0.0012718953713950728, 0.0013990849085345801, 0.0015389933993880383, 0.0016928927393268422, 0.0018621820132595267, 0.0020484002145854797, 0.0022532402360440277, 0.0024785642596484307, 0.002726420685613274, 0.0029990627541746015, 0.003298969029592062, 0.0036288659325512686, 0.003991752525806396, 0.0043909277783870364, 0.004830020556225741, 0.005313022611848316, 0.005844324873033148, 0.006428757360336463, 0.00707163309637011, 0.007778796406007121, 0.008556676046607835, 0.009412343651268619, 0.010353578016395481, 0.01138893581803503, 0.012527829399838533, 0.013780612339822387, 0.015158673573804626, 0.01667454093118509, 0.0183419950243036, 0.020176194526733963, 0.02219381397940736, 0.0244131953773481, 0.02685451491508291, 0.029539966406591206, 0.03249396304725033, 0.03574335935197537, 0.03931769528717291, 0.043249464815890204, 0.047574411297479226, 0.052331852427227155, 0.05756503766994987, 0.06332154143694486, 0.06965369558063936, 0.0766190651387033, 0.08428097165257363, 0.092709068817831, 0.10197997569961412, 0.11217797326957554, 0.1233957705965331, 0.13573534765618642, 0.14930888242180507, 0.1642397706639856, 0.18066374773038418, 0.19873012250342262, 0.2186031347537649, 0.2404634482291414, 0.2645097930520556, 0.29096077235726114, 0.3200568495929873, 0.3520625345522861, 0.38726878800751474, 0.4259956668082662, 0.4685952334890929, 0.5154547568380022, 0.5)</span></em>, <em class="sig-param"><span class="n">loss_key</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'__loss__'</span></em>, <em class="sig-param"><span class="n">trainer_run_funcs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs">sparseml.pytorch.utils.module.ModuleRunFuncs</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">trainer_loggers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger">sparseml.pytorch.utils.logger.PyTorchLogger</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">show_progress</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.LRLossSensitivityAnalysis" title="sparseml.optim.sensitivity.LRLossSensitivityAnalysis">sparseml.optim.sensitivity.LRLossSensitivityAnalysis</a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_lr.html#lr_loss_sensitivity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_lr.lr_loss_sensitivity" title="Permalink to this definition">¶</a></dt>
<dd><p>Implementation for handling running sensitivity analysis for
learning rates on modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to run the learning rate sensitivity analysis over,
it is expected to already be on the correct device</p></li>
<li><p><strong>data</strong> – the data to run through the module for calculating
the sensitivity analysis</p></li>
<li><p><strong>loss</strong> – the loss function to use for the sensitivity analysis</p></li>
<li><p><strong>optim</strong> – the optimizer to run the sensitivity analysis with</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex: cpu, cuda.
module must already be on that device, this is used to place then data
on that same device.</p></li>
<li><p><strong>steps_per_measurement</strong> – the number of batches to run through for
the analysis at each LR</p></li>
<li><p><strong>check_lrs</strong> – the learning rates to check for analysis
(will sort them small to large before running)</p></li>
<li><p><strong>loss_key</strong> – the key for the loss function to track in the returned dict</p></li>
<li><p><strong>trainer_run_funcs</strong> – override functions for ModuleTrainer class</p></li>
<li><p><strong>trainer_loggers</strong> – loggers to log data to while running the analysis</p></li>
<li><p><strong>show_progress</strong> – track progress of the runs if True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of tuples containing the analyzed learning rate at 0
and the ModuleRunResults in 1, ModuleRunResults being a collection
of all the batch results run through the module at that LR</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim.sensitivity_pruning">
<span id="sparseml-pytorch-optim-sensitivity-pruning-module"></span><h2>sparseml.pytorch.optim.sensitivity_pruning module<a class="headerlink" href="#module-sparseml.pytorch.optim.sensitivity_pruning" title="Permalink to this headline">¶</a></h2>
<p>Sensitivity analysis implementations for kernel sparsity on Modules against loss funcs.</p>
<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude">
<code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_pruning.</code><code class="sig-name descname">model_prunability_magnitude</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#model_prunability_magnitude"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.model_prunability_magnitude" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the approximate sensitivity for an overall model.
Range of the values are not scaled to anything, so must be taken in context
with other known models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> – the model to calculate the sensitivity for</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the approximated sensitivity</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude">
<code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_pruning.</code><code class="sig-name descname">pruning_loss_sens_magnitude</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">sparsity_levels</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99)</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_magnitude"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_magnitude" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximated kernel sparsity (pruning) loss analysis for a given model.
Returns the results for each prunable param (conv, linear) in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the model to calculate the sparse sensitivity analysis for</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to calculate the loss for for each param</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the analysis results for the model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot">
<code class="sig-prename descclassname">sparseml.pytorch.optim.sensitivity_pruning.</code><code class="sig-name descname">pruning_loss_sens_one_shot</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">module</span><span class="p">:</span> <span class="n">torch.nn.modules.module.Module</span></em>, <em class="sig-param"><span class="n">data</span><span class="p">:</span> <span class="n">torch.utils.data.dataloader.DataLoader</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Union<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.loss.LossWrapper" title="sparseml.pytorch.utils.loss.LossWrapper">sparseml.pytorch.utils.loss.LossWrapper</a><span class="p">, </span>Callable<span class="p">[</span><span class="p">[</span>Any<span class="p">, </span>Any<span class="p">]</span><span class="p">, </span>torch.Tensor<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">device</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">steps_per_measurement</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">sparsity_levels</span><span class="p">:</span> <span class="n">List<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">(0.0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99)</span></em>, <em class="sig-param"><span class="n">loss_key</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'__loss__'</span></em>, <em class="sig-param"><span class="n">tester_run_funcs</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.module.ModuleRunFuncs" title="sparseml.pytorch.utils.module.ModuleRunFuncs">sparseml.pytorch.utils.module.ModuleRunFuncs</a><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tester_loggers</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span><a class="reference internal" href="sparseml.pytorch.utils.html#sparseml.pytorch.utils.logger.PyTorchLogger" title="sparseml.pytorch.utils.logger.PyTorchLogger">sparseml.pytorch.utils.logger.PyTorchLogger</a><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">show_progress</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="sparseml.optim.html#sparseml.optim.sensitivity.PruningLossSensitivityAnalysis" title="sparseml.optim.sensitivity.PruningLossSensitivityAnalysis">sparseml.optim.sensitivity.PruningLossSensitivityAnalysis</a><a class="reference internal" href="../_modules/sparseml/pytorch/optim/sensitivity_pruning.html#pruning_loss_sens_one_shot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#sparseml.pytorch.optim.sensitivity_pruning.pruning_loss_sens_one_shot" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a one shot sensitivity analysis for kernel sparsity.
It does not retrain, and instead puts the model to eval mode.
Moves layer by layer to calculate the sensitivity analysis for each and
resets the previously run layers.
Note, by default it caches the data.
This means it is not parallel for data loading and the first run can take longer.
Subsequent sparsity checks for layers and levels will be much faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> – the module to run the kernel sparsity sensitivity analysis over
will extract all prunable layers out</p></li>
<li><p><strong>data</strong> – the data to run through the module for calculating the sensitivity
analysis</p></li>
<li><p><strong>loss</strong> – the loss function to use for the sensitivity analysis</p></li>
<li><p><strong>device</strong> – the device to run the analysis on; ex: cpu, cuda</p></li>
<li><p><strong>steps_per_measurement</strong> – the number of samples or items to take for each
measurement at each sparsity lev</p></li>
<li><p><strong>sparsity_levels</strong> – the sparsity levels to check for each layer to calculate
sensitivity</p></li>
<li><p><strong>loss_key</strong> – the key for the loss function to track in the returned dict</p></li>
<li><p><strong>tester_run_funcs</strong> – override functions to use in the ModuleTester that runs</p></li>
<li><p><strong>tester_loggers</strong> – loggers to log data to while running the analysis</p></li>
<li><p><strong>show_progress</strong> – track progress of the runs if True</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the sensitivity results for every layer that is prunable</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-sparseml.pytorch.optim">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sparseml.pytorch.optim" title="Permalink to this headline">¶</a></h2>
<p>Recalibration code for the PyTorch framework.
Handles things like model pruning and increasing activation sparsity.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="sparseml.pytorch.optim.quantization.html" class="btn btn-neutral float-right" title="sparseml.pytorch.optim.quantization package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="sparseml.pytorch.nn.html" class="btn btn-neutral float-left" title="sparseml.pytorch.nn package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>