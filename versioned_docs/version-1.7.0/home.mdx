---
slug: /
tags:
- neural-magic
- deep-learning
- model-optimization
- sparsification
- cpu-deployment
keywords:
- Neural Magic
- deep learning optimization
- CPU inference
- sparse models
- SparseML
- DeepSparse
sidebar_label: Home
title: What is Neural Magic?
description: Neural Magic empowers you to optimize and deploy deep learning models on CPUs with GPU-class performance. Unlock efficiency, accessibility, and cost savings with our software solutions.
sidebar_position: 0
---

# Neural Magic Documentation

Neural Magic is a leader in machine learning model optimization and inference acceleration on the hardware of your choice, CPU or GPU. Our software makes your deployments fast and efficient, and your machine learning practice manageable. 

:::tip[FEATURED DEVELOPER HIGHLIGHTS]
_Keep tabs on our innovation with resources, examples, news, and more:_
- Blog: [Neural Magic Leaps Into GPU Acceleration](https://neuralmagic.com/blog/bringing-the-neural-magic-to-gpus/)
- New community repo for GPU inferencing: [`nm-vllm`](https://github.com/neuralmagic/nm-vllm)
- [Notebook](https://github.com/neuralmagic/examples/tree/main/notebooks/marlin-nm-vllm): Quantize LLMs to 4-bit weights, deploy them with Marlin, inside of `nm-vllm`!

:::

_Example CPU Workflow_
![Neural Magic Flows](/img/nm-flows.png)


## Selecting a Model

Start by choosing the ideal model for your project:

- <b>Neural Magic's Model Repositories:</b> Discover a vast selection of pre-sparsified models across popular use cases in our [SparseZoo](https://sparsezoo.neuralmagic.com/) and [Hugging Face](https://huggingface.co/neuralmagic) repositories. These models are ready for immediate, high-performance deployment.
- <b>Custom Models:</b> Easily integrate your PyTorch or ONNX models into Neural Magic's workflow, applying cutting-edge optimization and deployment techniques tailored to your specific needs.

## Optimizing a Model

Achieve unmatched model efficiency with Neural Magic's SparseML. This powerful toolkit leverages state-of-the-art research to streamline your optimization process:

- <b>Advanced Sparsification:</b> Employ sophisticated pruning and quantization strategies to shrink model size and boost inference speed.
- <b>User-Friendly Approach:</b>  SparseML is intuitive and accessible, empowering users at all levels to apply advanced optimization techniques effortlessly.


## Deploying a Model

Deploy your optimized model with exceptional performance on CPUs using Neural Magic's DeepSparse:

- <b>CPU-Optimized Performance:</b> DeepSparse harnesses the potential of sparsity to deliver GPU-level performance on commodity CPUs, maximizing hardware utilization.
- <b>Seamless Integration:</b> Deploy easily, as DeepSparse smoothly integrates into your existing applications, minimizing development overhead.

## Next Steps

Ready to revolutionize your deep learning workflow? Dive into our detailed guides and documentation:

### Sections

<DocCardList items={['get-started/index', 'guides/index', 'products/index', 'details/index']} />

### Tasks

<DocCardList items={['llms/index', 'computer-vision/index', 'nlp/index']} />

<br></br>

---

✅ <b>Connect:</b> Join our [Slack community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) to meet like-minded ML practitioners.

✅ <b>Subscribe:</b> Stay informed with our regular [email updates](https://neuralmagic.com/deep-sparse-community/#subscribe).

✅ <b>Contribute:</b> Shape the future of Neural Magic on [GitHub](https://github.com/neuralmagic) (⭐s appreciated!).

✅ <b>Feedback:</b> Help us refine our [documentation](https://github.com/neuralmagic/docs).
