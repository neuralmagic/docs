---
sidebar_position: 1
---

# Deployment


## CV - Object Detection

:::note
Ensure you've installed the necessary packages and dependencies for CV - Object Detection as outlined in the [Install Guide](./install#object-detection).
:::

This section covers deploying object detection models with DeepSparse including benchmarking, pipeline creation, and server setup utilizing a pre-sparsified model from the SparseZoo.

The examples below use a sparse, quantized YOLOv8 L model to demonstrate the deployment process.
The model is referenced by the following SparseZoo stub for use in the Neural Magic suite:
```text
zoo:yolov8-l-coco-pruned85_quantized
```

For other models that work with these examples, browse through the [CV - Object Detection YOLOv8 models in the SparseZoo](https://sparsezoo.neuralmagic.com/?architectures=yolov8&datasets=coco&tasks=detection&modelSet=computer_vision) to find one that fits your needs.

Additionally, the Pipeline and Server examples expect an image to be located at `image.jpg` in the current working directory.
To download an example image, run the following command:
```bash
wget -O image.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg
```

<details>
    <summary>Benchmarking</summary>

    Benchmarking is an optional step that demonstrates how to validate the performance for both dense and sparsified models, while showcasing the benefits of sparsity in the process.

    First benchmark an unoptimized version of the model to establish the baseline.
    The SparseZoo stub for the dense model is:
    ```text
    zoo:yolov8-l-coco-base
    ```

    Utilizing this stub, benchmark the dense, baseline model with the following command:
    <Tabs>
        <TabItem value="bash" label="Bash" default>

            ```bash
            deepsparse.benchmark "zoo:yolov8-l-coco-base"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
        <TabItem value="python" label="Python">

            ```python
            from deepsparse import benchmark

            result = benchmark("zoo:yolov8-l-coco-base")
            print(result)

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

    As seen in the output, the baseline model achieves a throughput of `x` images per second on a 4-core Intel CPU.

    Next, benchmark the sparsified model to compare the performance:
    <Tabs>
        <TabItem value="bash" label="Bash" default>

            ```bash
            deepsparse.benchmark "zoo:yolov8-l-coco-pruned85_quantized"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
        <TabItem value="python" label="Python">

            ```python
            from deepsparse import benchmark

            result = benchmark("zoo:yolov8-l-coco-pruned85_quantized")
            print(result)

            # <output>
            # TODO: add output
            # </output>
            ```
        </TabItem>
    </Tabs>

    The sparsified model achieves a throughput of `y` images per second on the same CPU, which is `z` times faster than the baseline model!

</details>

<details>
    <summary>Evaluating</summary>

    This is an optional step that demonstrates how to evaluate a model on a given dataset or set of datasets using DeepSparse to validate and compare the accuracies of dense and sparsified models.

    First, evaluate the baseline model to establish the accuracy baseline.
    The SparseZoo stub for the dense model is:
    ```text
    zoo:yolov8-l-coco-base
    ```

    Utilizing this stub, evaluate the dense, baseline model on the COCO128 dataset (default) with the following command:
    <Tabs>
        <TabItem value="bash" label="Bash" default>

            ```bash
            deepsparse.yolov8.eval --model_path "zoo:yolov8-l-coco-base"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

    As seen in the output, the baseline model achieves a mAP of `x` on the validation dataset.

    Next, evaluate the sparsified model to compare the accuracy:

    <Tabs>
        <TabItem value="bash" label="Bash" default>

            ```bash
            deepsparse.yolov8.eval --model_path "zoo:yolov8-l-coco-pruned85_quantized"

            # <output>
            # TODO: add output
            # </output>
            ```

        </TabItem>
    </Tabs>

    The sparsified model achieves a mAP of `y` on the validation dataset, which is within `z` of the baseline model!

</details>

### Pipeline

DeepSparse pipelines for YOLOv8 models accept an image or images as inputs and return a list of bounding boxes and their associated classes and confidences.
To create an object detection pipeline for the example model and process an image, utilize the following code:

```python
from deepsparse import Pipeline

pipeline = Pipeline.create(task="yolov8", model_path="zoo:yolov8-l-coco-pruned85_quantized")
result = pipeline("basilica.jpg")
print(result)

# <output>
# TODO: add output
# </output>
```

This code initializes an object detection pipeline and processes an image.

### Server

The DeepSparse Server wraps a pipeline in a REST API, allowing for easy deployment and inference.
For object detection models, the server supports the MLServer inference standards, allowing for familiarity and easy integration with existing codebases.

To create a DeepSparse Pipeline server that will run on port 5543 (default) with the MLServer specs, utilize the following code:

```bash
deepsparse.server "zoo:yolov8-l-coco-pruned85_quantized"
```

Now with the server running, you can send an HTTP request that conforms to the MLServer spec to process an image.
Below are examples for using `curl` and `python` to send a request to the server:

<Tabs>
    <TabItem value="bash" label="Bash" default>

        ```bash
        curl -X POST http://localhost:5543/predict \
        -H "Content-Type: application/json" \
        -d '{"inputs": ["path/to/image.jpg"]}'

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
    <TabItem value="python" label="Python">

        ```python
        import requests

        response = requests.post(
        "http://localhost:5543/predict",
        json={"inputs": ["path/to/image.jpg"]}
        )

        # <output>
        # TODO: add output
        # </output>
        ```

    </TabItem>
</Tabs>

As seen in the output, the server processes the image and returns a list of bounding boxes and their associated classes and confidences.

