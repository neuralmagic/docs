---
title: Hugging Face Hub Models
---

# What are models on Hugging Face Hub?
Hugging Face Hub models are DeepSparse-ready models that haven't gone through rigorous testing to ensure they meet certain criteria such as the ones in the SparseZoo. The process enables us to produce models faster without having to wait for the entire quality-assurance process. However, you should always perform evaluation for ensure the model's results are not completely off. 

We have a [Sparse LLMs Collection](https://huggingface.co/collections/neuralmagic/sparse-llms-659d61e81774dd48343642bf) where we share the most interesting LLMs where we've pruned at least 50% of the weights and optimized them for inference.

You can find official models that we have optimized for DeepSparse at the Neural Magic Organization page at https://huggingface.co/neuralmagic and more experimental models made by the community at https://huggingface.co/nm-testing. 

## How to create your own HF model 
The first process in creating your own one-shot model is to identify a Hugging Face model you'd like to optimize. [TinyLlama Chat v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) is a great choice for demos becasue if its size. 

Ensure you have the lastest version of SparseML. 
```bash
git clone https://github.com/neuralmagic/sparseml
pip install -e "sparseml[transformers]"
```

Download a recipe and one shot the model.The recipe contains the hyperparameters for pruning and quantization. 
```bash
wget https://huggingface.co/nm-testing/TinyLlama-1.1B-Chat-v0.4-pruned50-quant/raw/main/recipe.yaml # download recipe
sparseml.transformers.text_generation.oneshot --model_name TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dataset_name open_platypus --recipe recipe.yaml --output_dir ./obcq_deployment --precision float16
```

Next evaluate the model using `lm-evaluation-harness`. Start by installing it: 
```bash
git clone https://github.com/neuralmagic/lm-evaluation-harness.git
cd lm-evaluation-harness
pip install -e 
```

Evaluate on hellaswag for example:
```
python main.py \
 --model hf-causal-experimental \
 --model_args pretrained=obcq_deployment,trust_remote_code=True \
 --tasks hellaswag \
 --batch_size 64 \
 --no_cache \
 --write_out \
 --output_path "obcq_deployment/hellaswag.json" \
 --device "cuda:0" \
 --num_fewshot 0
```

Once you are certain that the models performs well, you can export it for use in DeepSparse. 
```bash
sparseml.export --task text-generation obcq_deployment/
```

If you have any queries about the entire process, [send us a message on Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).