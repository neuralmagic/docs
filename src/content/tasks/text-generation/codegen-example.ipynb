{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CodeGen: End to End Demo**\n",
    "\n",
    "In this guide, we will walk through an example of how to use Neural Magic's stack to sparsify and run LLM inference with a text generation model, using [`Salesforce/codegen-350M-mono`](https://huggingface.co/Salesforce/codegen-350M-mono) as an example.\n",
    "\n",
    "There are a few steps:\n",
    "- Installation\n",
    "- Export to ONNX\n",
    "- Apply One Shot Pruning and Quantization\n",
    "- Evaluate Accuracy\n",
    "- Inject KV Cache\n",
    "- Run Inference with DeepSparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Sparsify and DeepSparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sparsify-nightly==1.6.0.20230817\n",
    "%pip install deepsparse-nightly==1.6.0.20230817[transformers] --upgrade\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate via the CLI token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sparsify.login YOUR_CLI_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ONNX Export**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Start by downloading and exporting the model to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'codegen-350M-mono'...\n",
      "remote: Enumerating objects: 37, done.\u001b[K\n",
      "remote: Total 37 (delta 0), reused 0 (delta 0), pack-reused 37\u001b[K\n",
      "Unpacking objects: 100% (37/37), 1.09 MiB | 6.33 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/Salesforce/codegen-350M-mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sparseml.transformers.export_onnx \\\n",
    "    --model_path ./codegen-350M-mono \\\n",
    "    --task text-generation \\\n",
    "    --sequence_length 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  model.onnx               tokenizer_config.json  vocab.json\n",
      "merges.txt   special_tokens_map.json  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "%mv ./deployment ./dense-fp32\n",
    "%ls ./dense-fp32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apply One-Shot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next optimize the model by applying pruning and quantization, using Sparsify, Neural Magic's model optimization toolkit.\n",
    "\n",
    "For compressing LLMs, we will use a post-training algorithm called `FastOBCQ`, which we can apply using the `sparsify.run one-shot` pathway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Format Dataset**\n",
    "\n",
    "`FastOBCQ` uses calibration data during the pruning and quantization process. In this case, we will use the [`codeparrot/apps`](https://huggingface.co/datasets/codeparrot/apps) dataset as the calibration data.\n",
    "\n",
    "The Sparsify One-Shot pathway requires preprocessed data to be passed as a folder holding `.npz` files. Using about 1000 samples is generally enough.\n",
    "\n",
    "Run the following to pre-process the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "SAMPLES = 1000\n",
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "model_path = \"./codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(\"codeparrot/apps\")\n",
    "def preprocess_function(examples):\n",
    "    text = \"\"\n",
    "    for solution in json.loads(examples[\"solutions\"]):\n",
    "      text += solution\n",
    "      text += \"\\n\"\n",
    "    \n",
    "    return tokenizer(text, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "ds_sampled = dataset[\"train\"].shuffle(seed=42).select(range(SAMPLES + 100)).with_format(\"torch\")\n",
    "tokenized_dataset = ds_sampled.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    ").filter(lambda example: example[\"input_ids\"].shape[1] == SEQUENCE_LENGTH).select(range(SAMPLES))\n",
    "\n",
    "print(tokenized_dataset[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to format and save the data as NPZ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "# numpy exporter helper\n",
    "class NumpyExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NumpyExportWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        self.numpy_data = []\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            inputs = {}\n",
    "            batch_size = 0\n",
    "\n",
    "            for index, arg in enumerate(args):\n",
    "                if isinstance(arg, Tensor):\n",
    "                    inputs[f\"input_{index}\"] = arg\n",
    "                    batch_size = arg.size[0]\n",
    "\n",
    "            for key, val in kwargs.items():\n",
    "                if isinstance(val, Tensor):\n",
    "                    inputs[key] = val\n",
    "                    batch_size = val.shape[0]\n",
    "\n",
    "            start_index = len(self.numpy_data)\n",
    "            for _ in range(batch_size):\n",
    "                self.numpy_data.append({})\n",
    "\n",
    "            for input_key in iter(inputs):\n",
    "              for idx, input in enumerate(inputs[input_key]):\n",
    "                  self.numpy_data[start_index+idx][input_key] = input\n",
    "\n",
    "            # uncomment if you want to inspect the results, but slows us down\n",
    "            # return self.model(*args, **kwargs)\n",
    "\n",
    "    def save(self, path: str = \"data\"):\n",
    "        for index, item in enumerate(self.numpy_data):\n",
    "            npz_file_path = f'{path}/input{str(index).zfill(4)}.npz'\n",
    "            np.savez(npz_file_path, **item)\n",
    "\n",
    "        print(f'Saved {len(self.numpy_data)} npz files to {path}')\n",
    "\n",
    "\n",
    "# wrap model with numpy exporter\n",
    "model = NumpyExportWrapper(model)\n",
    "\n",
    "# format as numpy\n",
    "for data in tokenized_dataset:\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# save to ./data\n",
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run FastOBCQ**\n",
    "\n",
    "With the data setup, we are ready to apply `FastOBCQ` using Sparsify One-Shot.\n",
    "\n",
    "First, we will create a Recipe to run `FastOBCQ`. Recipes specify the algorithm to apply as well as the hyperparameters to use during the pruning and quantization process. For CodeGen-350M-Mono, there is a [premade recipe](https://sparsezoo.neuralmagic.com/models/codegen_mono-350m-bigpython_bigquery_thepile-pruned50_quantized?hardware=deepsparse-c6i.12xlarge&comparison=codegen_mono-350m-bigpython_bigquery_thepile-base&tab=3) available in the SparseZoo:\n",
    "\n",
    "```yaml\n",
    "# recipe.yaml\n",
    "!FastOBCQModifier\n",
    "  target_sparsity: 0.5\n",
    "  block_size: 128\n",
    "  layers_per_gpu: 8\n",
    "  supported_ops: ['MatMul']\n",
    "  omit_edge_layers: True\n",
    "  mse:\n",
    "      norm: 2.4\n",
    "      grid: 100\n",
    "      max_shrink: 0.8\n",
    "  scheme:\n",
    "      input_activations:\n",
    "          num_bits: 8\n",
    "          symmetric: False\n",
    "      weights:\n",
    "          num_bits: 8\n",
    "          symmetric: True\n",
    "  scheme_overrides:\n",
    "      Gemm:\n",
    "          input_activations:\n",
    "              num_bits: 8\n",
    "              symmetric: True\n",
    "  ignore: ['ReduceMean', 'Tanh', 'Softmax', 'Equal', 'Pow', 'Add', 'Sub', 'Div', 'Neg', 'Softmax', 'ConstantOfShape', 'Constant', 'Sqrt', 'Mul', 'Gather']\n",
    "  quantize_non_obq_weights: False\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the recipe to a YAML file called `recipe.yaml`. Apply the recipe with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sparsify.run one-shot \\\n",
    "    --use-case text-generation \\\n",
    "\t--model ./dense-fp32/model.onnx \\\n",
    "\t--data ./data \\\n",
    "\t--recipe ./recipe.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a couple hours to run. The resulting ONNX model will be saved in a directory called `./deployment`.\n",
    "\n",
    "Let's copy over the tokenizer and configuration files from `./dense-fp32`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp -r dense-fp32 50sparse-int8\n",
    "%mv deployment/model.onnx 50sparse-int8/model.onnx\n",
    "%rm -rf deployment\n",
    "\n",
    "%ls 50sparse-int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the accuracy of the model using the `deepsparse.transformers.eval_downstream` CLI, which allows us to compute perplexity.\n",
    "\n",
    "Run the following to evaluate the dense-fp32 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepsparse.transformers.eval_downstream ./dense-fp32 --dataset openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to evaluate the 50sparse-int8 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepsparse.transformers.eval_downstream ./50sparse-int8 --dataset openai_humaneval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inject KV Cache**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With validation complete, we can now inject the KV-caching mechanism into the ONNX graph to enable performant inference with DeepSparse.\n",
    "\n",
    "Create a directory to house the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r 50sparse-int8 50sparse-int8-kvcache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following script to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparseml.exporters.kv_cache_injector import KeyValueCacheInjector\n",
    "import onnx\n",
    "\n",
    "input_file = \"50sparse-int8/model.onnx\"\n",
    "output_file = \"50sparse-int8-kvcache/model.onnx\"\n",
    "\n",
    "model = onnx.load(input_file, load_external_data=False)\n",
    "model = KeyValueCacheInjector(os.path.dirname(input_file)).apply(model)\n",
    "onnx.save(model, output_file)\n",
    "print(f\"Modified model saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run Inference With DeepSparse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run inference with DeepSparse using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsparse import Pipeline\n",
    "\n",
    "pipeline = Pipeline.create(\n",
    "    task=\"text-generation\", \n",
    "    model_path=\"./50sparse-int8-kvcache\",\n",
    "    max_generated_tokens=128)\n",
    "\n",
    "prompt = \"def fib(n):\"\n",
    "output = pipeline(sequences=prompt)\n",
    "print(f\"{prompt}{output.sequences[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
