---
title: "CV Object Detection"
metaTitle: "Transfer a Sparsified Model for Object Detection"
metaDescription: "Transfer a Sparsified Object Detection Model to your dataset enabling performant deep learning deployments in a faster amount of time"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/get-started/transfer-a-sparse-model/object-detection.mdx"
index: 2000
---

# Transfer a Sparsified Model for Object Detection

This page walks through an example of fine-tuning a pre-sparsified model from the SparseZoo onto a new dataset for object detection.

We will use SparseZoo to pull down a pre-sparsified YOLOv5l and will use SparseML to fine-tune onto the VOC dataset while preserving sparsity.

Make sure you install SparseML with torchvision before getting started:
```bash
pip install sparseml[torchvision]
```

## Transfer Learning

The SparseZoo contains several sparsified object detection models and transfer learning recipes, including YOLOv5l, which is used in this example. The SparseZoo stub is below:

```bash
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95
```

The `sparseml.yolov5.train` CLI command kicks off a run to fine-tune the sparsified YOLOv5 model onto the VOC dataset for object detection.
After the command completes, the trained model will reamin sparse, achieve an mAP@0.5 of around 0.80 on VOC, and will be stored in the local `models/sparsified` directory.

```bash
$ sparseml.yolov5.train \
    --project yolov5l \
    --name sparsified \
    --data VOC.yaml \
    --cfg models_v5.0/yolov5l.yaml \
    --weights zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95?recipe_type=transfer \
    --hyp data/hyps/hyp.finetune.yaml \
    --recipe recipes/yolov5.transfer_learn_pruned_quantized.md
```

The most important arguments are `--data`, `--weights`, and `--recipe`:
- `--data` specifies the dataset onto which the model will be fine-tuned
- `--weights` specifies the base model used to start the transfer learning process (can be a SparseZoo stub or local custom model path)
- `--recipe` specifies the hyperparameters of the fine-tuning process (can be a SparseZoo stub or a local custom recipe)

To utilize your own dataset, set up the appropriate image dataset structure and pass the path as the `--data` argument. 
An example for VOC is on [GitHub](https://github.com/neuralmagic/sparseml/blob/main/src/sparseml/yolov5/data/VOC.yaml).

The `--cfg` and `--hyp` are configuration files. You can checkout the examples on [GitHub](https://github.com/neuralmagic/sparseml/tree/main/src/sparseml/yolov5).

There are many additional command line arguments that can be passed to tweak your fine-tuning process. Run the following to see the full list of options:

```bash
$ sparseml.yolov5.train -h
```

## Exporting for Inference

With the sparsified model successfully trained, it is time to export it for inference.
The `sparseml.yolov5.export_onnx` command is used to export the training graph to a performant inference one.
After the command completes, a `model.onnx` file is created in `yolov5/sparsified` folder.
It is now ready for deployment with the DeepSparse Engine utilizing its pipelines.

```bash
$ sparseml.yolov5.export_onnx \
    --weights yolov5/sparsified/weights/best.pt \
    --dynamic
```
