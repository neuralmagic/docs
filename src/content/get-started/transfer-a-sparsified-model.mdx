---
title: "Transfer a Sparsified Model"
metaTitle: "Transfer a Sparsified Model"
metaDescription: "Transfer a Sparsified Model to your dataset, enabling performant deep learning deployments with limited training"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/get-started/transfer-a-sparsified-model.mdx"
index: 3000
---

# Transfer a Sparsified Model

Sparse transfer learning is the easiest pathway for creating a sparse model fine-tuned on your datasets.

Sparse transfer learning works by taking a sparse model pre-trained on a large dataset and fine-tuning it onto a smaller downstream dataset. 
SparseZoo and SparseML work together to accomplish this goal:
- SparseZoo is a growing repository of sparse models pre-trained on large datasets ready for fine-tuning
- SparseML contains convenient training CLIs that run transfer-learn while preserving the same level of sparsity as the starting model

By fine-tuning pre-sparsified models onto your dataset, you can avoid the time, money, and hyperparameter tuning involved with sparsifying a dense model from scratch.
Once trained, deploy your model on the DeepSparse Engine for GPU-level performance on CPUs. 

## Example Use Cases

The docs below walk through example use cases leveraging SparseML for sparse transfer learning.

<LinkCards>
  <LinkCard href="./nlp-text-classification" heading="NLP Text Classification">
    Example transferring an NLP model to a text classification use case utilizing HuggingFace Transformers.
  </LinkCard>

  <LinkCard href="./cv-object-detection" heading="CV Object Detection">
    Example transferring an object detection model to a new dataset utilizing Ultralytics YOLOv5.
  </LinkCard>
</LinkCards>

## Other Use Cases

More documentation, models, use cases, and examples are continually being added.
If you don't see one you're interested in, search the [DeepSparse Github repo](https://github.com/neuralmagic/deepsparse), the [SparseML Github repo](https://github.com/neuralmagic/sparseml), the [SparseZoo website](https://sparsezoo.neuralmagic.com/), or ask in the [Neural Magic Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).
