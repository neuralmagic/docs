---
title: "NLP Sentiment Analysis"
metaTitle: "Transfer a Sparse, NLP Model to Sentiment Analysis"
metaDescription: "Transfer a Sparse Sentiment Analysis Model to your dataset enabling performant deep learning deployments in a faster amount of time"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/get-started/transfer-a-sparse-model/object-detection.mdx"
index: 1000
---

# Transfer a Sparse, NLP Model to Sentiment Analysis

The SparseZoo [contains models](https://sparsezoo.neuralmagic.com/?domain=nlp&sub_domain=masked_language_modeling&page=1) pre-trained and sparsified using masked language modeling for fine tuning to use cases such as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis).
One of the common models used for this task is [DistilBERT](https://arxiv.org/abs/1910.01108) and the SparseZoo contains a more performant [pruned, quantized version](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fdistilbert-none%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned80_quant-none-vnni) along with the dense, FP32 baseline among others.
By transferring from one of these models, you can performantly determine the general sentiment of text sequences.
The SparseZoo stubs for the DistilBERT models listed can be found either on their SparseZoo model pages or below:
- [Sparse-quantized DistilBERT](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fdistilbert-none%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned80_quant-none-vnni)
  ```bash
  zoo:nlp/masked_language_modeling/distilbert-none/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni
  ```
- [Dense DistilBERT](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fdistilbert-none%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fbase-none)
  ```bash
  zoo:nlp/masked_language_modeling/distilbert-none/pytorch/huggingface/wikipedia_bookcorpus/base-none
  ```

## Create a Teacher

For NLP tasks, [distillation](../../../user-guide/sparsification/distillation) is a core technology that enables highly sparse and compressed models.
With it, a trained, dense version of the model is used to distill its accuracy and knowledge into the sparse version creating a more accurate model.
[SparseML](../../../products/sparseml) and its [Hugging Face integration](../../../use-cases/huggingface-transformers) enable easy model distillation with [recipes](../../../user-guide/recipes).

Before utilizing distillation with sparse transfer learning, you'll need to create the dense teacher.
To create the teacher, use the CLI tools for training NLP models that are [installed with the sparseml package](../../../install/sparseml).
Following the standard training of DistilBERT, you will create a dense BERT-base verion to later use for distillation.
The below command pulls down the pre-trained weights for a dense version of BERT-base, finetune it on the SST-2 dataset using a basic recipe, and saves the results to a *models/teacher* directory.

```bash
sparseml.transformers.text_classification \
    --output_dir models/teacher \
    --model_name_or_path zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/base-none \
    --recipe_args '{"init_lr":0.00003}' \
    --task_name sst2 --max_seq_length 128 \
    --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \
    --do_train --do_eval --evaluation_strategy epoch --fp16  \
    --save_strategy epoch --save_total_limit 1
```

## Transfer

With the dense model created, it can now be used as a teacher while sparse transfer learning.
Note, the distillation teacher can be removed from the training process, if desired, by utilizing *--distil_teacher disable* argument.

The below command pulls down the weights for a sparse, pre-trained DistilBERT, finetunes it using the teacher and a distillation recipe, and saves the results to a *models/sparse_quantized* directory.

```bash
sparseml.transformers.train.text_classification \
    --output_dir models/sparse_quantized \
    --model_name_or_path zoo:nlp/masked_language_modeling/distilbert-none/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni \
    --recipe zoo:nlp/sentiment_analysis/distilbert-none/pytorch/huggingface/sst2/pruned80_quant-none-vnni \
    --distill_teacher models/teacher \
    --task_name sst2 --max_seq_length 128 \
    --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \
    --do_train --do_eval --evaluation_strategy epoch --fp16  \
    --save_strategy epoch --save_total_limit 1
```

## Deploy

With the model successfully trained, it is now time export it and deploy it for inference.
With the SparseML training commands, an [ONNX](https://onnx.ai/) export command is additionally installed.
This export command is specifically designed to export the training graph for performance in runtimes like the [DeepSparse Engine](../../../products/deepsparse).
Run the below command to export the trained model for sequence length 128.

```bash
sparseml.transformers.export_onnx \
    --model_path models/sparse_quantized \
    --task 'text-classification' --finetuning_task sst2 \
    --sequence_length 128
```

A *model.onnx* file is created in the *models/sparse_quantized* folder that is used for deployment.
See the [use a sparse model](../../../use-a-sparse-model/sentiment-analysis) to plugin the exported model directory (*models/sparse_quantized*) for the *model_path* argument in the DeepSparse pipelines.
