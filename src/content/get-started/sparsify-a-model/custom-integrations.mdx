---
title: "Custom Integrations"
metaTitle: "Creating a Custom Integration for Sparsifying Models"
metaDescription: "Creating a Custom Integration for Sparsifying Models with SparseML for smaller, faster, and cheaper model inferences in deployment"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/get-started/sparsify-a-model/custom-integrations.mdx"
index: 2000
---

# Creating a Custom Integration for Sparsifying Models

This page explains how to apply a recipe to a custom model. For more details on the concepts of pruning/quantization
as well as how to create recipes, see the [prior page](/get-started/transfer-a-sparsified-model).

In addition to supported integrations described on the prior page, SparseML is set to enable easy integration in custom training pipelines.
This flexibility enables easy sparsification for any neural network architecture for custom models and use cases. Once SparseML is installed,
the necessary code can be plugged into most PyTorch/Keras training pipelines with only a few lines of code.

## Install Requirements

This section requires [SparseML Torchvision Install](/get-started/install/sparseml) to run the Apply the Recipe section.

## Integrate SparseML

To enable sparsification of models with recipes, a few edits to the training pipeline code need to be made.
Specifically, a `ScheduledModifierManager` instance is used to take over and inject the desired sparsification algorithms into the training process.
To do this properly in PyTorch, the `ScheduledModifierManager` requires the instance of the `model` to modify, the `optimizer` used for training,
and the number of `steps_per_epoch` to ensure algorithms are applied at the right time.

For the integration, the following code illustrates all that is needed:
```Python
from sparseml.pytorch.optim import ScheduledModifierManager
manager = ScheduledModifierManager.from_yaml(recipe_path)
optimizer = manager.modify(model, optimizer, steps_per_epoch)

# your typical training loop, using model/optimizer as usual

manager.finalize(model)
```
Walking through this code:
1. The `ScheduledModifierManager` is imported from the SparseML Python package.
2. An instance of the `ScheduledModifierManager` is created from a recipe stored as a local file or on the SparseZoo.
3. The optimizer and model are modified by `ScheduledModifierManager` so that the recipe will be applied while training.
A wrapped instance of the training optimizer is returned.
4. After training has been completed, a finalize call is invoked on the `ScheduledModifierManager` to release all resources.

A simple training example utilizing PyTorch and Torchvision with this SparseML integration is provided below:
```Python
import torch
from torch.nn import Linear
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
from torch.optim import SGD

from sparseml.pytorch.models import resnet50
from sparseml.pytorch.datasets import ImagenetteDataset, ImagenetteSize
from sparseml.pytorch.optim import ScheduledModifierManager

# Model creation
NUM_CLASSES = 10  # number of imagenette classes
model = resnet50(pretrained=True, num_classes=NUM_CLASSES)

# Dataset creation
batch_size = 64
train_dataset = ImagenetteDataset(train=True, dataset_size=ImagenetteSize.s320, image_size=224)
train_loader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, num_workers=8)

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# Loss setup
criterion = CrossEntropyLoss()
optimizer = SGD(model.parameters(), lr=10e-6, momentum=0.9)

# Recipe - in this case, we pull down a recipe from the SparseZoo for ResNet50
# This can be a be a path to a local file
recipe_path = "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=original"

# SparseML Integration
manager = ScheduledModifierManager.from_yaml(recipe_path)
optimizer = manager.modify(model, optimizer, steps_per_epoch=len(train_loader))

# Training Loop
for epoch in range(manager.max_epochs):
    running_loss = 0.0
    running_corrects = 0.0
    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        with torch.set_grad_enabled(True):
            outputs, _ = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)
            loss.backward()
            optimizer.step()
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(train_loader.dataset)
    epoch_acc = running_corrects.double() / len(train_loader.dataset)
    print("Training Loss: {:.4f} Acc: {:.4f}".format(epoch_loss, epoch_acc))

manager.finalize(model)
```

## Create a Recipe

The recipe used in the supported integrations page will also work for custom integrations.
To dive into the details of this recipe and how to edit it, visit [the prior page](/get-started/supported-integrations).
The resulting recipe is included here for easy integration and testing.

```yaml
modifiers:
    - !GlobalMagnitudePruningModifier
        init_sparsity: 0.05
        final_sparsity: 0.8
        start_epoch: 0.0
        end_epoch: 30.0
        update_frequency: 1.0
        params: __ALL_PRUNABLE__

    - !SetLearningRateModifier
        start_epoch: 0.0
        learning_rate: 0.05

    - !LearningRateFunctionModifier
        start_epoch: 30.0
        end_epoch: 50.0
        lr_func: cosine
        init_lr: 0.05
        final_lr: 0.001

    - !QuantizationModifier
        start_epoch: 50.0
        freeze_bn_stats_epoch: 53.0

    - !SetLearningRateModifier
        start_epoch: 50.0
        learning_rate: 10e-6

    - !EpochRangeModifier
        start_epoch: 0.0
        end_epoch: 55.0
```

## Sparsify a Model

The pipeline is ready to sparsify a model with the integration and recipe setup.
To begin sparsifying, save the recipe as a local file called `recipe.yaml`.
Next, pass in the path to the recipe to the training script for the `recipe_path` argument for the `ScheduledModifierManager.from_yaml(recipe_path)` line.
With that completed, start the training pipeline, and the result will be a sparsified model.
