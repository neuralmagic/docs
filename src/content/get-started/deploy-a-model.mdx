---
title: "Deploy a Model"
metaTitle: "Deploy a Model"
metaDescription: "Deploy a model with DeepSparse Server for easy and performant ML deployments"
index: 5000
---

# Deploy a Model

DeepSparse comes pre-installed with a server to enable easy and performant model deployments.
The server provides an HTTP interface to communicate and run inferences on the deployed model rather than the Python APIs or CLIs.
It is a production-ready model serving solution built on Neural Magic's sparsification solutions resulting in faster and cheaper deployments.

The inference server is built with performance and flexibility in mind, with support for multiple models and multiple simultaneous streams.
It is also designed to be a plug-and-play solution for many ML Ops deployment solutions, including Kubernetes and AWS SageMaker.

## Example Use Cases

The docs below walk through use cases leveraging DeepSparse Server for deployment.

<LinkCards>
  <LinkCard href="./nlp-text-classification" heading="NLP Text Classification">
    Example deployment for an NLP text classification use case utilizing HuggingFace Transformers.
  </LinkCard>

  <LinkCard href="./cv-object-detection" heading="CV Object Detection">
    Example deployment for a CV object detection use case utilizing Ultralytics YOLOv5.
  </LinkCard>
</LinkCards>

## Other Use Cases

More documentation, models, use cases, and examples are continually being added.
If you don't see one you're interested in, search the [DeepSparse Github repo](https://github.com/neuralmagic/deepsparse), the [SparseML Github repo](https://github.com/neuralmagic/sparseml), the [SparseZoo website](https://sparsezoo.neuralmagic.com/), or ask in the [Neural Magic Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).
