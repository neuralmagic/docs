---
title: "Try a Model"
metaTitle: "Try a Model"
metaDescription: "Try a Model with the DeepSparse Engine to deploy for faster and cheaper inference on CPUs"
index: 2000
---

# Try a Model

DeepSparse Engine supports fast inference on CPUs for sparse and dense models. For sparse models in particular, it achieves GPU-level performance in many use cases.

Around the engine, the DeepSparse package includes various utilities to simplify benchmarking performance and model deployment. For instance:
- Trained models are passed in the open ONNX file format, enabling easy exporting from common packages like PyTorch, Keras, and TensorFlow.
- Benchmaking latency and performance is available via a single CLI call, with various arguments to test scenarios.
- Pipelines utilities wrap the model execution with input pre-processing and output post-processing, simplifying deployment and adding functionality like multi-stream, bucketing, and dynamic shape.

## Use Case Examples

The examples below walk through use cases leveraging DeepSparse for testing and benchmarking ONNX models for integrated use cases.

<LinkCards>
  <LinkCard href="./nlp-text-classification" heading="NLP Text Classification">
    Example pipelines and benchmarking for an NLP text classification use case utilizing HuggingFace Transformers.
  </LinkCard>

  <LinkCard href="./cv-object-detection" heading="CV Object Detection">
    Example pipelines and benchmarking for a CV object detection use case utilizing Ultralytics YOLOv5.
  </LinkCard>

  <LinkCard href="./custom-use-case" heading="Custom Use Case">
    Example for how to create a pipeline for a custom model utilizing the DeepSparse Engine.
  </LinkCard>
</LinkCards>

## Other Use Cases

More documentation, models, use cases, and examples are continually being added.
If you don't see one you're interested in, search the [DeepSparse Github repo](https://github.com/neuralmagic/deepsparse), [SparseML Github repo](https://github.com/neuralmagic/sparseml), or [SparseZoo website](https://sparsezoo.neuralmagic.com/). Or, ask in the [Neural Magic Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ).
