---
title: "Embedding Extraction"
metaTitle: "Embedding Extraction Deployment"
metaDescription: "Generalized Embedding Extraction Deployment"
index: 5000
---

# Deploying Embedding Extraction Models with DeepSparse

This page explains how to create an Embedding Extraction Pipeline with DeepSparse. 

## Installation Requirements

This section requires the [DeepSparse Server Install](/get-started/install/deepsparse).

## Getting Started

Confirm your machine is compatible with our [hardware requirements](/user-guide/deepsparse-engine/hardware-support).

### Model Format

The Embedding Extraction Pipeline enables you to generate embeddings in any domain, meaning you can use it with any ONNX model.
It (optionally) removes the projection head from the model, such that you can re-use SparseZoo models and custom models 
you have trained in the embedding extraction scenario.

There are two options for passing a model to the Embedding Extraction Pipeline:
- Pass a Local ONNX File
- Pass a SparseZoo Stub (which identifies an ONNX model in the SparseZoo)

## Deployment APIs

DeepSparse provides both a Pipeline API and an out-of-the-box model
server that can be used for end-to-end inference in either Python
applications or as an HTTP endpoint. Both options provide similar specifications
for configurations.

### Python API

Pipelines are the default interface for running inference with DeepSparse.

Once a model is obtained, either through SparseML training or directly from SparseZoo, 
Pipelines can be used to handle pre-processing and post-processing of input, making it easy to add DeepSparse to your application.

### HTTP Server

As an alternative to the Python API, DeepSparse Server allows you to
serve an Embedding Extraction Pipeline over HTTP. Configuring the server uses the same parameters and schemas as the `Pipelines`. 
Once launched, a `/docs` endpoint is created with full endpoint descriptions and support for making sample requests. For more detailed, checkout
[the documentation for DeepSparse Server](/user-guide/deploying-deepsparse/deepsparse-server).

## Deployment Examples

The following section includes example usage of the Pipeline and Server APIs for various use cases. 
Each example uses a SparseZoo stub to pull down the model, but a local path to an ONNX file can also be passed as the `model_path`.

### Python API

The Embedding Extraction Pipeline handles some useful actions around inference:

- First, on initialization, the Pipeline (optionally) removes a projection head from a model. You can use 
the `emb_extraction_layer` argument to specify which layer to return (use `-1` to get the last layer). 
If your ONNX model has no projection head, you can set `emb_extraction_layer=None` (the default) to skip this step.

- Second, as with all DeepSparse Pipelines, it handles the pre-processing such that you can pass raw input.
You will notice that in addition to the typical `task` argument used in `Pipeline.create()`, 
the Embedding Extraction Pipeline includes a  `base_task` argument. This argument tells the Pipeline the domain of the model, 
such that the Pipeline can figure out what pre-processing to do.

This is an example of extracting the last layer from ResNet-50:

```python
from deepsparse import Pipeline
import numpy

rn50_emb_pipeline = Pipeline.create(
    task="embedding-extraction",
    base_task="image-classification", # tells the pipeline to expect images and normalize input with ImageNet means/stds
    model_path="zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/channel20_pruned75_quant-none-vnni",
    emb_extraction_layer=-1, # extracts the last layer
)

input_image = "my_image.png" # path to input image
embedding = rn50_embedding_pipeline(images=input_image) # returns final layer of resnet-model
```

This is an example of extracting the last layer from DistilBERT sentiment analysis:

```python
from deepsparse import Pipeline

distil_bert_emb_pipeline = Pipeline.create(
    task="embedding-extraction",
    base_task="sentiment-analysis",
    model_path="zoo:nlp/sentiment_analysis/distilbert-none/pytorch/huggingface/sst2/pruned80_quant-none-vnni",
    emb_extraction_layer=-1, # extracts the last layer
)

input_sequence = "The generalized embedding extraction Pipeline is the best!"
embedding = distil_bert_emb_pipeline(images=input_sequence)
```

This is an example of using the final layer from a BERT model with no projection head (so the Pipeline does 
not remove the projection head on initialization).

```python
from deepsparse import Pipeline

bert_emb_pipeline = Pipeline.create(
    task="embedding-extraction",
    base_task="transformers_embedding_extraction",
    model_path="zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni",
    # emb_extraction_layer=None, # since there is no projection head on this model, can use the output 
)

input_sequence = "The generalized embedding extraction Pipeline is the best!"
embedding = bert_emb_pipeline(images=input_sequence) # extracts the final layer of bert-model
```

In the NLP domain, you can use `transformer_embedding_extraction` as the `base_task` and 
specify an `extraction_strategy` to perform a reduction on the sequence dimension 
rather than returning an embedding for each token. The options are:
- `per_token`: returns the embedding for each token in the sequence (default)
- `reduce_mean`: returns the average token of the sequence
- `reduce_max`: returns the max token of the sequence
- `cls_token`: returns the cls token from the sequence

An example returing the average embeddings of the tokens looks like this:

```python
from deepsparse import Pipeline

bert_emb_pipeline = Pipeline.create(
    task="embedding-extraction",
    base_task="transformers_embedding_extraction",
    model_path="zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned80_quant-none-vnni",
    extraction_strategy="reduce_mean" # returns the average embedding of the tokens
)

input_sequence = "The generalized embedding extraction Pipeline is the best!"
embedding = bert_emb_pipeline(images=input_sequence) # embeddings reduced along the sequence dimension
```

### HTTP Server

The HTTP Server is a wrapper around the Pipeline. It can be launched from the command line with a configuration file.

Config file:
```yaml
endpoints:
    task: embedding_extraction
    model_path: zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
    base_task image_classification
    emb_extraction_layer: -1
```

Spinning up:
```bash
deepsparse.server --config_file config.yaml
```

Making a request:
```python
import requests

url = 'http://0.0.0.0:5543/predict/from_files'
path = ['goldfish.jpeg'] # just put the name of images in here
files = [('request', open(img, 'rb')) for img in path]
resp = requests.post(url=url, files=files)
```