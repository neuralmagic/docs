---
title: "Token Classification"
metaTitle: "NLP Token Classification"
metaDescription: "Token Classification with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/use-cases/natural-language-processing/text-classification.mdx"
index: 3000
---

# Token Classification with Hugging Face Transformers and SparseML

This page explains how to create and deploy a sparse Transformer for Token Classification.

SparseML Token Classification `Pipelines` integrate with Hugging Faceâ€™s Transformers library to enable the sparsification of a large set of transformers models.
Sparsification is a powerful technique that results in faster, smaller, and cheaper deployable models.
A sparse model can be deployed with Neural Magic's DeepSparse Engine with GPU-class performance directly on your CPU.

This integration enables you to create a sparse model in two ways:
- **Sparsification of Popular Transformer Models** - sparsify any popular Hugging Face Transformer model from scratch.
- **Sparse Transfer Learning** - fine-tune a sparse model (or use one of our [sparse pre-trained models](https://sparsezoo.neuralmagic.com/?page=1&domain=nlp&sub_domain=token_classification)) on your own private dataset.

Each option is useful in different situations:
- **Sparsification from Scratch** enables you to create a sparse version of any model (even those not in the SparseZoo), but requires hand-tuning the hyperparameters of the Sparsification algorithm.
- **Sparse Transfer Learning** is the easiest path to creating a sparse model trained on your data. Simply pull a pre-sparsified model and transfer learning recipe from the SparseZoo and fine-tune on your data with a single command.

## Installation

This section requires [SparseML Torch Install](/get-started/install/sparseml) and [DeepSparse General Install](/get-started/install/deepsparse).

It is recommended to run Python 3.8 as some of the scripts within the transformers repository require it.

Transformers will not immediately install with this command. Instead, a sparsification-compatible version of Transformers will install on the first invocation of the Transformers code in SparseML.

## Tutorials
There are some additional tutorials for this functionality on GitHub.
- [Sparse Named Entity Recognition With BERT](https://neuralmagic.com/use-cases/sparse-named-entity-recognition/)

## Getting Started

### Sparsifying Popular Transformer Models

In the example below, a dense BERT model is sparsified and fine-tuned on the CoNLL-2003 dataset.

```bash
sparseml.transformers.token_classification \
  --model_name_or_path bert-base-uncased \
  --dataset_name conll2003 \
  --do_train \
  --do_eval \
  --output_dir './output' \
  --cache_dir cache \
  --distill_teacher disable \
  --recipe zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned80_quant-none-vnni
```

The SparseML train script is a wrapper around a [Hugging Face script](https://huggingface.co/docs/transformers/run_scripts),
and usage for most arguments follows the Hugging Face. The most important arguments for SparseML are:

- `--model_name_or_path` indicates which model to start the pruning process from. It can be a SparseZoo stub, Hugging Face model identifier, or a path to a local model.
- `--recipe` points to recipe file containing the sparsification hyperparamters. It can be a SparseZoo stub or a local file. For more on creating a recipe see [here](/user-guide/recipes/creating).
- `--dataset_name` indicates that we should fine tune on the CoNLL-2003 dataset.

To utilize a custom dataset, use the `--train_file` and `--validation_file` arguments. To use a dataset from the Hugging Face hub, use `--dataset_name`.
See the [Hugging Face Docs](https://huggingface.co/docs/transformers/run_scripts#run-a-script) for more details.

Run the following to see the full list of options:
```bash
$ sparseml.transformers.token_classification -h
```

### Sparse Transfer Learning

SparseML also enables you to fine-tune a pre-sparsified model onto your own dataset.
While you are free to use your backbone, we encourage you to leverage one of our [sparse pre-trained models](https://sparsezoo.neuralmagic.com) to boost your productivity!

In the example below, we fetch a pruned, quantized BERT model, pre-trained on Wikipedia and Bookcorpus datasets. We then fine-tune the model to the CoNLL-2003 dataset.
```bash
sparseml.transformers.token_classification \
    --model_name_or_path zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/12layer_pruned80_quant-none-vnni \
    --dataset_name conll2003 \
    --do_train \
    --do_eval \
    --output_dir './output' \
    --distill_teacher disable \
    --recipe zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/12layer_pruned80_quant-none-vnni?recipe_type=transfer-token_classification
```

This usage of the script is the same as the above.

In this example, however, the starting model is a pruned-quantized version of BERT from SparseZoo (rather than a dense BERT)
and the recipe is a transfer learning recipe, which instructs Transformers to maintain sparsity of the base model (rather than
a recipe that sparsifies a model from scratch).

#### Knowledge Distillation
By modifying the `distill_teacher` argument, you can enable [Knowledge Distillation](https://neptune.ai/blog/knowledge-distillation) (KD) functionality. KD provides additional
support to the sparsification process, enabling higher accuracy at higher levels of sparsity.

For example, the `--distill_teacher` argument can be set to pull a dense model from the SparseZoo to support the training process:

```bash
--distill_teacher zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/base-none
```

Alternatively, the user may decide to train their own dense teacher model. The following command uses the dense BERT base model from the SparseZoo and fine-tunes it on the CoNLL-2003 dataset for use as a dense teacher.
```bash
sparseml.transformers.token_classification \
    --model_name_or_path zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/base-none \
    --dataset_name conll2003 \
    --do_train \
    --do_eval \
    --output_dir models/teacher \
    --recipe zoo:nlp/masked_language_modeling/bert-base/pytorch/huggingface/wikipedia_bookcorpus/12layer_pruned80_quant-none-vnni?recipe_type=transfer-token_classification \
    --distill_teacher zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/base-none
```

Once the dense teacher is trained we may reuse it for KD in Sparsification or Sparse Transfer learning.
Simply pass the path to the directory with the teacher model to the `--distill_teacher` argument. For example:

```bash
--distill_teacher models/teacher
```

## SparseML CLI

The SparseML installation provides a CLI for sparsifying your models for a specific task; appending the `--help` argument displays a full list of options for training in SparseML:
```bash
sparseml.transformers.token_classification --help
```
output:
```bash
  --model_name_or_path MODEL_NAME_OR_PATH
                        Path to pretrained model, sparsezoo stub. or model identifier from huggingface.co/models (default: None)
  --distill_teacher DISTILL_TEACHER
                        Teacher model which needs to be a trained NER model (default: None)
  --cache_dir CACHE_DIR
                        Where to store the pretrained data from huggingface.co (default: None)
  --recipe RECIPE
                        Path to a SparseML sparsification recipe, see https://github.com/neuralmagic/sparseml for more information (default: None)
  --dataset_name DATASET_NAME
                        The name of the dataset to use (via the datasets library) (default: None)
  ...
```

To learn about the Hugging Face Transformers run-scripts in more detail, refer to [Hugging Face Transformers documentation](https://huggingface.co/docs/transformers/run_scripts).

## Deploying with DeepSparse

The artifacts of the training process are saved to the directory `--output_dir`. Once the script terminates, the directory will have everything required to deploy or further modify the model such as:
- The recipe (with the full description of the sparsification attributes).
- Checkpoint files (saved in the appropriate framework format).
- Additional configuration files (e.g., tokenizer, dataset info).

### Exporting the Sparse Model to ONNX

The DeepSparse Engine uses the ONNX format to load neural networks and then deliver breakthrough performance for CPUs by leveraging the sparsity and quantization within a network.

The SparseML installation provides a `sparseml.transformers.export_onnx` command that you can use to load the training model folder and create a new `model.onnx` file within. Be sure the `--model_path` argument points to your trained model.
```bash
sparseml.transformers.export_onnx \
    --model_path './output' \
    --task 'token-classification'
```

### DeepSparse Engine Deployment

Once the model is exported in the ONNX format, it is ready for deployment with the DeepSparse Engine.

The deployment is intuitive due to the DeepSparse Python API.

```python
from deepsparse import Pipeline

tc_pipeline = Pipeline.create(
  task="token-classification",
  model_path='./output'
)
inference = tc_pipeline("We are flying from Texas to California")
>> [{'entity': 'LABEL_0', 'word': 'we', ...},
    {'entity': 'LABEL_0', 'word': 'are', ...},
    {'entity': 'LABEL_0', 'word': 'flying', ...},
    {'entity': 'LABEL_0', 'word': 'from', ...},
    {'entity': 'LABEL_5', 'word': 'texas', ...},
    {'entity': 'LABEL_0', 'word': 'to', ...},
    {'entity': 'LABEL_5', 'word': 'california', ...}]
```

To learn more, refer to the [appropriate documentation in the DeepSparse repository](https://github.com/neuralmagic/deepsparse/blob/main/src/deepsparse/transformers/README.md).

## Support

For Neural Magic Support, sign up or log in to our [Deep Sparse Community Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ). Bugs, feature requests, or additional questions can also be posted to our [GitHub Issue Queue](https://github.com/neuralmagic/sparseml/issues).
