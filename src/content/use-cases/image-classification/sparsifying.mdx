---
title: "Train with SparseML"
metaTitle: "Optimize an Image Classification Model for Inference with SparseML"
metaDescription: "Optimize an Image Classification Model for Inference with SparseML"
index: 1000
---

# Sparsifying Image Classification Models with SparseML

This page explains how to train a sparse image classification model with SparseML's Torchvision integration. After training, 
the sparse model can be deployed with DeepSparse for GPU-class performance directly on your CPU.

With SparseML, you can create a sparse model in two ways:
- **Sparse Transfer Learning**: Fine-tune a pre-sparsified model checkpoint onto a downstream dataset, while maintaining the sparsity structure of the network. 
This process works just like typical fine-tuning and is recommended for use in any scenario where there are 
[checkpoints available in SparseZoo](https://sparsezoo.neuralmagic.com/?domain=cv&sub_domain=classification&page=1)).
- **Sparsification from Scratch**: Apply state-of-the-art training-aware pruning and quantization algorithms to arbitrary dense networks. 
This process enables you to create a sparse version of any model, but requires you to experiment with the pruning and quantization algorithms.

Let's walk through some examples using SparseML's CLI and Python API.

## Installation Requirements

This use case requires installation of [SparseML Torchvision](/get-started/install/sparseml).

## SparseML CLI

SparseML's CLI enables you to kick-off training workflows with various utilities like 
dataset loading, checkpoint saving, metric reporting, and logging handled for you. Once you kick off training, 
the artifacts from the training process are saved to your local filesystem and loss/accuracy 
metrics are logged and printed to standard output. Once the script terminates, you should find everything required to 
deploy or further modify the model, including the recipe and checkpoint files.

To get started, we just need to pass three required arguments: a starting checkpoint, a SparseML recipe, and a dataset.
```bash
sparseml.image_classification.train \
    --checkpoint-path [CHECKPOINT-PATH] \
    --recipe [RECIPE-PATH] \
    --dataset-path [DATASET-PATH]
```

The `checkpoint-path` specifies the starting model to use in the training process. It can either be a local path to a PyTorch checkpoint 
or a SparseZoo stub (which SparseML uses to download a PyTorch checkpoint).

The `dataset-path` specifies the dataset uses for training. It can either be a keyword referencing built-in datasets 
(e.g. imagenet or imagenette) or a local path to a dataset in the ImageNet format (see Advanced Functionality for more details).

The `recipe` specifies the sparsity related parameters of the training process. It can either be a local path to a YAML recipe file
or a SparseZoo stub (which SparseML uses to download a YAML recipe file). The `recipe` is the key to enabling the sparsity-related algorithms
implemented by SparseML. The examples below will demonstrate what recipes looks like for Sparse Transfer and Sparsification from Scratch, and you
can check out the [User Guide](/user-guide/recipes) for more detailed documentation.

Run the help command to inspect the full list of arguments and configurations.
```bash
sparseml.image_classification.train --help
```

### Sparse Transfer Learning with the CLI

Let's use the CLI to kick-off a Sparse Transfer Learning. 

Run the following to fine-tune a 95% pruned and quantized ResNet-50 checkpoint onto Imagenette, while maintaining sparsity:

```bash
sparseml.image_classification.train \
    --checkpoint-path zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --arch-key resnet50 \
    --recipe zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --dataset-path imagenette
```

The script downloads the starting checkpoint and transfer learning recipe from SparseZoo as well as the Imagenette 
dataset and trains the model for 10 epochs, converging to ~99% accuracy on the validation set.

Here's what the transfer learning recipe looks like:

```yaml
# Epoch and Learning-Rate variables
num_epochs: 10.0
init_lr: 0.0005

# quantization variables
quantization_epochs: 6.0

training_modifiers:
  - !EpochRangeModifier
    start_epoch: 0.0
    end_epoch: eval(num_epochs)

  - !LearningRateFunctionModifier
    final_lr: 0.0
    init_lr: eval(init_lr)
    lr_func: cosine
    start_epoch: 0.0
    end_epoch: eval(num_epochs)

# Phase 1 Sparse Transfer Learning / Recovery
sparse_transfer_learning_modifiers:
  - !ConstantPruningModifier
    start_epoch: 0.0
    params: __ALL_PRUNABLE__

# Phase 2 Apply quantization
sparse_quantized_transfer_learning_modifiers:
  - !QuantizationModifier
    start_epoch: eval(num_epochs - quantization_epochs)
```

The "Modifiers" encode how SparseML should modify the training process for Sparse Transfer Learning.
- `ConstantPruningModifier` tells SparseML to pin weights at 0 over all epochs, maintaining the sparsity structure of the network
- `QuantizationModifier` tells SparseML to quanitze the weights with quantization aware training over the last 6 epochs

As a result, the final checkpoint is trained on ImageNette, with 95% of weights pruned and quantization applied.

### Sparsify From Scratch with the CLI

Let's try an example of using the CLI to kick-off a Sparsification From Scratch run. We can use the same CLI script and 
simply swap out the transfer learning recipe for a sparsification from scratch recipe.

Run the following to create a 95% pruned and quantized ResNet-50, starting with the dense, unoptimized version:

```bash
sparseml.image_classification.train \
    --checkpoint-path zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none \
    --arch-key resnet50 \
    --recipe zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none \
    --dataset-path imagenet
```

The script downloads the starting dense checkpoint and sparsification recipe from SparseZoo as well as
the ImageNet dataset and trains the model for 206 epochs, converging to 75.8% top1 validation accuracy, 
recovering over 99% of the top1 validation accuracy of baseline dense model (76.1%).

<details open>
    <summary>Here's what the sparsification from scratch recipe for ResNet-50 looks like:</summary>

```yaml
# Pruning Variables
pruning_epochs_fraction: 0.925
pruning_epochs: eval(int((num_epochs - quantization_epochs) * pruning_epochs_fraction))
pruning_start_epoch: eval(lr_warmup_epochs)
pruning_end_epoch: eval(pruning_start_epoch + pruning_epochs)
pruning_update_frequency: 5
pruning_sparsity: 0.95
pruning_final_lr: 0.0

training_modifiers:
  - !EpochRangeModifier
    start_epoch: 0
    end_epoch: eval(num_epochs)

  - !LearningRateFunctionModifier
    start_epoch: 0
    end_epoch: eval(lr_warmup_epochs)
    lr_func: linear
    init_lr: eval(init_lr)
    final_lr: eval(warmup_lr)

  - !LearningRateFunctionModifier
    start_epoch: eval(lr_warmup_epochs)
    end_epoch: eval(quantization_start_epoch)
    lr_func: cosine
    init_lr: eval(warmup_lr)
    final_lr: eval(pruning_final_lr)

  - !SetWeightDecayModifier
    start_epoch: 0
    end_epoch: eval(quantization_start_epoch)
    weight_decay: eval(weight_decay)

pruning_modifiers:
  - !ACDCPruningModifier
    compression_sparsity: eval(pruning_sparsity)
    start_epoch: eval(pruning_start_epoch)
    end_epoch: eval(pruning_end_epoch)
    global_sparsity: True 
    update_frequency: eval(pruning_update_frequency)
    params:
      - sections.0.0.conv1.weight
      - sections.0.0.conv3.weight
      - sections.0.0.identity.conv.weight
      - sections.0.1.conv3.weight
      - sections.0.2.conv3.weight
      - sections.1.0.conv3.weight
      - sections.0.0.conv2.weight
      - sections.0.1.conv1.weight
      - sections.0.2.conv1.weight
      - sections.1.0.conv1.weight
      - sections.1.0.identity.conv.weight
      - sections.1.1.conv3.weight
      - sections.1.2.conv3.weight
      - sections.1.3.conv3.weight
      - sections.2.0.conv3.weight
      - sections.0.1.conv2.weight
      - sections.0.2.conv2.weight
      - sections.1.0.conv2.weight
      - sections.1.1.conv1.weight
      - sections.1.2.conv1.weight
      - sections.1.3.conv1.weight
      - sections.2.0.conv1.weight
      - sections.2.0.identity.conv.weight
      - sections.2.1.conv3.weight
      - sections.2.2.conv3.weight
      - sections.2.3.conv3.weight
      - sections.2.4.conv3.weight
      - sections.2.5.conv3.weight
      - sections.3.0.conv3.weight
      - sections.3.1.conv3.weight
      - sections.3.2.conv3.weight
      - sections.1.1.conv2.weight
      - sections.1.2.conv2.weight
      - sections.1.3.conv2.weight
      - sections.2.0.conv2.weight
      - sections.2.1.conv1.weight
      - sections.2.2.conv1.weight
      - sections.2.3.conv1.weight
      - sections.2.4.conv1.weight
      - sections.2.5.conv1.weight
      - sections.3.0.conv1.weight
      - sections.3.0.identity.conv.weight
      - sections.3.1.conv1.weight
      - sections.3.2.conv1.weight
      - sections.1.1.conv2.weight
      - sections.1.2.conv2.weight
      - sections.1.3.conv2.weight
      - sections.2.0.conv2.weight
      - sections.2.1.conv1.weight
      - sections.2.2.conv1.weight
      - sections.2.3.conv1.weight
      - sections.2.4.conv1.weight
      - sections.2.5.conv1.weight
      - sections.3.0.conv1.weight
      - sections.3.0.identity.conv.weight
      - sections.3.1.conv1.weight
      - sections.3.2.conv1.weight
      - sections.2.1.conv2.weight
      - sections.2.2.conv2.weight
      - sections.2.3.conv2.weight
      - sections.2.4.conv2.weight
      - sections.2.5.conv2.weight
      - sections.3.0.conv2.weight
      - sections.3.1.conv2.weight
      - sections.3.2.conv2.weight

quantization_modifiers:
  - !SetLearningRateModifier
    start_epoch: eval(quantization_start_epoch)
    learning_rate: eval(quantization_init_lr)

  - !LearningRateFunctionModifier
    final_lr: 0.0
    init_lr: eval(quantization_init_lr)
    lr_func: cosine
    start_epoch: eval(quantization_start_epoch + quantization_keep_bn_epochs)
    end_epoch: eval(num_epochs)

  - !QuantizationModifier
    start_epoch: eval(quantization_start_epoch)
    submodules:
      - input
      - sections
    disable_quantization_observer_epoch: eval(quantization_start_epoch + quantization_observer_epochs)
    freeze_bn_stats_epoch: eval(quantization_start_epoch + quantization_keep_bn_epochs)
```
</details>

Just like the Sparse Transfer Learning example, the "Modifiers" encode how SparseML should modify the training process. In this case,
we swapped the `ConstantPruningModifier` for the `ACDCPruningModifier`. As a result, rather than maintaining sparsity during the training process, 
SparseML applies the [AC/DC pruning algorithm](https://arxiv.org/pdf/2106.12379.pdf) to iteratively prune weights from the network until it reaches
95% sparsity, Additionally, just like Sparsification from Scratch example, we use the `QuantizationModifier` to apply quantization aware training (QAT) over
the final epochs.

As a result, the final checkpoint is trained on ImageNet, with 95% of weights pruned and quantization applied.

For an in-depth discussion of recipes and pruning algorithms checkout the [User Guide](/user-guide/recipes).

### Export to ONNX

Once you have trained your model, export to ONNX in order to deploy with DeepSparse.

The artifacts of the training process are saved to your local filesystem. Run the following to convert your PyTorch checkpoints to ONNX:
```bash
sparseml.image_classification.export_onnx \
  --checkpoint-path checkpoint.pth \
  --arch-key resnet50 \
  --dataset-path /root/.cache/nm_datasets/imagenette/imagenette-320
```

### Advanced Functionality

#### Modify a Recipe

Depending on your dataset, you may want to modify the recipe to train for more epochs. For instance, for
more diffcult datasets than Imagenette, it is common to run training for more epochs.

To update a recipe, you can download the YAML file from SparseZoo, make updates to the YAML directly, and pass the local path to SparseML. 

Alternatively, you can use `--recipe_args` to modify a recipe on the fly. The following runs for 15 epochs instead of 10:

```bash
sparseml.image_classification.train \
    --checkpoint-path zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --arch-key resnet50 \ 
    --recipe zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --recipe_args '{"num_epochs": 15"}' \
    --dataset-path imagenette
```

#### Use a Custom Dataset

SparseML's Torchvision CLI conforms to the ImageNet dataset format, where images are arranged into directories representing each class. 
To use a custom dataset with the SparseML CLI, you will need to arrange your data into this format.

For example, the following Python script downloads the ImageNette dataset to `/root/.cache/nm_datasets/imagenette/imagenette-320/`.

```python
from sparseml.pytorch.datasets import ImagenetteDataset, ImagenetteSize
train_dataset = ImagenetteDataset(train=True, dataset_size=ImagenetteSize.s320, image_size=224)
val_dataset = ImagenetteDataset(train=False, dataset_size=ImagenetteSize.s320, image_size=224)
```

The resulting `imagenette-320` directory looks like the following (where each subdirectory like `n01440764` 
represents one of the 10 classes in the ImageNette dataset) and each `.JPEG` file is a training example.
```bash
|-train
    |-n01440764
        |-n01440764_10026.JPEG
        |-n01440764_10027.JPEG
        |...
    |-n02979186
    |-n03028079
    |-n03417042
    |-n03445777
    |-n02102040
    |-n03000684
    |-n03394916
    |-n03425413
    |-n03888257
|-val
    |-n01440764
    |-n02979186
    | ...
```

We can then kick off sparse transfer learning with the following:

```bash
sparseml.image_classification.train \
    --checkpoint-path zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --arch-key resnet50 \
    --recipe zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification \
    --dataset-path /root/.cache/nm_datasets/imagenette/imagenette-320
```

## SparseML Python API

SparseML also has a Python API, enabling you to add SparseML to your existing PyTorch training flows.

Just like the CLI, the Python API uses YAML-based recipes to encode the parameters of the sparsification process, allowing you
to add SparseML with just a few lines of code.

The `ScheduleModifierManager` class is responsible for parsing the YAML recipes and overriding the standard PyTorch model and optimizer objects, 
encoding the logic of the sparsity algorithms from the recipe. Once you have called `manager.modify`, you can then use the model and 
optimizer as usual, as SparseML abstracts away the complexity of the sparsification algorithms.

The workflow looks like this:

```python
# typical model, optimizer, dataset definition
model = Model()
optimizer = Optimizer()
train_data = TrainData()

# parse recipe + edit model/optimizer with sparsity-logic
from sparseml.pytorch.optim import ScheduledModifierManager
manager = ScheduledModifierManager.from_yaml(PATH_TO_RECIPE)
optimizer = manager.modify(model, optimizer, len(train_data))

# PyTorch training loop, using the model/optimizer as usual

# clean-up
manager.finalize(model)
```

### Sparse Transfer Learning with the Python API

Let's walk through an example running Sparse Transfer Learning with the Python API. 

The following fine-tunes a 95% pruned-quantized ResNet-50 checkpoint onto the ImageNette dataset:

```python
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
from torch.optim import Adam
from sparseml.pytorch.datasets import ImagenetteDataset, ImagenetteSize
from sparseml.pytorch.models import ModelRegistry
from sparseml.pytorch.optim import ScheduledModifierManager
from sparseml.pytorch.utils import ModuleExporter
from tqdm.auto import tqdm
import math

device = "cuda" if torch.cuda.is_available() else "cpu"

BATCH_SIZE = 32
IMAGE_SIZE = 224
ZOO_STUB = "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none?recipe_type=transfer-classification"

# Step 1: Define your train and validation datasets below, here we will use Imagenette
train_dataset = ImagenetteDataset(train=True, dataset_size=ImagenetteSize.s320, image_size=IMAGE_SIZE)
val_dataset = ImagenetteDataset(train=False, dataset_size=ImagenetteSize.s320, image_size=IMAGE_SIZE)
train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=8)
val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=8)

# Step 2: Use Model Registry to download 95% pruned+quantized ResNet-50 model
model = ModelRegistry.create(
    key="resnet50",
    pretrained_path=ZOO_STUB,
    num_classes=10,
    ignore_error_tensors=["classifier.fc.weight", "classifier.fc.bias"],
)
model.to(device) # this is just a typical PyTorch object

# Step 3: Setup Loss Function and Optimizer
criterion = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=8e-3)

# Step 4: Download Transfer Learning Recipe and Update Model/Optimizer
zoo_model = Model(ZOO_STUB)
recipe_path = zoo_model.recipes.default.path
manager = ScheduledModifierManager.from_yaml(recipe_path)
optimizer = manager.modify(model, optimizer, steps_per_epoch=len(train_loader))

# Step 5: Run Transfer Learning
def run_model_one_epoch(model, data_loader, criterion, device, train=False, optimizer=None):
    if train:
        model.train()
    else:
        model.eval()

    running_loss = 0.0
    total_correct = 0
    total_predictions = 0

    for step, (inputs, labels) in tqdm(enumerate(data_loader), total=len(data_loader)):
        inputs = inputs.to(device)
        labels = labels.to(device)
        if train:
            optimizer.zero_grad()

        outputs, _ = model(inputs)  # model returns logits and softmax as a tuple
        loss = criterion(outputs, labels)

        if train:
            loss.backward()
            optimizer.step()

        running_loss += loss.item()
        predictions = outputs.argmax(dim=1)
        total_correct += torch.sum(predictions == labels).item()
        total_predictions += inputs.size(0)
        
    loss = running_loss / (step + 1.0)
    accuracy = total_correct / total_predictions
    return loss, accuracy

epoch = manager.min_epochs
for epoch in range(manager.max_epochs):
    epoch_name = f"{epoch + 1}/{manager.max_epochs}"
    
    # run training loop
    print(f"Running Training Epoch {epoch_name}")
    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)
    print(f"Training Epoch: {epoch_name}\nTraining Loss: {train_loss}\nTop 1 Acc: {train_acc}\n")

    # run validation loop
    print(f"Running Validation Epoch {epoch_name}")
    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)
    print(f"Validation Epoch: {epoch_name}\nVal Loss: {val_loss}\nTop 1 Acc: {val_acc}\n")

manager.finalize(model)

# Step 6: Export to ONNX
save_dir = "pytorch_classification"
exporter = ModuleExporter(model, output_dir=save_dir)
exporter.export_pytorch(name="resnet50_imagenette_pruned.pth")
exporter.export_onnx(torch.randn(1, 3, 224, 224), name="model.onnx", convert_qat=True)
```

There are 6 steps:
1. Create PyTorch `DataLoaders` for ImageNette. These are familiar native PyTorch objects.

2. Use SparseML's `ModelRegistry` to download a pre-sparsified version of ResNet-50 from SparseZoo. 
The `model` object is just a standard PyTorch model with pre-trained weights loaded from SparseZoo.

3. Setup setup an Optimizer and Loss function. These are familiar native PyTorch objects.

4. Download the Transfer Learning recipe from SparseZoo (the recipe is the exact same as the one used in the CLI example above). Use 
`ScheduledModifierManager` to loads the recipe and parse the modifiers. Call `manager.modify` to update the `model` and wraps the `optimizer`
with the logic of the Sparse Transfer Learning recipe.

5. We run a typical PyTorch training loop. The wrapped `optimizer` and `model` objects are used just like usual, as SparseML has abstracted 
away the implementation of the sparsification algorithms, so the sparsity structure of the network is maintained as the fine-tuning occurs.

6. Exports the model to ONNX, it can be deployed with DeepSparse for GPU-class performance on the CPU.

You have successfully fine-tuned a 95% pruned and quantized ResNet-50 onto Imagenette!

### Sparsification from Scratch with the Python API

Let's walk through an example of Sparsifying a Model from Scratch with the Python API. 

In this case, we will show how to sparsify MobileNetV2, which does not exist in the SparseZoo. Because of the recipe-driven
approach, the workflow is very similar to that of sparse transfer learing.

#### Create a Recipe

First, since MobileNetV2 is not in the SparseZoo, we will need to create a sparsification recipe. 

The example below is the simple recipe for sparsifying model, which prunes all layers to 80% sparsity using the Gradual 
Magnitude Pruning algorithm and quantizes the weights to INT8 with Quantization Aware Training:

Save the following as `custom_recipe.yaml` in your local directory.

```yaml
# General Epoch/LR variables
num_epochs: &num_epochs 100

# Pruning variables
pruning_start_epoch: &pruning_start_epoch 0.0
pruning_end_epoch: &pruning_end_epoch 40.0
pruning_update_frequency: &pruning_update_frequency 0.4
init_sparsity: &init_sparsity 0.05
final_sparsity: &final_sparsity 0.8

training_modifiers:
  - !EpochRangeModifier
    end_epoch: *num_epochs
    start_epoch: 0.0

  - !SetLearningRateModifier
    start_epoch: 0.0
    learning_rate: 0.005
    
  - !LearningRateModifier
    start_epoch: 40.0
    lr_class: CosineAnnealingWarmRestarts
    lr_kwargs:
      lr_min: 0.00005
      cycle_epochs: 50
    init_lr: 0.005

  - !SetLearningRateModifier
    start_epoch: 90.0
    learning_rate: 0.00001
    
  - !SetLearningRateModifier
    start_epoch: 91.0
    learning_rate: 0.000001
    
  - !SetWeightDecayModifier
    start_epoch: 90.0
    weight_decay: 0.0
    
pruning_modifiers:
  - !GMPruningModifier
    params: __ALL_PRUNABLE__
    start_epoch: *pruning_start_epoch
    end_epoch: *pruning_end_epoch
    update_frequency: *pruning_update_frequency
    final_sparsity: *final_sparsity
    init_sparsity: *init_sparsity
        
quantization_modifiers:
  - !QuantizationModifier
    start_epoch: 90.0
```

The `GMPruningModifier` instructs SparseML to run the gradual magnitude pruning (GMP) algorithm. 
Few algorithms are better than GMP in overall results, and none beat the simplicity.
GMP works by taking a trained dense network and iteratively removes the weights closest to zero at the end of 
serveral training epochs, gradually inducing sparsity in the network. 

The `QuantizationModifier` instructs SparseML to run the quantization aware training (QAT)
algorithm. QAT works fake quantization operators are injected into the graph before quantizable 
nodes for activations, and weights are wrapped with fake quantization operators. 
The fake quantization operators interpolate the weights and activations down to INT8 on the forward pass 
but enable a full update of the weights at FP32 on the backward pass, allowing the model to adapt to the loss of 
information from quantization on the forward pass. 

In our case, the recipe instucts SparseML to run GMP over the first 40 epochs, starting with 5% sparsity and ending with 80% sparsity and
to run QAT over the last 10 epochs.

Checkout the [Recipes User Guide](/user-guide/recipes) for more details on recipes.

#### Run Training

Second, now that we have our recipe, we can run the training process just like sparse transfer learning.

The workflow looks almost identical to Sparse Transfer Learning, except we will use MobileNetV2 from Torchvision, the ImageNet
dataset, and the custom recipe.

The example below assumes the recipe from above is saved in your local directory as `custom-recipe.yaml`.

```python
from torchvision.models import mobilenet_v2, MobileNet_V2_Weights
from torchvision.datasets import ImageNet
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
from torch.optim import Adam
from sparseml.pytorch.datasets import ImagenetteDataset, ImagenetteSize
from sparseml.pytorch.optim import ScheduledModifierManager
from sparseml.pytorch.utils import ModuleExporter
from tqdm.auto import tqdm
import math

device = "cuda" if torch.cuda.is_available() else "cpu"

BATCH_SIZE = 32
IMAGE_SIZE = 224
WEIGHTS = MobileNet_V2_Weights.IMAGENET1K_V1
RECIPE_PATH = "custom-recipe.yaml"

# Step 1: Define your train and validation datasets below, here we will use Imagenette

train_dataset = ImageNet(split="train", transform=WEIGHTS.transform)
val_dataset = ImageNet(split="val", transform=WEIGHTS.transform)
train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=8)
val_loader = DataLoader(val_dataset, BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=8)

# Step 2: Use Model Registry to download 95% pruned+quantized ResNet-50 model
model = mobilenet_v2(weights=WEIGHTS)
model.to(device) # this is just a typical PyTorch object

# Step 3: Setup Loss Function and Optimizer
criterion = CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=8e-3)

# Step 4: Load Custom Recipe and Update Model/Optimizer
manager = ScheduledModifierManager.from_yaml(RECIPE)
optimizer = manager.modify(model, optimizer, steps_per_epoch=len(train_loader))

# Step 5: Run Transfer Learning
def run_model_one_epoch(model, data_loader, criterion, device, train=False, optimizer=None):
    if train:
        model.train()
    else:
        model.eval()

    running_loss = 0.0
    total_correct = 0
    total_predictions = 0

    for step, (inputs, labels) in tqdm(enumerate(data_loader), total=len(data_loader)):
        inputs = inputs.to(device)
        labels = labels.to(device)
        if train:
            optimizer.zero_grad()

        outputs, _ = model(inputs)  # model returns logits and softmax as a tuple
        loss = criterion(outputs, labels)

        if train:
            loss.backward()
            optimizer.step()

        running_loss += loss.item()
        predictions = outputs.argmax(dim=1)
        total_correct += torch.sum(predictions == labels).item()
        total_predictions += inputs.size(0)
        
    loss = running_loss / (step + 1.0)
    accuracy = total_correct / total_predictions
    return loss, accuracy

epoch = manager.min_epochs
for epoch in range(manager.max_epochs):
    epoch_name = f"{epoch + 1}/{manager.max_epochs}"
    
    # run training loop
    print(f"Running Training Epoch {epoch_name}")
    train_loss, train_acc = run_model_one_epoch(model, train_loader, criterion, device, train=True, optimizer=optimizer)
    print(f"Training Epoch: {epoch_name}\nTraining Loss: {train_loss}\nTop 1 Acc: {train_acc}\n")

    # run validation loop
    print(f"Running Validation Epoch {epoch_name}")
    val_loss, val_acc = run_model_one_epoch(model, val_loader, criterion, device)
    print(f"Validation Epoch: {epoch_name}\nVal Loss: {val_loss}\nTop 1 Acc: {val_acc}\n")

manager.finalize(model)

# Step 6: Export to ONNX
save_dir = "pytorch_classification"
exporter = ModuleExporter(model, output_dir=save_dir)
exporter.export_pytorch(name="resnet50_imagenette_pruned.pth")
exporter.export_onnx(torch.randn(1, 3, 224, 224), name="model.onnx", convert_qat=True)
```

We have the same 6 steps as before:
1. Create PyTorch `DataLoaders` for ImageNet. These are familiar native PyTorch objects.

2. Use Torchvision to download MobileNetV2 with pretrained weights.

3. Setup an Optimizer and Loss function. These are familiar native PyTorch objects.

4. Setup the `ScheduledModifierManager` with the `custom-recipe.yaml` and call `manager.modify` to update the `model` and wraps the `optimizer` 
with the logic of the Sparsification recipe (i.e. the GMP algorithm and the QAT algorithm).

5. Run a typical PyTorch training loop. The wrapped `optimizer` and `model` objects are 
used just like usual, as SparseML has abstracted away the implementation of the GMP and QAT algorithms. At the end of each epoch,
SparseML prunes the weights according to the schedule specified in the recipe.

6. Export to ONNX, enabling the model to be deployed with DeepSparse for GPU-class performance on the CPU.

You have successfully created an 80% pruned and quantized MobileNetV2!