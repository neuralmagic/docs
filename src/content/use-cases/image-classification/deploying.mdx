---
title: "Deploy with DeepSparse"
metaTitle: "Image Classification Deployments with DeepSparse"
metaDescription: "Image Classification deployments with DeepSparse to create cheaper and more performant models"
index: 2000
---

# Deploying Image Classification Models with DeepSparse

This page explains how to benchmark and deploy an image classification model with DeepSparse.

DeepSparse accepts models in the open ONNX format, granting you the flexibility to serve your
model in a framework-agnostic manner.

There are three interfaces for interacting with DeepSparse.
- `Engine` is the lowest-level API. It enables you to compile a model and make requests on raw input tensors
- `Pipeline` is the default API. Similiar in concept to Hugging Face Pipelines, it wraps the `Engine` with pre-preprocessing
and post-processing, allowing you to make requests on raw data and recieve post-processed predictions
- `Server` is a REST API wrapper around `Pipeline` built on FastAPI and Uvicorn. It enables you to stand up a model serving
endpoint running DeepSparse with a single CLI

## Installation Requirements

This use case requires the installation of [DeepSparse Server](/get-started/install/deepsparse).

Before you start using DeepSparse, confirm your machine is
compatible with our [hardware requirements](/user-guide/deepsparse-engine/hardware-support).

## Benchmarking Performance

Before deploying, we can use DeepSparse's benchmarking utility to demonstrate the performance gains running DeepSparse.

#### ONNX Runtime Performance

As a baseline, let's check out ONNX Runtime's performance on ResNet-50, downloaded from SparseZoo.

Install with:
```bash
pip install onnxruntime
```

Benchmark performance at batch 32 (numbers reported from a `c6i.8xlarge` AWS instance ... 16 cores).
```
deepsparse.benchmark zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none -b 32 -s sync -nstreams 1 -e onnxruntime

>> XXX
```

ONNX Runtime achieves XXX items/second.

#### DeepSparse Performance

Now, let's run DeepSparse on the 95% pruned-quantized version of ResNet-50 from SparseZoo.
```
deepsparse.benchmark zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none -b 32 -s sync -nstreams 1

>> XXX
```

DeepSparse achieves XXX items/second. This is an XXX speed-up over ONNX Runtime, on the same system!

Now that we have seen the performance gains from using DeepSparse with sparse models, let's see how to add DeepSparse to
your application.

## DeepSparse Engine

Engine is the lowest-level API for interacting with DeepSparse. As much as possible, we recommended you use the Pipeline
API but Engine is available as needed if you want to handle pre- or post-processing yourself.

With Engine, we can compile an ONNX file and run inference on raw tensors.

Here's an example, using a 95% pruned-quantized ResNet-50 from SparseZoo:

```python
from deepsparse import compile_model
from deepsparse.utils import generate_random_inputs, model_to_path
import numpy as np

# download onnx from sparsezoo and compile with batchsize 1
sparsezoo_stub = "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none"
batch_size = 1
rn50_engine = compile_model(
  model=sparsezoo_stub,   # sparsezoo stub or path to local ONNX
  batch_size=batch_size   # defaults to batch size 1
)

# input is raw numpy tensors, output is raw scores, probs
inputs = generate_random_inputs(model_to_path(sparsezoo_stub), batch_size)
scores, probs = rn50_engine(inputs)
print(probs.shape)
print(np.sum(probs))

# >> (1,1000)
# >> 1.0
```

## DeepSparse Pipelines

Pipeline is the default interface for interacting with DeepSparse. 

Just like Hugging Face Pipelines, DeepSparse Pipelines wrap pre- and post-processing around the inference performed by the Engine.
This creates a clean API that allows you to pass raw images to DeepSparse and recieve back the post-processed prediction,
making it easy to add DeepSparse to your application.

We will use the `Pipeline.create()` constructor to create an instance of an image classifcation Pipeline
with a 95% pruned-quantized version of ResNet-50. We can then pass the Pipeline raw image files or numpy arrays
and recieve back the predictions. All of the pre-processing (resizing to 256x256, randomly cropping to 224x224, 
and normalizing with ImageNet means / standard deviations) is handled by the Pipeline.

Download the following image file for the example:
```
wget https://raw.githubusercontent.com/neuralmagic/docs/rs/embedding-extraction-feature/files-for-examples/user-guide/deepsparse/logging/goldfish.jpg
```

```python
from deepsparse import Pipeline
from PIL import Image
import numpy as np

# download onnx from sparsezoo and compile with batch size 1
sparsezoo_stub = "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none"
batch_size = 1
ic_pipeline = Pipeline.create(
  task="image_classification",
  model_path=sparse_zoo_stub,   # sparsezoo stub or path to local ONNX
  batch_size=1                  # default batch size is 1
)

# run inference on image file
prediction = ic_pipeline(images="goldfish.jpg")
print(prediction)

# >>> labels=[1] scores=[24.028440475463867] ... (label 1 in ImageNet is Goldfish!)
```

## DeepSparse Server

Built on the popular FastAPI and Uvicorn stack, DeepSparse Server enables you to set-up a REST endpoint 
for serving inferences over HTTP. Since DeepSparse Server wraps the Pipeline API, it
inherits all of the utilities provided by Pipelines.

The CLI command below launches an image classifcation pipeline with a 95% pruned-quantized 
ResNet-50 identifed by its SparseZoo stub:

```bash
deepsparse.server \
  --task image_classification \
  --model_path "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none" # or path/to/onnx
```

You should see Uvicorn report that it is running on `http://0.0.0.0:5543`. Once launched, a `/docs` path is 
created with full endpoint descriptions and support for making sample requests.

Here is an example client request, using the Python `requests` library for formatting the HTTP:
```python
import requests

# Uvicorn is running on this port
url = 'http://0.0.0.0:5543/predict/from_files'
path = ['goldfish.jpg'] # just put the name of images in here

# send the raw image
files = [('request', open(img, 'rb')) for img in path]
resp = requests.post(url=url, files=files)

# recieve the post-processed output
print(resp.text)
# >> {"labels":[1],"scores":[24.028444290161133]} ... (label 1 in ImageNet is Goldfish!)
```

## Advanced Functionality

DeepSparse Pipelines and Server have multiple utilities that simplify deployment across a variety of
use cases. To learn more about the options for configuring your DeepSparse deployment, check-out the 
documentation for each feature in the DeepSparse user guide:

- DeepSparse Pipelines Overview
- DeepSparse Server Overview
- DeepSparse Logging Overview