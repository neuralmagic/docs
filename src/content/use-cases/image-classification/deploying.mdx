---
title: "Deploying"
metaTitle: "Image Classification Deployments with DeepSparse"
metaDescription: "Image Classification deployments with DeepSparse to create cheaper and more performant models"
githubURL: "https://github.com/neuralmagic/docs/blob/main/src/content/use-cases/image-classification/deploying.mdx"
index: 2000
---

# Deploying Image Classification Models with DeepSparse

This page explains how to deploy an Image Classification model with DeepSparse.

DeepSparse allows accelerated inference, serving, and benchmarking of sparsified image classification models. 
These integrations enables you to easily deploy sparsified image classification models onto the DeepSparse Engine for GPU-class performance directly on the CPU.

## Installation Requirements

This section requires the [DeepSparse Server Install](/get-started/install/deepsparse).

## Getting Started

Before you start using the DeepSparse Engine, confirm your machine is
compatible with our [hardware requirements](https://docs.neuralmagic.com/deepsparse/source/hardware.html).

### Model Format

To deploy an image classification model using DeepSparse Engine, pass the model in the ONNX format.
This grants the engine the flexibility to serve any model in a framework-agnostic environment.

There are two options to creating the model in ONNX format:

<details>

<summary><b>1) Export the ONNX/Config Files From SparseML</b></summary>

This pathway is relevant if you intend to deploy a model created using SparseML library.

After training your model with SparseML, locate the `.pth` file for the checkpoint you'd like to export and run the `SparseML` integrated export script below.

```bash
sparseml.image_classification.export_onnx \
    --arch-key resnet50 \
    --dataset imagenet \
    --dataset-path ~/datasets/ILSVRC2012 \
    --checkpoint-path ~/checkpoints/resnet50_checkpoint.pth
```
This creates `model.onnx` file.

The examples below use SparseZoo stubs, but simply pass the path to `model.onnx` in place of the stubs to use the local model.

</details>

<details>

<summary><b>2) Pass a SparseZoo Stub To DeepSparse</b></summary>

This pathway is relevant if you plan to use an off-the-shelf model from the SparseZoo. 

All of DeepSparse's `Pipelines` and APIs can use a SparseZoo stub in place of a local folder. 
The `Pipelines` use the stubs to locate and download the ONNX and config files from the SparseZoo repo.

All of DeepSparse's pipelines and APIs can use a SparseZoo stub in place of a local folder. 
The examples use SparseZoo stubs to highlight this pathway.

</details>

The examples below use option 2. However, you can pass the local path to the ONNX file as needed.

## Deployment APIs

DeepSparse provides both a Python `Pipeline` API and an out-of-the-box model
server that can be used for end-to-end inference in either Python
workflows or as an HTTP endpoint. Both options provide similar specifications
for configurations and support a variety of Image Classification models.

### Python API

`Pipelines` are the default interface for running inference with the
DeepSparse Engine.

Once a model is obtained, either through SparseML training or directly from SparseZoo,
a `Pipeline` can be used to easily facilitate end to end inference and deployment
of the sparsified image classification model.

If no model is specified to the `Pipeline` for a given task, the `Pipeline` will automatically
select a pruned and quantized model for the task from the SparseZoo that can be used for accelerated
inference. Note that other models in the SparseZoo will have different tradeoffs between speed, size,
and accuracy.

### HTTP Server

As an alternative to Python API, the DeepSparse Server allows you to
serve ONNX models and pipelines in HTTP. Configuring the server uses the same parameters and schemas as the `Pipelines`,
enabling simple deployment. Once launched, a `/docs` endpoint is created with full
endpoint descriptions and support for making sample requests.

An example deployment using a 95% pruned ResNet50 is given below.

For full documentation on deploying sparse image classification models with the
DeepSparse Server, see the [documentation for DeepSparse Server](/user-cases/deploying-deepsparse/deepsparse-server).

## Deployment Examples

The following section includes example usage of the `Pipeline` and server APIs for
various image classification models. Each example uses a SparseZoo stub to pull down the model,
but a local path to an ONNX file can also be passed as the `model_path`.

### Python API

Create a `Pipeline` to run inference with the following code. The `Pipeline` handles the pre-processing (e.g. subtracting by ImageNet 
means, dividing by ImageNet standard deviation) and post-processing so you can pass an raw image and recieve an class without any extra code.

```python
from deepsparse import Pipeline
cv_pipeline = Pipeline.create(
  task='image_classification',
  model_path='zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none',  # Path to checkpoint or SparseZoo stub
  class_names=None # optional dict / json mapping class ids to labels (if not using ImageNet classes)
)
input_image = "my_image.png" # path to input image
inference = cv_pipeline(images=input_image)
```

### HTTP Server

Spinning up:
```bash
deepsparse.server \
    task image_classification \
    --model_path "zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none" \
    --port 5543
```

Making a request:
```python
import requests

url = 'http://0.0.0.0:5543/predict/from_files'
path = ['goldfish.jpeg'] # just put the name of images in here
files = [('request', open(img, 'rb')) for img in path]
resp = requests.post(url=url, files=files)
```

## Benchmarking

The mission of Neural Magic is to enable GPU-class inference performance on commodity CPUs.
Want to find out how fast our sparse ONNX models perform inference? You can quickly run benchmarking tests on your own with a single CLI command.

You only need to provide the model path of a SparseZoo ONNX model or your own local ONNX model to get started:
```bash
deepsparse.benchmark zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none
```
Output:
```bash
Original Model Path: zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none
Batch Size: 1
Scenario: async
Throughput (items/sec): 299.2372
Latency Mean (ms/batch): 16.6677
Latency Median (ms/batch): 16.6748
Latency Std (ms/batch): 0.1728
Iterations: 2995
```

To learn more about benchmarking, refer to the appropriate documentation.
Also, check out our [Benchmarking Tutorial on GitHub  ](https://github.com/neuralmagic/deepsparse/tree/main/src/deepsparse/benchmark)!
