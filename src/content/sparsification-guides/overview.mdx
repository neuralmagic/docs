---
title: "Overview"
metaTitle: "Sparsification Overview"
metaDescription: "Overview of sparsification"
index: 0
---

# Sparsification Overview

## Sparsifying With SparseML

As described in the guides in this section, there are three ways to sparsify a model using SparseML:

- Recipes
- Python API
- SparseML CLI

Sparsification and recipe concepts are explored more here.

## Sparsification Workflows

SparseZoo and SparseML extend PyTorch and TensorFlow with features for creating sparse models trained on custom data. Together, they enable two workflows:

- **Sparse transfer learning**<br>This workflow fine-tunes a pre-sparsified foundation model (like ResNet-50 or BERT) from the SparseZoo onto a custom dataset while maintaining sparsity. This pathway works just like typical fine-tuning used in training CV and NLP models. Sparse transfer learning is strongly preferred if your model architecture is available in SparseZoo.

- **Sparsification from scratch**<br>This workflow applies training-aware pruning and quantization algorithms to any trained PyTorch, TensorFlow, or Hugging Face model, with fine-grained control of hyperparameters. This pathway requires more experimentation, but allows you to create a sparse version of any model.

### Sparse Transfer Learning

Sparse transfer learning is the easiest path to creating a sparse model trained on custom data. It is preferred for any scenario in which a pre-sparsified foundation model exists in SparseZoo.

Neural Magic's research team has invested many hours in creating state-of-the-art pruned and quantized versions of popular foundation models trained on large open datasets. These models (including the hyperparameters of the sparsification process) are publicly available in the SparseZoo.

SparseML enables you to fine-tune the pre-sparsified SparseZoo models onto custom data *while maintaining the same level of sparsity* (which we call **sparse transfer learning**). Under the hood, SparseML extends PyTorch and TensorFlow to update only non-zero weights during backprogation. You then can sparse transfer learn with just a single CLI command or five additional lines of code around a custom PyTorch training loop.

This means any engineer (without deep knowledge of cutting-edge sparsity algorithms) can easily create accurate, inference-optimized sparse models for specific tasks.

✅ Check out:
- Sparse transfer learning example [LINK??]
- Our pre-sparsified models on [SparseZoo](https://sparsezoo.neuralmagic.com/)

✅ Request:
- A model in our [Community Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)

### Sparsification From Scratch

SparseML enables you to create a sparse model from scratch. The library contains state-of-the-art sparsification algorithms, including pruning, distillation, and quantization techniques. Sparsification from scratch can be applied to any model, providing power-users a path to create sparse versions of a model.

Training-aware unstructured pruning and training-aware quantization are the best techniques for creating models with the highest levels of sparsity without suffering from much accuracy degradation.

- **Gradual magnitude pruning (GMP)**<br>GMP is the best algorithm for unstructured pruning. With GMP, pruning occurs gradually over a training run. Over several epochs or training steps, the least impactful weights are iteratively removed. The non-zero weights are then fine-tuned to the objective function. This iterative process enables the model to adjust to a new optimization space after pathways are removed before pruning again.

- **Quantization-aware training (QAT)**<br>QAT is the best algorithm for quantization. With QAT, fake quantization operators are injected into the graph before quantizable nodes for activations, and weights are wrapped with fake quantization operators. The fake quantization operators interpolate the weights and activations down to INT8 on the forward pass, but enable a full update of the weights at FP32 on the backward pass. The updates to the weights at FP32 throughout the training process allow the model to adapt to the loss of information from quantization on the forward pass.

Applying these algorithms correctly in an ad-hoc way is challenging. To solve this dilemma, Neural Magic created SparseML, which implements these algorithms on top of PyTorch and TensorFlow.

Using SparseML, you can apply the algorithms to your trained PyTorch and TensorFlow models with just five additional lines of code around a training loop. This enables ML engineers to shift focus and time from (re)building sparsity algorithms to running experiments and tuning hyperparameters of the pruning and quantization process.

These algorithms are built on top of sparsification recipes, enabling easy integration into custom ML training pipelines to sparsify most neural networks. Additionally, SparseML integrates with popular ML repositories like Hugging Face Transformers and Ultralytics YOLO. With these integrations, creating a recipe and passing it to a CLI is all you need to sparsify a model.

Aside from sparsification algorithms, SparseML contains generic export pathways for performant deployments. These export pathways ensure the model saves in the correct format and rewrites the inference graphs for performance, such as quantized operator folding. The results are simple to export CLIs and APIs that guarantee performance for sparsified models in their given deployment environment.

Ultimately, creating a sparse model from scratch is a form of architecture search. This is an inherently “research-like” exercise that requires tuning the hyperparameters of GMP and QAT, and running experiments to test accuracy with various changes to the model. SparseML dramatically increases the productivity of developers running these experiments.

✅ Check out:
- Sparsifying from scratch example [LINK???]
- Guide on [creating a hyperparameter recipe](/sparsification-guides/creating-sparseml-recipes)
- Blog on [pruning a model](https://neuralmagic.com/blog/pruning-overview/)

✅ Request:
A model in our [Community Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)

## Sparsification Recipes

Sparsification recipes are [YAML](https://yaml.org/) or [Markdown](https://www.markdownguide.org/) files that encode the instructions for how to sparsify or sparse transfer learn a model. These instructions include the sparsification algorithms to apply along with any hyperparameters. Recipes work with the SparseML library to easily apply sparse transfer learning or sparsification algorithms to any neural network and training pipeline.

All SparseML sparsification APIs are designed to work with recipes. The files encode the instructions needed for modifying the model and training process as a list of modifiers. Example modifiers can be anything from setting the learning rate for the optimizer to gradual magnitude pruning. The rest of the SparseML system is coded to parse the recipe files into a native format for the desired framework and apply the modifications to the model and training pipeline.

## Tutorials

### PyTorch

- [Sparse Transfer Learning with the CLI](https://github.com/neuralmagic/sparseml/blob/main/integrations/torchvision/tutorials/sparse-transfer-learning.md)
- [Sparse Transfer Learning with the Python API](https://github.com/neuralmagic/sparseml/blob/main/integrations/torchvision/tutorials/docs-torchvision-python-transfer-imagenette.ipynb)
- [Sparsify From Scratch with the Python API](https://github.com/neuralmagic/sparseml/blob/main/integrations/torchvision/tutorials/docs-torchvision-sparsify-from-scratch-resnet50-beans.ipynb)

### Hugging Face Transformers

- [Sparse Transfer Learning Overview with the Python API](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/sparse-transfer-learning-bert-python.md)
- [Sparse Transfer Learning Overview with the CLI](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/sparse-transfer-learning-bert.md)
- [Sparse Transfer Learning for Sentiment Analysis](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/sentiment-analysis/sentiment-analysis-cli.md), [for Text Classification](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/text-classification/text-classification-cli.md), [for Token Classification](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/token-classification/token-classification-cli.md), [for Question Answering](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers/tutorials/question-answering/question-answering-cli.md)

### Ultralytics YOLOv5

- [Sparse Transfer Learning with the CLI](https://github.com/neuralmagic/sparseml/blob/main/integrations/ultralytics-yolov5/tutorials/sparse-transfer-learning.md)
- [Sparsify From Scatch with the CLI](https://github.com/neuralmagic/sparseml/blob/main/integrations/ultralytics-yolov5/tutorials/sparsify-from-scratch.md)

### Links to Additional Examples

- [PyTorch](https://github.com/neuralmagic/sparseml/blob/main/integrations/torchvision#tutorials)
- [Hugging Face Transformers](https://github.com/neuralmagic/sparseml/blob/main/integrations/huggingface-transformers#tutorials)
- [Ultralytics YOLOv5](https://github.com/neuralmagic/sparseml/blob/main/integrations/ultralytics-yolov5#tutorials)
