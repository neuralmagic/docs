---
title: "Overview"
metaTitle: "Model Selection Overview"
metaDescription: "Overview of model selection"
index: 3000
skipToChild: True
---

# Model Selection Overview

<img src="/src/images/infographic-concepts.png">

DeepSparse accepts models in the ONNX format. ONNX models can be passed in one of two ways:

- Your own model, or
- SparseZoo model

## Your Own Model

You can provide your own ONNX models, whether dense or sparse. For example:

CODE (source?)

## SparseZoo Model

[SparseZoo is a constantly-growing repository](https://sparsezoo.neuralmagic.com) of sparsified (pruned and pruned-quantized) models with matching sparsification recipes for neural networks. SparseZoo simplifies and accelerates your time-to-value in building performant deep learning models with a collection of inference-optimized models and recipes from which to prototype.

Available via API and hosted in the cloud, the SparseZoo contains both baseline and models sparsified to different degrees of inference performance versus baseline loss recovery. Recipe-driven approaches built around sparsification algorithms allow you to use the models as provided, transfer learn from the models onto private datasets, or transfer the recipes to your architectures.

The [GitHub repository](https://github.com/neuralmagic/sparsezoo) contains the Python API code to handle the connection and authentication to the cloud.

<img alt="SparseZoo Flow" src="https://docs.neuralmagic.com/docs/source/infographics/sparsezoo.png" width="960px" />

### Highlights

- [Model Stub Architecture Overview](https://github.com/neuralmagic/sparsezoo/blob/main/docs/source/models.md)
- [Available Model Recipes](https://github.com/neuralmagic/sparsezoo/blob/main/docs/source/recipes.md)
- [sparsezoo.neuralmagic.com](https://sparsezoo.neuralmagic.com)

## Transferring a Sparsified Model

Sparse transfer learning is the easiest pathway for creating a sparse model fine-tuned on your datasets. Sparse transfer learning works by taking a sparse model pre-trained on a large dataset and fine-tuning it onto a smaller downstream dataset. SparseZoo and SparseML work together to accomplish this goal:

- SparseZoo is a growing repository of sparse models pre-trained on large datasets ready for fine-tuning.
- SparseML contains convenient training CLIs that run transfer-learn while preserving the same level of sparsity as the starting model.

By fine-tuning pre-sparsified models onto your dataset, you can avoid the time, money, and hyperparameter tuning involved with sparsifying a dense model from scratch. Once trained, deploy your model with DeepSparse for GPU-level performance on CPUs.
