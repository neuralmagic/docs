---
title: "Deploying on CPUs With DeepSparse"
metaTitle: "Deploying on CPUs With DeepSparse"
metaDescription: "Instructions for deploying on CPUs with DeepSparse"
index: 6000
---

# Deploying on CPUs With DeepSparse

## Pipeline Usage: Python APIs

For a supported task, the `Pipeline` class is a workhorse. Simply specify a task via the task argument and a model in ONNX format via the `model_path` argument.

```python
from deepsparse import Pipeline
example_pipeline = Pipeline.create(
    task="example_task",     # e.g. image_classification or question_answering
    model_path="model.onnx", # local model or SparseZoo stub
)

# pass raw, unprocessed input 
pipeline_inputs = ["The quick brown fox jumps over the lazy dog"]

# get back post-processed outputs
pipeline_outputs = example_pipeline(pipeline_inputs)
```

For an unsupported task, you will use `CustomTaskPipeline` to create a pipeline. Simply specify a pre- and post-processing function and a model in ONNX format.

``` python
from deepsparse.pipelines.custom_pipeline import CustomTaskPipeline

def preprocess(inputs):
    pass # define your function
def postprocess(outputs):
    pass # define your function

custom_pipeline = CustomTaskPipeline(
    model_path="custom_model.onnx",
    process_inputs_fn=preprocess,
    process_outputs_fn=postprocess,
)

# pass raw, unprocessed input
pipeline_inputs = ["The quick brown fox jumps over the lazy dog"]

# get back post-processed outputs
pipeline_outputs = custom_pipeline(pipeline_inputs)                       
```

✅ Learn more about:
- Using a model
- Using a model in a custom task
- Tasks and details on usage of supported use cases

Beyond pre- and post-processing, Pipelines have other useful utilities like data logging, multi-stream inference, and dynamic batch. Check out documentation on the Pipeline Class or the Multi-Stream Scheduling Overview.

## Server Usage: Launch From CLI

DeepSparse Server is launched from the CLI with configuration via either command line arguments or a configuration file.
With the command line argument path, you specify a task via the task argument (such as image_classification or question_answering) as well as a model (either a local ONNX file or a SparseZoo stub) via the model_path argument:
[CODE]
With the config file path, you create a YAML file that specifies the Server configuration. A YAML file looks like the following:
[CODE]
The Server is then launched with the following:
[CODE]
Clients interact with the Server via HTTP. Because the Server uses Pipelines internally, you can simply pass raw data to the Server and receive back post-processed predictions.  For example, you would do the following to query a Question Answering endpoint:
[CODE]
✅ Learn more about:
Deploying a model with DeepSparse Server
Tasks and detailed usage of each supported task with Server
The Server has additional useful utilities like data logging, multi-stream inference, multiple model inference, and dynamic batch. Check out documentation on the Server Class or the Multi-Stream Scheduling Overview.
The Engine class is the workhorse for this pathway. Simply call the constructor with your desired parameters to create an instance with the runtime. Once the engine is initialized, a pass list of numpy arrays (which are a batch of input tensors, the same as would be passed to a PyTorch model) and the engine will return a list of outputs. For example:
[CODE]
A MultiModelEngine is available if you want to interact directly with an engine running multiple models. This pathway is strongly preferred if you want to run multiple models on the same CPU.)
✅ Learn more about:
Code on GitHub
We also have a lower-level C++ API. Stay tuned for new documentation on this pathway or reach out in Community Slack for details.
