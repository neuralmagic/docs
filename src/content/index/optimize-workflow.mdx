---
title: "Optimizing Workflow"
metaTitle: "Optimizingn Workflow"
metaDescription: "Overview of Neural Magic Platform features"
index: 1000
---

## How to Create an Inference-Optimized Sparse Model

SparseML and SparseZoo extend PyTorch and TensorFlow with features for 
creating sparse models trained on custom data.

Together, they enable two workflows:
1. **Sparse Transfer Learning**: fine-tune a pre-sparsified foundation model (like ResNet-50 or BERT) from the SparseZoo
    onto a custom dataset
2. **Sparsification From Scratch**: apply training-aware pruning and quantization algorithms to any trained
    PyTorch, TensorFlow, and Hugging Face model, with fine-grained control of hyperparameters

### Sparse Transfer Learning

Sparse Transfer Learning is the easiest path to creating a sparse model trained on custom data 
and is preferred for any scenario where a pre-sparsified foundation model exists in SparseZoo.

Neural Magic's research team has invested many hours in creating state-of-the-art pruned and quantized verisons of popular foundation
models trained on large open datasets. These state-of-the-art models (including the hyperparameters of 
the sparsification process) are publically available in the SparseZoo. 

SparseML enables users to fine-tune the pre-sparsified models in SparseZoo onto custom data *while maintaining the same 
level of sparsity* (which we call "Sparse Transfer Learning"). Under the hood, SparseML extends PyTorch and TensorFlow 
to only update non-zero weights during backprogation. Users, then, can Sparse Transfer Learn
with just a single CLI command or five additional lines of code around a custom PyTorch training loop.

This means that any engineer (without deep knowledge of cutting-edge sparsity algorithms) can easily 
create accurate, inference-optimized sparse models for their specific use cases.

**Additional Resources**
- [Check out a sparse transfer learning example](https://docs.neuralmagic.com/get-started/transfer-a-sparsified-model)
- [Check out our pre-sparsified models on SparseZoo](https://sparsezoo.neuralmagic.com)
- [Request a model in our community Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)

### Sparsification From Scratch

Sparsification From Scratch can be applied to any model, providing power-users a path to create sparse versions of any model. 

As described in the conceptual section above, Training-Aware Unstructured Pruning and
Training-Aware Quantization are the best techniques for creating models with the highest levels of sparsity
without suffering from much accuracy degradation.

**Gradual Magnitude Pruning (GMP)** is the best algorithm for unstructured pruning:
- With GMP, pruning occurs gradually over a training run. Over several epochs or training steps the least impactful weights are 
iteratively removed. The non-zero weights are then fine-tuned to the objective function. 
This iterative process enables the model to adjust to a new optimization space after pathways are removed before pruning again. 

**Quantization-Aware Training (QAT)** is the best algorithm for quantization:
- With QAT, fake quantization operators are injected into the graph before quantizable nodes for activations, 
and weights are wrapped with fake quantization operators. The fake quantization operators interpolate the weights and 
activations down to INT8 on the forward pass but enable a full update of the weights at FP32 on the backward pass. 
The updates to the weights at FP32 throughout the training process allow the model to adapt to the loss of information 
from quantization on the forward pass.

Applying these algorithms correctly in an ad-hoc way is challenging. As such, Neural Magic created 
SparseML, which implements these algorithms on top of PyTorch and TensorFlow.

Using SparseML, users can apply these algorithms to their trained PyTorch and TensorFlow models 
with just five additional lines of code around a training loop. This enables ML Engineers to shift focus 
and time from (re)building sparsity algorithms to running experiments and tuning hyperparameters of the 
pruning and quantization process.

Ultimately, creating a sparse model from scratch is a form of architecture search. This is an inherently 
“research-like” exercise, which requires tuning hyperparameters of GMP and QAT and running experiments to test accuracy 
with various changes to the model. SparseML dramatically increases the productivity of developers
running these experiements.

#### Additional Resources
- [Check out a sparsifying from scatch example](https://docs.neuralmagic.com/get-started/sparsify-a-model/custom-integrations)
- [Check out our guide on creating a hyperparameter recipe](https://docs.neuralmagic.com/user-guides/recipes/creating)
- [Check out our blog on pruning a model](https://neuralmagic.com/blog/pruning-overview/)
- [Request a model in our community Slack](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ)
