---
title: "DeepSparse ARM Alpha"
metaTitle: "DeepSparse ARM Alpha"
metaDescription: "Description of how to trial DeepSparse on ARM"
index: 6000
---

# DeepSparse ARM Alpha

DeepSparse has Alpha support for ARM platforms! This page explains how to trial it.

[Sign up for early access to the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist)

## Want to Particpate in Our Trial?

Neural Magic is offering a $50 Amazon Gift card for a 30 minute conversation to discuss: 
- Experience with the ARM Alpha
- Requests for features for DeepSparse on ARM
- Requests for ARM platform support (e.g. Raspberry Pi, Rockchip, Qualcomm)

[Reach out to set up time to discuss](https://neuralmagic.com/contact/)

## Understand Alpha Status

DeepSparse on ARM is currently in Alpha state. 

It has the following limitations:
- **Long Compile Times** (compiling BERT on `M6g.16xlarge` takes XXX minutes)
- **Slower Performance than DeepSparse x86** 
- **No Performance Gains from Quantization**
- **Limited Set of Models** (we have tested ResNet-50, YOLOv5, YOLACT, and BERT)
- **AWS Graviton Only**

The general release of DeepSparse on ARM will deliver similiar compile times, performance, model, and platform support 
as the x86 version.

## Check Out Performance

The following throughput statistics were generated using `deepsparse-nightly==xxx` on an `M6g.16xlarge` instance (64 cores):

<p align="center">
    <img width="90%" src="https://raw.githubusercontent.com/neuralmagic/docs/arm-alpha-new/src/images/arm-benchmarks.png"/>
</p>


You can use DeepSparse's benchmarking script to replicate, filling in the `SPARSEZOO_STUB`.
```
deepsparse.benchmark SPARSEZOO_STUB --scenario sync --batch_size 64 --engine deepsparse
deepsparse.benchmark SPARSEZOO_STUB --scenario sync --batch_size 64 --engine onnxruntime
```

## Try The Alpha

### System Requirements

The General Release of DeepSparse for ARM will offer support for ARMv8+ devices, including server
systems like Ampere, edge devices like Rasbperry Pi, and mobile devices like Android. However, the ARM Alpha only runs 
on the following AWS Graviton2 and Graviton3 instances:
- Graviton2 General Purpose: `M6g`, `M6gd`, `T4g`
- Graviton2 Compute Optimized: `C6g`, `C6gd`, `C6gn`
- Graviton2 Memory Optimized: `R6g`, `R6gd`, `X2gd`
- Graviton2 Storage Optimized: `Im4gn`, `Is4gen`
- Graviton3 Compute Optimized: `C7g`, `C7gn`

### Install

The ARM Alpha is included in the DeepSparse nightly package, available for download via PyPI. We recommend using a virtual enviornment with Python.

Simply run the following on your Graviton instance:
```bash
pip install deepsparse-nightly[server]
```

### Models

The General Release of DeepSparse for ARM will offer support for all models in ONNX format. However, the 
ARM Alpha has been tested for accuracy on a subset a limited subset of models.
The following models have been tested for accuracy and performance with the DeepSparse ARM Alpha, 
with their corresponding SparseZoo stubs.

ResNet-50 (Image Classification):
```
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
```

YOLOv5 (Object Detection):
```
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned-aggressive_96
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned-aggressive_98
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95
```

YOLACT (Segmentation):
```
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/base-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned90-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned82_quant-none
```

BERT-base squad (Question Answering):
```
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned90-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

BERT-base sst2 (Sentiment Analysis):
```
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/base-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned90-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned80_quant-none-vnni
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/base-none 
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90-none
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90_quant-none
```

BERT-base conll2003 (Token Classification):
```
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/base-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned90-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned80_quant-none-vnni
```

DistilBERT squad (Question Answering):
```
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

## Get Started

Once you install the `deepsparse-nightly` package, the system will automatically detect that it is 
running on ARM. Therefore, once you install with PyPI, you can use DeepSparse as usual.

#### Benchmark Performance

Let's benchmark DeepSparse's throughtput on BERT to ONNX Runtime's on an AWS `M6g.16xlarge` instance.

Make sure you have ONNX Runtime installed:

```
pip install onnxruntime
```

At batch 64, ONNX runtime achieves 19 items/second running the standard BERT model.

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none -s sync -b 64 -e onnxruntime

> TBU
```

At batch 64, DeepSparse achieves 55 items/second running a pruned version of BERT - **this is a 3.5 performance gain**!

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none -s sync -b 64 -e deepsparse

> TBU
```

#### Deploy a Model

DeepSparse provides both a Python Pipeline API and an out-of-the-box model server that can be used for end-to-end 
inference in either Python workflows or as an HTTP endpoint. Both options provide similar specifications for configurations and support a variety of models.

Here's an example of the Pipeline API for the sentiment analysis task.

```
from deepsparse import Pipeline

sa_pipeline = Pipeline.create(task="sentiment-analysis")
inference = sa_pipeline("Snorlax loves my Tesla!")

> [{'label': 'LABEL_1', 'score': 0.9884248375892639}]  # positive sentiment
```

Here's an example spinning up a REST endpoint with DeepSparse Server from the CLI for the question answering task.
```
deepsparse.server \
    --task question-answering \
    --model_path "zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni"
```

Making a request:
```
import requests
url = "http://localhost:5543/predict" # Server's port default to 5543
obj = {
    "question": "Who is Mark?",
    "context": "Mark is batman."
}
response = requests.post(url, json=obj)
response.text

> '{"score":0.9534820914268494,"start":8,"end":14,"answer":"batman"}'
```

Check out [Use Cases](/use-cases/natural-language-processing/question-answering) for more details on each supported task.

## Next Steps

[Reach out to us to set up time to discuss your experience](https://neuralmagic.com/contact/)

[Sign up for early access to the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist)
