---
title: "DeepSparse ARM Alpha"
metaTitle: "DeepSparse ARM Alpha"
metaDescription: "Description of how to trial DeepSparse on ARM"
index: 6000
---

# DeepSparse ARM Alpha

With our recent 1.4 product release, we launched alpha support for DeepSparse on AWS Graviton and Ampere. We are working toward a general release across ARM servers, embedded (like Raspberry Pi), and mobile (like Android) platforms in the first half of 2023.

[Sign up to join the waitlist for the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist)

### Table of Contents

In this guide, we will:
- Explain the alpha status
- Assess current performance against the ONNX Runtime baseline
- Set up the DeepSparse ARM Alpha on an AWS Graviton or Ampere system
- Benchmark performance
- Deploy a model

### Want to Particpate in Our Trial?

Neural Magic is offering a gift card for a 45 minute conversation to discuss experience with the ARM Alpha and feature requests for DeepSparse on ARM (e.g. what platforms would you like to see supported?). [Reach out to set up time to discuss](https://neuralmagic.com/contact/)

## Understand Alpha Status

DeepSparse on ARM is currently in Alpha state. It has the following limitations:
- AWS Graviton and Ampere Only
- Long Model Compilation Times (e.g., compiling BERT on a `c7g.8xlarge` instance takes ~2.5 minutes)
- Slower Performance than DeepSparse on x86
- Performance and Accuracy Support for Limited Set of Models (we have tested ResNet-50, YOLOv5, and BERT)

Over the coming months as we approach general release, we will be finalizing DeepSparse on ARM to deliver similiar compile times, performance, model selection, and platform support as the x86 version.

## Current Performance

The following throughput statistics were generated on an AWS Graviton3 `c7g.8xlarge` instance (32 cores):

<p align="center">
    <img width="80%" src="https://raw.githubusercontent.com/neuralmagic/docs/arm-alpha-new/src/images/arm-benchmarks.png"/>
</p>

## Get Set Up

### System Requirements

The general release of DeepSparse for ARM will offer support for ARMv8+ devices, including server
systems like Ampere, edge devices like Rasbperry Pi, and mobile devices like Android. 

However, the ARM Alpha only runs on the following AWS Graviton and Ampere systems, including:
- AWS Graviton3, such as `c7g`, `c7gn`
- AWS Graviton2, such as `c6g`, `c6gd`, `c6gn`
- GCP Ampere Instances, such as `t2a`
- Ampere, such as `Altra` and `Altra Max`

### Model Requirements

The general release of DeepSparse on ARM will offer support for the same set of models as DeepSparse on x86. At current, we have tested accuracy and performance on a limited subset of models, including BERT, ResNet-50, and YOLOv5.

### Install via `pip`

The ARM Alpha is available in the DeepSpase-nightly, available for download via PyPI. We recommend using a virtual enviornment with Python.

Simply run the following on your Graviton instance or Ampere system:
```bash
pip install deepsparse-nightly[server]
```

Note: if you are running on Ubuntu on AWS Graviton3, your install may fail with `ERROR: Failed building wheel for psutil`. Run the following to install the necessary packages needed to build the wheel for `psutil`:
```
sudo apt-get install gcc python3-dev
```

## Benchmark Performance

Let's benchmark DeepSparse's throughtput on BERT on an AWS `c7g.8xlarge` instance using the `deepsparse.benchmark` utility.

#### ONNX Runtime Baseline

Make sure you have ONNX Runtime installed (`pip install onnxruntime`).

ORT achieves 15 items/second running the standard BERT model at batch 64 with sequence length 384.
```bash
deepsparse.benchmark zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/base-none -b64 -s sync -nstreams 1 -e onnxruntime

>> Original Model Path: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/base-none
>> Batch Size: 64
>> Scenario: sync
>> Throughput (items/sec): 14.9156
```

#### DeepSparse Dense Performance

With dense, unoptimized models, DeepSparse achieves a competitive performance with ONNX Runtime, reaching 16.5 items/second.

```bash
deepsparse.benchmark zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/base-none -b64 -s sync -nstreams 1

>> Original Model Path: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/base-none
>> Batch Size: 64
>> Scenario: sync
>> Throughput (items/sec): 16.5698
```

#### DeepSparse Sparse Performance

With sparsity applied to the model, DeepSparse achieves significant performance speedups over ONNX Runtime. With a 90% pruned version of BERT, DeepSparse achieves 50 items/second - **this is a 3.5x performance gain over the ORT baseline**!

```bash
deepsparse.benchmark zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none -b64 -s sync -nstreams 1

>> Original Model Path: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none
>> Batch Size: 64
>> Scenario: sync
>> Throughput (items/sec): 50.5135
```

#### DeepSparse Sparse-Quantized Performance

With sparsity and quantization applied to the model, DeepSparse achieves an additional performance speedup. With a 80% 4-block pruned-quantized version of BERT, DeepSparse achieves 71 items/second - **this is a 4.3x performance gain over the ORT baseline**!

```bash
deepsparse.benchmark zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned80_quant-none-vnni -b64 -s sync

>> Original Model Path: zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned80_quant-none-vnni
>> Batch Size: 64
>> Scenario: sync
>> Throughput (items/sec): 71.3083
```

*Note that DeepSparse on ARM currently only sees a performance gain over the FP32 baseline from quantizations with 4-block pruned models, where weights are pruned in groups of 4. In the general release of ARM, we will support performance speedups from quantization applied to unstructured sparse models*

[In SparseZoo](https://sparsezoo.neuralmagic.com/), you can tell if a model has been pruned in a 4-block structure if the stub ends in `-vnni`. For instance, the first model is 80% 4-block pruned with quantization and the second model is 90% pruned in an unstructured manner with quantization:
```
zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned80_quant-none-vnni
zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none
```

## Deploy a Model

Just like DeepSparse on x86, DeepSparse on ARM provides both a Python Pipeline API and an out-of-the-box model server. Both options provide similar specifications for configurations and support a variety of models.

Here's an example Pipeline for the sentiment analysis task:
```python
from deepsparse import Pipeline

zoo_stub = "zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/pruned90-none"
sa_pipeline = Pipeline.create(task="sentiment-analysis", model_path=zoo_stub)
inference = sa_pipeline("Snorlax loves my Tesla!")
print(inference)

> labels=['LABEL_1'] scores=[0.997743546962738]  # positive sentiment
```

Here's an example spinning up a REST endpoint for the question answering task:
```bash
deepsparse.server \
    --task question-answering \
    --model_path "zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned80_quant-none-vnni"
```

Making a request:
```python
import requests
url = "http://localhost:5543/predict" # Server's port default to 5543
obj = {
    "question": "Who is Mark?",
    "context": "Mark is batman."
}
response = requests.post(url, json=obj)
print(response.text)

> '{"score":0.9534820914268494,"start":8,"end":14,"answer":"batman"}'
```

## Next Steps

[Reach out to us to set up time to discuss your experience and feature / platform request](https://neuralmagic.com/contact/).

[Sign up for early access to the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist).
