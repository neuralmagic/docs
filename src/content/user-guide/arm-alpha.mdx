---
title: "DeepSparse ARM Alpha"
metaTitle: "DeepSparse ARM Alpha"
metaDescription: "Description of how to trial DeepSparse on ARM"
index: 6000
---

# DeepSparse ARM Alpha

DeepSparse has Alpha support for ARM platforms! 

This page explains how to trial it.

[Sign-up for early access to the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist)

## Feedback

Neural Magic is offering a $50 Amazon Gift card for a 30 minute conversation to discuss: 
- Experience with the ARM Alpha
- Requests for features for DeepSparse on ARM
- Requests for ARM platform support (e.g. Raspberry Pi, Rockchip, Qualcomm)

[Reach out to us to set up time to chat](https://neuralmagic.com/contact/)

## Installation

The ARM Alpha is included in the DeepSparse nightly package, available for download via PyPI. We recommend using a virtual enviornment with Python.

Simply run the following on your Graviton instance:
```bash
pip install deepsparse-nightly
```

## Alpha Status

DeepSparse on ARM is currently in Alpha state. 

It has the following limitations:
- **Long Compile Times** (compiling BERT on `M6g.16xlarge` takes XXX minutes)
- **Slower Performance than DeepSparse x86** 
- **No Performance Gains from Quantization**
- **Limited Set of Models** (we have tested ResNet-50, YOLOv5, YOLACT, and BERT)
- **AWS Graviton Only**

The general release of DeepSparse on ARM will deliver similiar compile times, performance, model, and platform support 
as the x86 version.

## Current Performance

The following performance statistics were generated using `deepsparse-nightly==xxx` on an `M6g.16xlarge` instance (64 cores).

<p align="center">
    <img src="/Users/robertgshaw/dev/docs/src/images/arm-alpha-throughput-compare.png"/>
</p>



The following commands were used to generate the performance numbers, filling in the `SPARSEZOO_STUB` for the model you want to test:
```
deepsparse.benchmark $SPARSEZOO_STUB --scenario sync --batch_size 64 --engine deepsparse
deepsparse.benchmark $SPARSEZOO_STUB --scenario sync --batch_size 64 --engine onnxruntime
```

## Requirements

### System Requirements

The General Release of DeepSparse for ARM will offer support for ARMv8+ devices, including server
systems like Ampere, edge devices like Rasbperry Pi, and mobile devices like Android and iPhone. 

**However, the ARM Alpha only runs on the following AWS Graviton2 and Graviton3 instances**:
- Graviton2 General Purpose: `M6g`, `M6gd`, `T4g`
- Graviton2 Compute Optimized: `C6g`, `C6gd`, `C6gn`
- Graviton2 Memory Optimized: `R6g`, `R6gd`, `X2gd`
- Graviton2 Storage Optimized: `Im4gn`, `Is4gen`
- Graviton3 Compute Optimized: `C7g`, `C7gn`

### Model Requirements

The General Release of DeepSparse for ARM will offer support for all models in ONNX format. 
**However, the ARM Alpha has been tested for accuracy on a subset a limited subset of models**.

The following models have been tested for accuracy and performance with the DeepSparse ARM Alpha, 
with their corresponding SparseZoo stubs.

ResNet-50 (Image Classification):
```
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
```

YOLOv5 (Object Detection):
```
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned-aggressive_96
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned-aggressive_98
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95
```

YOLACT (Segmentation):
```
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/base-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned90-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned82_quant-none
```

BERT-base squad (Question Answering):
```
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned90-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

BERT-base sst2 (Sentiment Analysis):
```
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/base-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned90-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned80_quant-none-vnni
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/base-none 
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90-none
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90_quant-none
```

BERT-base conll2003 (Token Classification):
```
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/base-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned90-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned80_quant-none-vnni
```

DistilBERT squad (Question Answering):
```
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

## Getting Started

Once you install the `deepsparse-nightly` package, the system will automatically detect that it is 
running on ARM. Therefore, once you install with PyPI, you can use DeepSparse as usual.

Checkout the [Use Cases section](/use-cases/natural-language-processing/question-answering) for more details on how to
setup an inference pipeline or REST endpoint with DeepSparse.

### Benchmarking Example

Let's benchmark DeepSparse's performance on BERT performance on an AWS `XXX` instance.

#### ONNX Runtime Performance

Install ONNX Runtime with `pip install onnxruntime`.

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none -s sync -b 64 -e onnxruntime

> output
> output
> output
```

At batch 64, ONNX runtime achieves 19 items/second running BERT.

Pruned Performance:

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none -s sync -b 64 -e deepsparse

> output
> output
> output
```

At batch 64, DeepSparse achieves 55 items/second running a pruned version of BERT - **this is a 3.5 performance gain**!

## Next Steps

[Sign-up for early access to the general release of DeepSparse on ARM](https://neuralmagic.com/deepsparse-arm-waitlist).

[Reachout to us to set-up time for platforms or feature request for DeepSparse on ARM](https://neuralmagic.com/contact/).