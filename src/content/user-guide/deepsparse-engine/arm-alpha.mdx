---
title: "ARM Alpha"
metaTitle: "ARM Alpha"
metaDescription: "Description of how to trial DeepSparse on ARM"
index: 7000
---

# DeepSparse on ARM

As of December 2022's 1.3 release, DeepSparse has Alpha support for running on AWS Graviton. 

This page explains how to trial the ARM Alpha.

## Installation

The ARM Alpha is included in the DeepSparse nightly package, available for download via PyPI. We recommend using a virtual enviornment.

```bash
pip install deepsparse-nightly
```

## Requirements

### System Requirements

The General Release of DeepSparse for ARM will offer support for ARMv8+ devices, including server
systems like Ampere, edge devices like Rasbperry Pi, and mobile devices like Android and iPhone. 
However, at this time, the **DeepSparse ARM Alpha has only runs on AWS Graviton instances**.

### Model Requirements

The General Release of DeepSparse for ARM will offer support for a broad range of models, equivalent to
DeepSparse for x86. However, at this time, the **DeepSparse ARM Alpha has been tested and certified to 
work with a limited subset of models**.

The following models have been tested for accuracy and performance with the DeepSparse ARM Alpha, 
with their corresponding SparseZoo stubs.

ResNet-50 (Image Classification):
```
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/base-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none
zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95_quant-none
```

YOLOv5 (Object Detection):
```
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned-aggressive_96
zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned_quant-aggressive_94
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/base-none
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned-aggressive_98
zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned_quant-aggressive_95
```

YOLACT (Segmentation):
```
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/base-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned90-none
zoo:cv/segmentation/yolact-darknet53/pytorch/dbolya/coco/pruned82_quant-none
```

BERT-base squad (Question Answering):
```
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned90-none
zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/bert-base_cased/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

BERT-base sst2 (Sentiment Analysis):
```
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/base-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned90-none
zoo:nlp/sentiment_analysis/bert-base/pytorch/huggingface/sst2/12layer_pruned80_quant-none-vnni
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/base-none 
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90-none
zoo:nlp/sentiment_analysis/bert-base_cased/pytorch/huggingface/sst2/pruned90_quant-none
```

BERT-base conll2003 (Token Classification):
```
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/base-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned90-none
zoo:nlp/token_classification/bert-base/pytorch/huggingface/conll2003/12layer_pruned80_quant-none-vnni
```

DistilBERT squad (Question Answering):
```
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/base-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned90-none
zoo:nlp/question_answering/distilbert-none/pytorch/huggingface/squad/pruned80_quant-none-vnni
```

## Getting Started

Once you install the `deepsparse-nightly` package, the system will automatically detect that it is 
running on ARM. Therefore, once you install with PyPI, you can use DeepSparse as usual.

Checkout the [Use Cases section](/use-cases/natural-language-processing/question-answering) for more details on how to
setup an inference pipeline or REST endpoint with DeepSparse.

### Benchmarking Example

Let's benchmark DeepSparse's performance on BERT performance on an AWS `XXX` instance.

#### ONNX Runtime Performance

Install ONNX Runtime with `pip install onnxruntime`.

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none -s sync -b 64 -e onnxruntime

> output
> output
> output
```

Dense Performance:

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none -s sync -b 64

> output
> output
> output
```

We can see that for the dense model, DeepSparse's throughtput beats ORT by XX.

Pruned Performance:

```bash
deepsparse.benchmark zoo:cv/classification/resnet_v1-50/pytorch/sparseml/imagenet/pruned95-none -s sync -b 64

> output
> output
> output
```

When applying sparsity to the model, DeepSparse's throughput beats the ORT baseline by XX.

Pruned-Quantized Performance:

```bash
deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni -s sync -b 64

> output
> output
> output
```

When applying pruning and quantization to the model, DeepSparse's throughput beat the ORT baseline by XX.

## Feedback / Early Access

[Sign-up on the waitlist](link/to/form) to get early access to the general release of DeepSparse for ARM.

Neural Magic is offering $50 for a 30 minute conversation to discuss:
- Experience with the DeepSparse ARM Alpha
- Requests for features associated with general release of DeepSparse for ARM (edge or server)

[Reachout to us to set-up time to chat](https://neuralmagic.com/contact/).