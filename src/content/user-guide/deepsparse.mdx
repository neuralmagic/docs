---
title: "DeepSparse"
metaTitle: "User Guides for DeepSparse Engine"
metaDescription: "User Guides for DeepSparse Engine"
index: 4000
---

# Overview

This page describes how to use DeepSparse to deploy a model on CPUs.

## Model Format

DeepSparse accepts models in the ONNX format. 

ONNX files can either be passed as a SparseZoo stub, which identifies a model and downloads an 
ONNX file from SparseZoo or a local path to an ONNX file.

Checkout the [use case section](/use-cases) for details on exporting models trained in SparseML to ONNX.

## Benchmarking Performance

DeepSparse's key differentiation is its performance, as it offers GPU-class performance on CPUs
for inference-optimized sparse models.

To demonstrate its performance, DeepSparse offers a benchmarking utility that enables you
to quickly assess throughput and latency under a variety of scenarios and compare performance.

Let's walk through an example compareing ORT and DeepSparse's performance on BERT on a 16 core
`c6i.8xlarge` AWS instance.

### Dense Performance

For unoptimized dense models, DeepSparse's performance is competitive with CPU-based inference runtimes 
like ONNX Runtime. 

Make sure you have ONNX Runtime installed.
```bash
pip install onnxruntime
```

At batch size 32, ORT achieves XXX items/second with the unoptimized BERT model.
```bash
deepsparse.benchmark zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/base-none -b 32 -s sync -nstreams 1
```

At batch size 32, DeepSparse achieves XXX items/second with the unoptimized BERT model.
```bash
deepsparse.benchmark zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/base-none -b 32 -s sync -nstreams 1
```

### Sparse Performance

DeepSparse's performance shines when optimization techniques like pruning and quantization are
applied to a model. We will compare ONNX Runtime against DeepSparse with a 90% pruned and quantized
BERT model.

At batch size 32, ORT achives XXX items/second with the pruned-quantized BERT model. There is no performance
gain from sparsity. 
```bash
deepsparse.benchmark zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none -b 32 -s sync -nstreams 1 -e onnxruntime

```

At batch size 32, DeepSparse achieves XXX items/second with the pruned-quantized BERT model. This is an
YYx performance gain over ONNX Runtime.
```
deepsparse.benchmark zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none -b 32 -s sync -nstreams 1
```

## Deployment APIs

Now that we have seen DeepSparse's performance gains, let's take a look at how we can add DeepSparse to an application.

DeepSparse includes three deployment APIs:
- Engine is the lowest-level API. With Engine, you pass input tensors and recieve the raw logits.
- Pipeline wraps the Engine with pre- and post-processing. With Pipeline, you pass raw data and
recieve the prediction.
- Server wraps Pipelines with a REST API using FastAPI. With Server, you send raw data to an endpoint over HTTP
and recieve the prediction.

Let's walk through a simple example of each API to give a sense of usage. As an example, we will use
the sentiment analysis use-case with a 90% pruned-quantized version of BERT. 

Check out the [use case section](/use-cases) for details on the APIs of each supported use case.

### Engine

Engine is the lowest-level API, allowing you to run inference directly on input tensors.

The example below downloads a 90% pruned-quantized BERT model for sentiment analysis 
in ONNX format from SparseZoo, compiles the model, and runs inference on randomly generated input.

```python
from deepsparse import compile_model
from deepsparse.utils import generate_random_inputs, model_to_path

# download onnx, compile model
zoo_stub = "zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none"  
batch_size = 1
bert_model = compile_model(
  model=zoo_stub,         # sparsezoo stub or path/to/local/model.onnx
  batch_size=batch_size   # default is batch 1
)

# run inference (input is raw numpy tensors, output is raw scores)
inputs = generate_random_inputs(model_to_path(zoo_stub), batch_size)
output = bert_model(inputs)
print(output)

# > [array([[-0.3380675 ,  0.09602544]], dtype=float32)] << raw scores
```

### Pipeline

Pipeline is the default API for interacting with DeepSparse. Similar to Hugging Face Pipelines,
DeepSparse Pipelines wrap Engine with pre- and post-processing (as well as other utilities), 
enabling you to send raw data to DeepSparse and recieve the post-processed prediction.

The example below downloads a 90% pruned-quantized BERT model for sentiment analysis 
in ONNX format from SparseZoo, sets up a pipeline, and runs inference on sample data.

```python
from deepsparse import Pipeline

# download onnx, set up pipeline
zoo_stub = "zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none"  
batch_size = 1
sentiment_analysis_pipeline = Pipeline.create(
  task="sentiment-analysis",    # name of the task
  model_path=zoo_stub,          # zoo stub or path to local onnx file
  batch_size=batch_size         # default is batch 1
)

# run inference (input is a sentence, output is the prediction)
prediction = sentiment_analysis_pipeline("I love using DeepSparse Pipelines")
print(prediction)
# > labels=['positive'] scores=[0.9954759478569031]
```

### Server

Server wraps Pipelines with REST APIs, that make it easy to stand up a model serving endpoint
running DeepSparse. This enables you to send raw data to DeepSparse over HTTP and recieve the post-processed
predictions.

DeepSparse Server is launched from the command line, configured via arguments or a server configuration file.

The following downloads a 90% pruned-quantized BERT model for sentiment analysis in ONNX format
from SparseZoo and launches a sentiment analysis endpoint:

```bash
deepsparse.server \
  --task sentiment-analysis \
  --model_path zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none
```

Alternatively, the following configuration file can launch the Server.

```yaml
# config.yaml

endpoints:
  - task: sentiment-analysis
    route: /predict
    model: zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none
    batch_size: 1
```

Spinning up:
```bash
deepsparse.server --config-file config.yaml
```

You should see Uvicorn report that it is running on port 5543. Navigating to the `/docs` endpoint will
show the exposed routes as well as sample requests.

We can then send a request over HTTP. In this example, we will use the Python requests package
to format the HTTP.

```python
import requests

url = "http://localhost:5543/predict" # Server's port default to 5543
obj = {"sequences": "Snorlax loves my Tesla!"}

response = requests.post(url, json=obj)
print(response.text)
# {"labels":["positive"],"scores":[0.9965094327926636]}
```

## Advanced Functionality

This user guide offers more information for exploring additional and advanced functionality for DeepSparse.

<LinkCards>
  <LinkCard href="./hardware-support" heading="Supported Hardware">
    Lists supported hardware for DeepSparse, including CPU types and instruction sets.
  </LinkCard>

  <LinkCard href="./scheduler" heading="Inference Types">
    Describes inference types and tradeoffs with DeepSparse Scheduler, such as single and multi-stream.
  </LinkCard>

  <LinkCard href="./benchmarking" heading="Benchmarking">
    Explains how to benchmark ONNX models with DeepSparse.
  </LinkCard>

  <LinkCard href="./diagnostics-debugging" heading="Diagnostics/Debugging">
    Provides logging guidance for diagnosing and debugging any issues.
  </LinkCard>

  <LinkCard href="./numactl-utility" heading="numactl Utility">
    Explains how to use the numactl utility for controlling resource utilization with DeepSparse.
  </LinkCard>

  <LinkCard href="./logging" heading="DeepSparse Logging">
    Explains how to use DeepSparse Logging for monitoring production models.
  </LinkCard>
</LinkCards>
