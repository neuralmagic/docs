---
title: "Home"
metaTitle: "Neural Magic Docs"
metaDescription: "Documentation for the Neural Magic Platform"
index: 0
---

# Neural Magic Platform Documentation

Deploy deep learning models on commodity CPUs with GPU-class performance.

## Why DeepSparse on CPUs?

CPU-based deep learning deployments on commodity hardware are simple and scalable.

Because DeepSparse reaches GPU-class performance with commodity CPUs, users no longer need to tether deployments to
accelerators to reach the performance needed for production. Free from specialized hardware, 
deployments can take advantage of the flexibiloty and scalability of software-defined inference:
- Deploy the same model/runtime on any hardware from Intel to AMD to ARM and from cloud to data center to edge, including on pre-existing systems
- Scale vertically from 1 to 192 cores, tailoring the footprint to an app's exact needs
- Scale horizontally with standard Kubernetes, including using services like EKS/GKE
- No wrestling with drivers, operator support, and compatibility issues

Simply put, deep learning deployments no longer need to choose between the performance of GPUs and simplicty of software!

## Neural Magic Platform

The Neural Magic Platform enables two major workflows.

### 1. Optimize a Model for Inference

**SparseML** and **SparseZoo** work together to optimize models for inference with
techniques like pruning and quantization (which we call "sparsity").

- [SparseML](/products/sparseml) is an open-source library that extends PyTorch and TensorFlow to simplify the process of 
  applying sparsity algorithms. Via simple CLI scipts or 5 lines of code, users can sparsify any model from scratch 
  or sparse transfer learn from pre-sparsified versions of foundation models like ResNet, YOLOv5, or BERT.

- [SparseZoo](/products/sparsezoo) is an open-source repository of pre-sparsified models
  (for example, sparse ResNet-50 has 95% of weights set to 0 while maintaining 99% of the baseline accuracy). SparseZoo is integrated with 
  SparseML, making it trival for users to fine-tune from sparse model (which we call "Sparse Transfer Learning") onto their data.

### 2. Deploy a Model on CPUs

**DeepSparse** runs inference-optimized sparse models with GPU-class performance on CPUs.

- [DeepSparse](/products/deepsparse) is a deep learning deployment SDK, which includes an inference runtime wrapped with APIs.
  With inference-optimized sparse models, the runtime offers GPU-class performance on commodity hardware. 
  The APIs enable users to integrate a model into an application and to monitor the performance of models running in production.

## Docs Content

The documentation is organized into several sections:

- **GET STARTED** provides install instructions and a tour of major functionality
- **USE CASES** walks through detailed examples using SparseML and DeepSparse
- **USER GUIDE** shows more advanced functionality with specific tasks
- **PRODUCTS** provides API-level docs for all classes and functions 
- **DETAILS** includes research papers and a glossary of terms

**Not Sure Where to Start?**
- Jump to [Optimize For Inference](/index/optimize-workflow) to learn about using Sparsity for inference speedups
- Jump to [Deploy on CPUs](/index/deploy-workflow) to learn about the benefits of deploying on CPUs
- Jump to [Quick Tour](/index/quick-tour) for a run through of our capabilities.

## External Resources

✅ Join our [community](https://join.slack.com/t/discuss-neuralmagic/shared_invite/zt-q1a1cnvo-YBoICSIw3L1dmQpjBeDurQ) if you need any help
or [subscribe](https://neuralmagic.com/deep-sparse-community/#subscribe) for regular email updates.

✅ Check out our [GitHub repositories](https://github.com/neuralmagic) and give us a ⭐.

✅ Help us improve this [documentation](https://github.com/neuralmagic/docs).
