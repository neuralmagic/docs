{"componentChunkName":"component---src-root-jsx","path":"/user-guides/recipes/creating","result":{"data":{"site":{"siteMetadata":{"title":null,"docsLocation":"https://docs.neuralmagic.com"}},"mdx":{"fields":{"id":"2b874d9c-9c67-549d-8c79-b9b261359734","title":"Creating","slug":"/user-guides/recipes/creating","githubURL":"https://github.com/neuralmagic/docs/blob/main/src/content/user-guides/recipes/creating.mdx"},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Creating\",\n  \"metaTitle\": \"Creating Sparsification Recipes\",\n  \"metaDescription\": \"Creating Sparsification Recipes\",\n  \"index\": 1000\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Creating Sparsification Recipes\"), mdx(\"p\", null, \"All SparseML Sparsification APIs are designed to work with recipes.\\nThe files encode the instructions needed for modifying the model and/or training process as a list of modifiers.\\nExample modifiers can be anything from setting the learning rate for the optimizer to gradual magnitude pruning.\\nThe files are written in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://yaml.org/\"\n  }, \"YAML\"), \" and stored in YAML or\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.markdownguide.org/\"\n  }, \"Markdown\"), \" files using\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://assemble.io/docs/YAML-front-matter.html\"\n  }, \"YAML front matter.\"), \"\\nThe rest of the SparseML system is coded to parse the recipe files into a native format for the desired framework,\\nand apply the modifications to the model and training pipeline.\"), mdx(\"p\", null, \"In a recipe, modifiers must be written in a list that includes \\\"modifiers\\\" in its name.\"), mdx(\"p\", null, \"The easiest ways to get or create recipes are by either using the pre-configured recipes in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/sparsezoo\"\n  }, \"SparseZoo\"), \" or using \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/neuralmagic/sparsify\"\n  }, \"Sparsify's\"), \" automatic creation.\\nEspecially for users performing sparse transfer learning from our pre-sparsified models in the SparseZoo, we highly recommend using the\\npre-made transfer learning recipes found on SparseZoo. However, power users may be inclined to create their recipes to enable more\\nfine-grained control or add custom modifiers when sparsifying a new model from scratch.\"), mdx(\"p\", null, \"A sample recipe for pruning a model generally looks like the following:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"version: 0.1.0\\nmodifiers:\\n    - !EpochRangeModifier\\n        start_epoch: 0.0\\n        end_epoch: 70.0\\n\\n    - !LearningRateModifier\\n        start_epoch: 0\\n        end_epoch: -1.0\\n        update_frequency: -1.0\\n        init_lr: 0.005\\n        lr_class: MultiStepLR\\n        lr_kwargs: {'milestones': [43, 60], 'gamma': 0.1}\\n\\n    - !GMPruningModifier\\n        start_epoch: 0\\n        end_epoch: 40\\n        update_frequency: 1.0\\n        init_sparsity: 0.05\\n        final_sparsity: 0.85\\n        mask_type: unstructured\\n        params: ['sections.0.0.conv1.weight', 'sections.0.0.conv2.weight', 'sections.0.0.conv3.weight']\\n\")), mdx(\"h2\", null, \"Modifiers Introduction\"), mdx(\"p\", null, \"Recipes can contain multiple modifiers, each modifying a portion of the training process in a different way.\\nIn general, each modifier will have a start and end epoch for when the modifier should be active.\\nThe modifiers will start at \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch\"), \" and run until \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \".\\nNote that it does not run \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"through\"), \" \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \".\\nAdditionally, all epoch values support decimal values such that they can be started anywhere within an epoch.\\nFor example, \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch: 2.5\"), \" will start in the middle of the second training epoch.\"), mdx(\"p\", null, \"The most commonly used modifiers are enumerated as subsections below.\"), mdx(\"h2\", null, \"Training Epoch Modifiers\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"EpochRangeModifier\"), \" controls the range of epochs for training a model.\\nEach supported ML framework has an implementation to enable easily retrieving this number of epochs.\\nNote that this is not a hard rule and, if other modifiers have a larger \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \" or smaller \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch\"), \",\\nthose values will be used instead.\"), mdx(\"p\", null, \"The only parameters that can be controlled for \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"EpochRangeModifier\"), \" are the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \".\\nBoth parameters are required:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" indicates the start range for the epoch (0 indexed).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"end_epoch\"), \" indicates the end range for the epoch.\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !EpochRangeModifier\\n       start_epoch: 0.0\\n       end_epoch: 25.0\\n\")), mdx(\"h2\", null, \"Pruning Modifiers\"), mdx(\"p\", null, \"The pruning modifiers handle \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/blog/pruning-overview/\"\n  }, \"pruning\"), \"\\nthe specified layer(s) in a given model.\"), mdx(\"h3\", null, \"ConstantPruningModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"ConstantPruningModifier\"), \" enforces the sparsity structure and level for an already pruned layer(s) in a model.\\nThe modifier is generally used for transfer learning from an already pruned model or\\nto enforce sparsity while quantizing.\\nThe weights remain trainable in this setup; however, the sparsity is unchanged.\"), mdx(\"p\", null, \"The required parameter is:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"params\"), \"indicates the parameters in the model to prune.\\nThis can be set to a string containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"__ALL__\"), \" to prune all parameters, a list to specify the targeted parameters,\\nor regex patterns prefixed by 're:' of parameter name patterns to match.\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['blocks.1.conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['mnist_net/blocks/conv0/conv']\"), \" for TensorFlow.\\nRegex can also be used to match all conv params: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*/conv']\"), \" for TensorFlow.\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !ConstantPruningModifier\\n        params: __ALL__\\n\")), mdx(\"h4\", null, \"GMPruningModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"GMPruningModifier\"), \" prunes the parameter(s) in a model to a\\ntarget sparsity (percentage of 0s for a layer's parameter/variable)\\nusing \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://neuralmagic.com/blog/pruning-gmp/\"\n  }, \"gradual magnitude pruning.\"), \"\\nThis is done gradually from an initial to final sparsity (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"init_sparsity\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"final_sparsity\"), \")\\nover a range of epochs (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \") and updated at a specific interval defined by the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"update_frequency\"), \".\\nFor example, using the following settings:\"), mdx(\"p\", null, mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"start_epoch: 0\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch: 5\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"update_frequency: 1\"), \",\\n\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"init_sparsity: 0.05\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"final_sparsity: 0.8\"), \" \"), mdx(\"p\", null, \"will do the following.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"At epoch 0, set the sparsity for the specified param(s) to 5%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Once every epoch, gradually increase the sparsity toward 80%\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"By the start of epoch 5, stop pruning and set the final sparsity for the specified parameter(s) to 80%\")), mdx(\"p\", null, \"The required parameters are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"params\"), \" indicates the parameters in the model to prune.\\nThis can be set to a string containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"__ALL__\"), \" to prune all parameters, a list to specify the targeted parameters,\\nor regex patterns prefixed by 're:' of parameter name patterns to match.\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['blocks.1.conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['mnist_net/blocks/conv0/conv']\"), \" for TensorFlow.\\nRegex can also be used to match all conv params: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*/conv']\"), \" for TensorFlow.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"init_sparsity\"), \" is the decimal value for the initial sparsity with which to start pruning.\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" will set the sparsity for the parameter/variable to this value.\\nGenerally, this is kept at 0.05 (5%).\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"final_sparsity\"), \" is the decimal value for the final sparsity with which to end pruning.\\nBy the start of \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"end_epoch\"), \" will set the sparsity for the parameter/variable to this value.\\nGenerally, this is kept in a range from 0.6 to 0.95, depending on the model and layer.\\nAnything less than 0.4 is not useful for performance.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" sets the epoch at which to start the pruning (0 indexed).\\nThis supports floating point values to enable starting pruning between epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"end_epoch\"), \" sets the epoch before which to stop pruning.\\nThis supports floating point values to enable stopping pruning between epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"update_frequency\"), \" is the number of epochs/fractions of an epoch between each pruning step.\\nIt supports floating point values to enable updating inside of epochs.\\nGenerally, this is set to update once per epoch (\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"1.0\"), \").\\nHowever, if the loss for the model recovers quickly, it should be set to a lesser value.\\nFor example, set it to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"0.5\"), \" for once every half epoch (twice per epoch).\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !GMPruningModifier\\n        params: ['blocks.1.conv']\\n        init_sparsity: 0.05\\n        final_sparsity: 0.8\\n        start_epoch: 5.0\\n        end_epoch: 20.0\\n        update_frequency: 1.0\\n\")), mdx(\"h3\", null, \"Quantization Modifiers\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"QuantizationModifier\"), \" sets the model to run with\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://pytorch.org/docs/stable/quantization.html\"\n  }, \"quantization-aware training (QAT).\"), \"\\nQAT emulates the precision loss of int8 quantization during training so weights can be\\nlearned to limit any accuracy loss from quantization.\\nOnce the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"QuantizationModifier\"), \" is enabled, it cannot be disabled (no \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"end_epoch\"), \").\\nQuantization zero points are set to be asymmetric for activations and symmetric for weights.\\nCurrently, quantization modifiers are available only in PyTorch.\"), mdx(\"p\", null, \"Notes:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"ONNX exports of PyTorch QAT models will be QAT models themselves (emulated quantization).\\nTo convert your QAT ONNX model to a fully quantizerd model, use\\nthe script \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"scripts/pytorch/model_quantize_qat_export.py\"), \" or the function\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"neuralmagicML.pytorch.quantization.quantize_qat_export\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If performing QAT on a sparse model, you must preserve sparsity during QAT by\\napplying a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"ConstantPruningModifier\"), \" or have already used a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"GMPruningModifier\"), \" with\\n\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"leave_enabled\"), \" set to True.\")), mdx(\"p\", null, \"The required parameter is:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" sets the epoch to start QAT. This supports floating-point values to enable\\nstarting pruning between epochs.\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !QuantizationModifier\\n        start_epoch: 0.0\\n\")), mdx(\"h3\", null, \"Learning Rate Modifiers\"), mdx(\"p\", null, \"The learning rate modifiers set the learning rate (LR) for an optimizer during training.\\nIf you are using an Adam optimizer, then generally, these are not useful.\\nIf you are using a standard stochastic gradient descent optimizer, these give a convenient way to control the LR.\"), mdx(\"h4\", null, \"SetLearningRateModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SetLearningRateModifier\"), \" sets the LR for the optimizer to a specific value at a specific point\\nin the training process.\"), mdx(\"p\", null, \"Required parameters are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" is the epoch in the training process to set the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"learning_rate\"), \" value for the optimizer.\\nThis supports floating point values to enable setting the LR between epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"learning_rate\"), \" is the floating-point value to set as the LR for the optimizer at \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \".\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !SetLearningRateModifier\\n        start_epoch: 5.0\\n        learning_rate: 0.1\\n\")), mdx(\"h4\", null, \"LearningRateModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"LearningRateModifier\"), \" sets schedules for controlling the LR for an optimizer during training.\\nIf you are using an Adam optimizer, then generally, these are not useful.\\nIf you are using a standard stochastic gradient descent optimizer, these give a convenient way to control the LR.\\nProvided schedules from which to choose are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"ExponentialLR\"), \" multiplies the LR by a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \" value every epoch.\\nTo use this, \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_kwargs\"), \" should be set to a dictionary containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \".\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"{'gamma': 0.9}\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"StepLR\"), \" multiplies the LR by a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \" value after a certain epoch period defined by \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"step\"), \".\\nTo use this, \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_kwargs\"), \" must be set to a dictionary containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"step_size\"), \".\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"{'gamma': 0.9, 'step_size': 2.0}\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"MultiStepLR\"), \" multiplies the LR by a \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \" value at specific epoch points defined by \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"milestones\"), \".\\nTo use this, \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_kwargs\"), \" must be set to a dictionary containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"gamma\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"milestones\"), \".\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"{'gamma': 0.9, 'milestones': [2.0, 5.5, 10.0]}\"))), mdx(\"p\", null, \"Required parameters are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" sets the epoch at which to start modifying the LR (0 indexed).\\nThis supports floating point values to enable starting pruning between epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"end_epoch\"), \" sets the epoch before which to stop modifying the LR.\\nThis supports floating point values to enable stopping pruning between epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_class\"), \" is the LR class to use, one of \", \"[\", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"ExponentialLR\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"StepLR\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"MultiStepLR\"), \"]\", \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_kwargs\"), \" is the named argument for the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"lr_class\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"init_lr\"), \" \", \"[Optional]\", \" is the initial LR to set at \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" and to use for creating the schedules.\\nIf not given, the optimizer's current LR will be used at startup.\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !LearningRateModifier\\n       start_epoch: 0.0\\n       end_epoch: 25.0\\n       lr_class: MultiStepLR\\n       lr_kwargs:\\n           gamma: 0.9\\n           milestones: [2.0, 5.5, 10.0]\\n       init_lr: 0.1\\n\")), mdx(\"h3\", null, \"Params/Variables Modifiers\"), mdx(\"h4\", null, \"TrainableParamsModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"TrainableParamsModifier\"), \" controls the parameters that are marked as trainable for the current optimizer.\\nThis is generally useful when transfer learning to easily mark which parameters should or should not be frozen/trained.\"), mdx(\"p\", null, \"The required parameter is:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"params\"), \" indicates the names of parameters to mark as trainable or not.\\nThis can be set to a string containing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"__ALL__\"), \" to mark all parameters, a list to specify the targeted parameters,\\nor regex patterns prefixed by 're:' of parameter name patterns to match.\\nFor example: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['blocks.1.conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['mnist_net/blocks/conv0/conv']\"), \" for TensorFlow.\\nRegex can also be used to match all conv params: \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*conv']\"), \" for PyTorch and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"['re:.*/conv']\"), \" for TensorFlow.\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !TrainableParamsModifier\\n      params: __ALL__\\n\")), mdx(\"h3\", null, \"Optimizer Modifiers\"), mdx(\"h4\", null, \"SetWeightDecayModifier\"), mdx(\"p\", null, \"The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SetWeightDecayModifier\"), \" sets the weight decay (L2 penalty) for the optimizer to a\\nspecific value at a specific point in the training process.\"), mdx(\"p\", null, \"Required parameters are:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \" is the epoch in the training process to set the \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"weight_decay\"), \" value for the\\noptimizer. This supports floating point values to enable setting the weight decay\\nbetween epochs.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"weight_decay\"), \" is the floating point value to set as the weight decay for the optimizer\\nat \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"start_epoch\"), \".\")), mdx(\"p\", null, \"For example:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"    - !SetWeightDecayModifier\\n        start_epoch: 5.0\\n        weight_decay: 0.0\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","tableOfContents":{"items":[{"url":"#creating-sparsification-recipes","title":"Creating Sparsification Recipes","items":[{"url":"#modifiers-introduction","title":"Modifiers Introduction"},{"url":"#training-epoch-modifiers","title":"Training Epoch Modifiers"},{"url":"#pruning-modifiers","title":"Pruning Modifiers","items":[{"url":"#constantpruningmodifier","title":"ConstantPruningModifier","items":[{"url":"#gmpruningmodifier","title":"GMPruningModifier"}]},{"url":"#quantization-modifiers","title":"Quantization Modifiers"},{"url":"#learning-rate-modifiers","title":"Learning Rate Modifiers","items":[{"url":"#setlearningratemodifier","title":"SetLearningRateModifier"},{"url":"#learningratemodifier","title":"LearningRateModifier"}]},{"url":"#paramsvariables-modifiers","title":"Params/Variables Modifiers","items":[{"url":"#trainableparamsmodifier","title":"TrainableParamsModifier"}]},{"url":"#optimizer-modifiers","title":"Optimizer Modifiers","items":[{"url":"#setweightdecaymodifier","title":"SetWeightDecayModifier"}]}]}]}]},"parent":{"relativePath":"user-guides/recipes/creating.mdx"},"frontmatter":{"metaTitle":"Creating Sparsification Recipes","metaDescription":"Creating Sparsification Recipes","index":1000,"skipToChild":null}},"allMdx":{"edges":[{"node":{"fields":{"id":"220abd27-24cf-5408-9402-3e7b0591a7ec","slug":"/details","title":"Details"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":true,"metaTitle":"Details","metaDescription":"Details"}}},{"node":{"fields":{"id":"bfcfecba-6eb1-59e8-8379-9c6d0c7a6a46","slug":"/get-started","title":"Get Started"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":true,"metaTitle":"Get Started","metaDescription":"Getting started with the Neural Magic Platform"}}},{"node":{"fields":{"id":"c1d1d524-e761-5294-a8e7-e21f3164f48a","slug":"/","title":"Home"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"Neural Magic Documentation","metaDescription":"Documentation for the Neural Magic Platform"}}},{"node":{"fields":{"id":"ee9f8c1f-d776-5134-89e6-b60f31e11b65","slug":"/products","title":"Products"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":true,"metaTitle":"Products","metaDescription":"Products"}}},{"node":{"fields":{"id":"0a2bc29c-0df2-59e5-a94c-bce091e6dc6d","slug":"/use-cases","title":"Use Cases"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":true,"metaTitle":"Use Cases","metaDescription":"Use Cases for the Neural Magic Platform"}}},{"node":{"fields":{"id":"cba43102-b32f-5b1a-9c69-f46222a36403","slug":"/user-guides/deepsparse-engine","title":"DeepSparse"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"User Guides for DeepSparse Engine","metaDescription":"User Guides for DeepSparse Engine"}}},{"node":{"fields":{"id":"a500248d-7a7d-5be6-9c51-ffab9753b5e0","slug":"/user-guides","title":"User Guides"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":true,"metaTitle":"User Guides","metaDescription":"User Guides"}}},{"node":{"fields":{"id":"ceb703c1-4adc-510d-95bd-fb316521b486","slug":"/user-guides/deploying-deepsparse","title":"Deploying DeepSparse"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying DeepSparse","metaDescription":"Deploying Deepsparse"}}},{"node":{"fields":{"id":"36031f98-c9cf-5658-b606-b28762ed208f","slug":"/user-guides/onnx-export","title":"ONNX Export"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Exporting to the ONNX Format","metaDescription":"Exporting to the ONNX Format"}}},{"node":{"fields":{"id":"e0110db2-2460-5fd1-8cf7-16533e4ce767","slug":"/user-guides/recipes","title":"Recipes"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"What are Sparsification Recipes?","metaDescription":"Description of sparsification recipes enabling smaller and more performant neural networks in deep learning"}}},{"node":{"fields":{"id":"d098532a-4bc3-5224-91ca-b51b42b779e7","slug":"/user-guides/sparsification","title":"Sparsification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"What is Sparsification?","metaDescription":"Description of model sparsification enabling smaller and more performant neural networks in deep learning"}}},{"node":{"fields":{"id":"2b874d9c-9c67-549d-8c79-b9b261359734","slug":"/user-guides/recipes/creating","title":"Creating"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Creating Sparsification Recipes","metaDescription":"Creating Sparsification Recipes"}}},{"node":{"fields":{"id":"b16b5557-b4f4-5f6a-8c4d-ee3673781d7f","slug":"/user-guides/recipes/enabling","title":"Enabling Pipelines"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Enabling Pipelines to Work with SparseML Recipes","metaDescription":"Enabling Pipelines to work with SparseML Recipess"}}},{"node":{"fields":{"id":"2b513d53-228c-5cf7-9d36-42d82e22cc0b","slug":"/user-guides/deploying-deepsparse/deepsparse-server","title":"DeepSparse Server"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying With DeepSparse Server","metaDescription":"Deploying With DeepSparse Server for faster and cheaper model deployments behind an HTTP API"}}},{"node":{"fields":{"id":"5c89e3b2-04e2-59ed-9b42-1c5df17a2df4","slug":"/user-guides/deploying-deepsparse/google-cloud-run","title":"Google Cloud Run"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Using DeepSparse on Google Cloud Run","metaDescription":"Deploy DeepSparse in a Serverless framework with Google Cloud Run"}}},{"node":{"fields":{"id":"6085e38d-6a1d-58ac-a6c2-052f8208cf6e","slug":"/user-guides/deepsparse-engine/diagnostics-debugging","title":"Diagnostics/Debugging"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Logging Guidance for Diagnostics and Debugging","metaDescription":"Logging Guidance for Diagnostics and Debugging"}}},{"node":{"fields":{"id":"b35e8125-4431-5427-bd08-6b379e810c9b","slug":"/user-guides/deploying-deepsparse/aws-lambda","title":"AWS Lambda"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Using DeepSparse on AWS Lambda","metaDescription":"Deploy DeepSparse in a Serverless framework with AWS Lambda"}}},{"node":{"fields":{"id":"49f60901-1870-5d76-9fcc-12408f802b71","slug":"/user-guides/deploying-deepsparse/amazon-sagemaker","title":"Amazon SageMaker"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploying With DeepSparse on Amazon SageMaker","metaDescription":"Deploying with DeepSparse on Amazon SageMaker for faster and cheaper model deployments behind an HTTP API"}}},{"node":{"fields":{"id":"115b6c49-fc7b-5dc6-9872-5a690e2b4a25","slug":"/user-guides/deepsparse-engine/benchmarking","title":"Benchmarking"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Benchmarking ONNX Models With DeepSparse","metaDescription":"Benchmarking ONNX models with DeepSparse"}}},{"node":{"fields":{"id":"86938249-6f57-5091-84c8-9ad393d417b1","slug":"/user-guides/deepsparse-engine/numactl-utility","title":"numactl Utility"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Using the numactl Utility to Control Resource Utilization With DeepSparse","metaDescription":"Using the numactl Utility to Control Resource Utilization With DeepSparse"}}},{"node":{"fields":{"id":"88604204-656d-5710-a23b-b7cc05eca1de","slug":"/user-guides/deepsparse-engine/hardware-support","title":"Supported Hardware"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Supported Hardware for DeepSparse","metaDescription":"Supported hardware for DeepSparse including CPU types and instruction sets"}}},{"node":{"fields":{"id":"8da43520-8b07-5bcc-a88f-f328137a2270","slug":"/user-guides/deepsparse-engine/logging","title":"DeepSparse Logging"},"frontmatter":{"index":6000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Logging","metaDescription":"System and Data Logging with DeepSparse"}}},{"node":{"fields":{"id":"ddada45f-c6b9-5146-91e4-f56c28e31284","slug":"/user-guides/deepsparse-engine/scheduler","title":"Inference Types"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Inference Types With DeepSparse Scheduler","metaDescription":"Inference Types with DeepSparse Scheduler"}}},{"node":{"fields":{"id":"baad04ad-efd7-5e85-8645-6455fce34324","slug":"/use-cases/image-classification","title":"Image Classification"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":true,"metaTitle":"Image Classification","metaDescription":"Image Classification with PyTorch Torchvision"}}},{"node":{"fields":{"id":"b7365467-d2b2-5dad-be10-a3aaa773d3a3","slug":"/use-cases/natural-language-processing","title":"Natural Language Processing"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":true,"metaTitle":"Natural Language Processing","metaDescription":"NLP with HuggingFace Transformers"}}},{"node":{"fields":{"id":"1ef7c7c8-073e-5abf-b57a-af03628c0714","slug":"/use-cases/object-detection","title":"Object Detection"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":true,"metaTitle":"Object Detection","metaDescription":"Object Detection with Ultralytics YOLOv5"}}},{"node":{"fields":{"id":"d1d7a8f3-9061-539a-aff8-d3957da959c6","slug":"/use-cases/object-detection/deploying","title":"Deploying"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Object Detection Deployments With DeepSparse","metaDescription":"Object Detection deployments with Ultralytics YOLOv5 and DeepSparse to create cheaper and more performant models"}}},{"node":{"fields":{"id":"113c5c42-984c-53b3-8392-7058bdd946cb","slug":"/use-cases/embedding-extraction","title":"Embedding Extraction"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Embedding Extraction Deployment","metaDescription":"Generalized Embedding Extraction Deployment"}}},{"node":{"fields":{"id":"3f751c96-1e87-5f4a-a0cb-d277724c2a0b","slug":"/use-cases/object-detection/sparsifying","title":"Sparsifying"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying Object Detection With Ultralytics YOLOv5 and SparseML","metaDescription":"Sparsifying Object Detection with Ultralytics YOLOv5 and SparseML to create cheaper and more performant models"}}},{"node":{"fields":{"id":"d363f68d-8b46-5da1-a1ef-fc14776d0a03","slug":"/use-cases/natural-language-processing/deploying","title":"Deploying"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Deployments with DeepSparse","metaDescription":"NLP deployments with Hugging Face Transformers and DeepSparse to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"bd363f12-8b17-5aac-af39-0ae0e7b0b829","slug":"/use-cases/natural-language-processing/question-answering","title":"Question Answering"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Question Answering","metaDescription":"Question Answering with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"ecaebb25-0fd8-572a-9d98-52302d0a0e4e","slug":"/use-cases/image-classification/sparsifying","title":"Sparsifying"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying Image Classification Models With SparseML","metaDescription":"Sparsifying Image Classification models with SparseML to create cheaper and more performant models"}}},{"node":{"fields":{"id":"0d031520-ee95-58db-bdb3-86689d6a0941","slug":"/use-cases/image-classification/deploying","title":"Deploying"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Image Classification Deployments With DeepSparse","metaDescription":"Image classification deployments with DeepSparse to create cheaper and more performant models"}}},{"node":{"fields":{"id":"c3c4bea9-f7cd-5044-9f34-8e33c8d2b36d","slug":"/use-cases/natural-language-processing/token-classification","title":"Token Classification"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Token Classification","metaDescription":"Token Classification with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"4ba00f44-34e5-5677-a8c6-862b44d48e7f","slug":"/products/deepsparse","title":"DeepSparse"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"3b06a681-3381-5cdc-9dad-827cdc6f60ac","slug":"/use-cases/deploying-deepsparse/docker","title":"Docker"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Using/Creating a DeepSparse Docker Image","metaDescription":"Using/Creating a DeepSparse Docker Image for repeatable build processes"}}},{"node":{"fields":{"id":"ae7d3726-718a-5be3-91a4-2559a2445fc4","slug":"/use-cases/natural-language-processing/text-classification","title":"Text Classification"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"NLP Text Classification","metaDescription":"Text Classification with Hugging Face Transformers and SparseML to create cheaper and more performant NLP models"}}},{"node":{"fields":{"id":"97cb8313-bc03-5a24-bf92-015086fc9071","slug":"/products/sparsezoo/models","title":"Models"},"frontmatter":{"index":1000,"targetURL":"https://sparsezoo.neuralmagic.com/","skipToChild":null,"metaTitle":"Models","metaDescription":"Models"}}},{"node":{"fields":{"id":"76657780-4f8a-5f37-8f3c-1c6c04637503","slug":"/products/sparseml","title":"SparseML"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML","metaDescription":"Sparsity-aware neural network inference engine for GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"be1fa4d5-bae3-5c19-9da5-f56a08f2fa40","slug":"/products/sparsezoo","title":"SparseZoo"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseZoo","metaDescription":"Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes"}}},{"node":{"fields":{"id":"aafbc5b1-5291-5291-ab86-da09c479a4c1","slug":"/products/sparsezoo/python-api","title":"Python API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Python API","metaDescription":"Documentation for the Python APIs available with SparseZoo enabling use of SOTA sparsified models and recipes"}}},{"node":{"fields":{"id":"b6a850d0-afc3-5418-ae82-146d2cb68706","slug":"/products/sparseml/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML CLI","metaDescription":"Documentation for the CLI commands installed with SparseML enabling SOTA model sparsification"}}},{"node":{"fields":{"id":"018cd1d6-6542-547d-ac3b-2743f179ac04","slug":"/products/sparseml/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Python API","metaDescription":"Documentation for the Python APIs available with SparseML enabling SOTA model sparsification"}}},{"node":{"fields":{"id":"08d07abb-4c37-5f58-8bdd-277facdeaf05","slug":"/products/sparsezoo/cli","title":"CLI"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML CLI","metaDescription":"Documentation for the CLI commands installed with SparseZoo enabling use of SOTA sparsified models and recipes"}}},{"node":{"fields":{"id":"d79d91ee-f596-5f3a-aafd-02c85a389909","slug":"/products/deepsparse/enterprise","title":"DeepSparse Enterprise"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Enterprise","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"233b31f0-deac-5b52-9101-caf3e3474595","slug":"/products/deepsparse/community","title":"DeepSparse Community"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Community","metaDescription":"An inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application"}}},{"node":{"fields":{"id":"e3c4460c-c09f-5838-8047-fea6cf8f0511","slug":"/products/deepsparse/enterprise/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse CLI","metaDescription":"Documentation for the CLI commands installed with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"f92e631d-dc67-592a-b7c2-dd4bb5e7a3fe","slug":"/products/deepsparse/community/cpp-api","title":"C++ API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse C++ API","metaDescription":"Documentation for the C++ APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"0d2bd4a5-a7f1-508a-8652-91e3eed40ceb","slug":"/products/deepsparse/community/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Python API","metaDescription":"Documentation for the Python APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"d6bf031b-89e2-5374-9997-1676f340f25a","slug":"/products/deepsparse/enterprise/cpp-api","title":"C++ API"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse C++ API","metaDescription":"Documentation for the C++ APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"478d6817-c021-5181-922d-2689a65470c5","slug":"/index/quick-tour","title":"Quick Tour"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Quick Tour","metaDescription":"Quick tour of the available functionality"}}},{"node":{"fields":{"id":"4491ba6b-e373-50e3-a8b5-e8e173ecba81","slug":"/index/deploy-workflow","title":"Deploy on CPUs"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy on CPUs","metaDescription":"Overview of deployment capabilities in the Neural Magic Platform"}}},{"node":{"fields":{"id":"27a0b7a5-09e0-56ab-9ee5-68f45d8795fd","slug":"/index/optimize-workflow","title":"Optimize for Inference"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Optimize for Inference","metaDescription":"Overview of deployment capabilities in the Neural Magic Platform"}}},{"node":{"fields":{"id":"cc2c9b73-3a5b-5764-ad54-38c919cb3e78","slug":"/get-started/install","title":"Installation"},"frontmatter":{"index":0,"targetURL":null,"skipToChild":null,"metaTitle":"Install Neural Magic Platform","metaDescription":"Installation instructions for the Neural Magic Platform including DeepSparse, SparseML, SparseZoo"}}},{"node":{"fields":{"id":"43a5dd03-4ffe-5c02-ac47-7ee94de63602","slug":"/get-started/sparsify-a-model","title":"Sparsify a Model"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsify a Model","metaDescription":"Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"fb19b2bb-688f-5f4e-98e0-d1dbf478a015","slug":"/get-started/transfer-a-sparsified-model","title":"Transfer a Sparsified Model"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model","metaDescription":"Transfer a Sparsified Model to your dataset, enabling performant deep learning deployments with limited training"}}},{"node":{"fields":{"id":"665dac23-0ae5-5037-a151-d359bad8f8c2","slug":"/get-started/deploy-a-model","title":"Deploy a Model"},"frontmatter":{"index":5000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy a Model","metaDescription":"Deploy a model with DeepSparse Server for easy and performant ML deployments"}}},{"node":{"fields":{"id":"00c56759-713b-5731-92b9-027dab853257","slug":"/get-started/use-a-model/custom-use-case","title":"Custom Use Case"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Custom Use Case","metaDescription":"Use a Custom Use Case and Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"5689a5f7-7564-5b91-b21d-0f6aed1218a9","slug":"/get-started/use-a-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Use an Object Detection Model","metaDescription":"Use an Object Detection Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"4ac6dd90-ac18-5c3c-8c85-06021527df5e","slug":"/get-started/use-a-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Text Classification Model","metaDescription":"Use a NLP Text Classification Model with DeepSparse for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"3bd4ca0d-2d04-5d9f-9037-06f9ae7dfb73","slug":"/get-started/transfer-a-sparsified-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model for Object Detection","metaDescription":"Transfer a Sparsified Object Detection Model to your dataset enabling performant deep learning deployments in a faster amount of time"}}},{"node":{"fields":{"id":"c32bcbda-4c9f-5b2a-b18d-5083b0003aef","slug":"/get-started/transfer-a-sparsified-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Transfer a Sparsified Model for Text Classification","metaDescription":"Transfer a Sparsified NLP Model to your sentiment analysis dataset enabling performant deep learning deployments with limited training"}}},{"node":{"fields":{"id":"8efaf662-8889-5bc3-bcae-9868aaa8aa53","slug":"/get-started/sparsify-a-model/supported-integrations","title":"Supported Integrations"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Sparsifying a Model for SparseML Integrations","metaDescription":"Sparsify a model with SparseML and recipes for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"6662f291-f43f-5616-8f60-dea883911f57","slug":"/get-started/sparsify-a-model/custom-integrations","title":"Custom Integrations"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Creating a Custom Integration for Sparsifying Models","metaDescription":"Creating a Custom Integration for Sparsifying Models with SparseML for smaller, faster, and cheaper model inferences in deployment"}}},{"node":{"fields":{"id":"8323781e-a7b4-5bb5-a453-86e2c472d6cf","slug":"/get-started/install/deepsparse-ent","title":"DeepSparse Enterprise"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Enterprise Installation","metaDescription":"Installation instructions for DeepSparse enabling performant neural network deployments"}}},{"node":{"fields":{"id":"dc9f9ab1-1fb3-57b4-910a-fa3200519b0d","slug":"/get-started/install/sparseml","title":"SparseML"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseML Installation","metaDescription":"Installation instructions for SparseML neural network optimization, training, and sparsification"}}},{"node":{"fields":{"id":"5ba330af-8819-52cd-8818-2374113da636","slug":"/get-started/use-a-model","title":"Use a Model"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Use a Model","metaDescription":"Use a Model with DeepSparse to deploy for faster and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"5cebfec6-69f9-59bb-9a47-58bd80ce5b29","slug":"/get-started/install/deepsparse","title":"DeepSparse Community"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Community Installation","metaDescription":"Installation instructions for DeepSparse enabling performant neural network deployments"}}},{"node":{"fields":{"id":"02f4ebab-32cf-58e1-a39b-db269f740d8b","slug":"/get-started/deploy-a-model/cv-object-detection","title":"CV Object Detection"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy an Object Detection Model","metaDescription":"Deploy an object detection model with DeepSparse Server for easier, faster, and cheaper inference on CPUs"}}},{"node":{"fields":{"id":"ac411c38-91fa-56bf-8dfd-c00cab6602ad","slug":"/get-started/install/sparsezoo","title":"SparseZoo"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"SparseZoo Installation","metaDescription":"Installation instructions for the SparseZoo sparse model repository"}}},{"node":{"fields":{"id":"a597113f-8c9b-5620-baaa-95555edb534c","slug":"/details/faqs","title":"FAQs"},"frontmatter":{"index":4000,"targetURL":null,"skipToChild":null,"metaTitle":"FAQs","metaDescription":"FAQs for the Neural Magic Platform"}}},{"node":{"fields":{"id":"e5f9763e-9c0a-5d84-9f65-550685441793","slug":"/details/research-papers","title":"Research Papers"},"frontmatter":{"index":1000,"targetURL":"https://neuralmagic.com/resources/technical-papers/","skipToChild":null,"metaTitle":"Research Papers","metaDescription":"Research Papers"}}},{"node":{"fields":{"id":"f2d385fc-cb79-5c11-8873-f4e6f6d6da23","slug":"/details/glossary","title":"Glossary"},"frontmatter":{"index":3000,"targetURL":null,"skipToChild":null,"metaTitle":"Glossary","metaDescription":"Glossary for the Neural Magic product"}}},{"node":{"fields":{"id":"5e0531ff-a237-587a-a318-72f000810bb0","slug":"/products/deepsparse/enterprise/python-api","title":"Python API"},"frontmatter":{"index":2000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse Python API","metaDescription":"Documentation for the Python APIs available with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"b386abe1-47d1-5b57-aa77-a3f73f3ebe21","slug":"/products/deepsparse/community/cli","title":"CLI"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"DeepSparse CLI","metaDescription":"Documentation for the CLI commands installed with DeepSparse enabling GPU-class performance on CPUs"}}},{"node":{"fields":{"id":"7d12ac36-8c45-565c-9b28-6472fdeb4a99","slug":"/get-started/deploy-a-model/nlp-text-classification","title":"NLP Text Classification"},"frontmatter":{"index":1000,"targetURL":null,"skipToChild":null,"metaTitle":"Deploy a Text Classification Model","metaDescription":"Deploy a text classification model with DeepSparse Server for easier, faster, and cheaper inference on CPUs"}}}]}},"pageContext":{"id":"2b874d9c-9c67-549d-8c79-b9b261359734","pageType":"docs"}},"staticQueryHashes":[]}