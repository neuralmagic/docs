"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[1146],{7809:(e,s,t)=>{t.r(s),t.d(s,{assets:()=>o,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var r=t(4848),n=t(8453);const a={tags:["DeepSparse","features"],keywords:["DeepSparse features"],description:"Inference Types With DeepSparse Scheduler",sidebar_label:"Inference Types",sidebar_position:2},i="Inference Types With DeepSparse Scheduler",l={id:"guides/deepsparse-engine/scheduler",title:"Inference Types With DeepSparse Scheduler",description:"Inference Types With DeepSparse Scheduler",source:"@site/docs/guides/deepsparse-engine/scheduler.mdx",sourceDirName:"guides/deepsparse-engine",slug:"/guides/deepsparse-engine/scheduler",permalink:"/next/guides/deepsparse-engine/scheduler",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/deepsparse-engine/scheduler.mdx",tags:[{label:"DeepSparse",permalink:"/next/tags/deep-sparse"},{label:"features",permalink:"/next/tags/features"}],version:"current",sidebarPosition:2,frontMatter:{tags:["DeepSparse","features"],keywords:["DeepSparse features"],description:"Inference Types With DeepSparse Scheduler",sidebar_label:"Inference Types",sidebar_position:2},sidebar:"autogenerated_docs",previous:{title:"Supported Hardware",permalink:"/next/guides/deepsparse-engine/hardware-support"},next:{title:"Benchmarking",permalink:"/next/guides/deepsparse-engine/benchmarking"}},o={},c=[{value:"Single Stream (Default)",id:"single-stream-default",level:2},{value:"Multi-Stream",id:"multi-stream",level:2},{value:"Enabling a Scheduler",id:"enabling-a-scheduler",level:2}];function d(e){const s={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.h1,{id:"inference-types-with-deepsparse-scheduler",children:"Inference Types With DeepSparse Scheduler"}),"\n",(0,r.jsx)(s.p,{children:"This page explains the various settings for DeepSparse, which enable you to tune the performance to your workload."}),"\n",(0,r.jsx)(s.p,{children:"Schedulers are special system software, which handle the distribution of work across cores in parallel computation.\nThe goal of a good scheduler is to ensure that, while work is available, cores are not sitting idle.\nOn the contrary, as long as parallel tasks are available, all cores should be kept busy."}),"\n",(0,r.jsx)(s.h2,{id:"single-stream-default",children:"Single Stream (Default)"}),"\n",(0,r.jsx)(s.p,{children:"In most use cases, the default scheduler is the preferred choice when running inferences with DeepSparse.\nThe default scheduler is highly optimized for minimum per-request latency, using all of the system's resources provided to it on every request it gets.\nOften, particularly when working with large batch sizes, the scheduler is able to distribute the workload of a single request across as many cores as it's provided."}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.em,{children:"Single-stream scheduling; requests execute serially by default:"})}),"\n",(0,r.jsx)("img",{src:"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/source/single-stream.png",alt:"single stream diagram"}),"\n",(0,r.jsx)(s.h2,{id:"multi-stream",children:"Multi-Stream"}),"\n",(0,r.jsx)(s.p,{children:"There are circumstances in which more cores does not imply better performance. If the computation can't be divided up to produce enough parallelism (while maximizing use of the CPU cache), then adding more cores simply adds more compute power with little to apply it to."}),"\n",(0,r.jsx)(s.p,{children:"An alternative, multi-stream scheduler is provided with the software. In cases where parallelism is low, sending multiple requests simultaneously can more adequately saturate the available cores. In other words, if speedup can't be achieved by adding more cores, then perhaps speedup can be achieved by adding more work."}),"\n",(0,r.jsxs)(s.p,{children:["If increasing core count does not decrease latency, that's a strong indicator that parallelism is low in your particular model/batch-size combination. It may be that total throughput can be increased by making more requests simultaneously. Using the ",(0,r.jsx)(s.a,{href:"https://docs.neuralmagic.com/archive/deepsparse/api/deepsparse.html#module-deepsparse.engine",children:"deepsparse.engine.Scheduler API,"})," the multi-stream scheduler can be selected, and requests made by multiple Python threads will be handled concurrently."]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.em,{children:"Multi-stream scheduling; requests execute in parallel and may better utilize hardware resources:"})}),"\n",(0,r.jsx)("img",{src:"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/docs/source/multi-stream.png",alt:"multi stream diagram"}),"\n",(0,r.jsxs)(s.p,{children:["Whereas the default scheduler will queue up requests made simultaneously and handle them serially, the multi-stream scheduler allows multiple requests to be run in parallel. The ",(0,r.jsx)(s.code,{children:"num_streams"})," argument to the Engine/Context classes controls how the multi-streams scheduler partitions up the machine. Each stream maps to a contiguous set of hardware threads. By default, only one hyperthread per core is used. There is no sharing amongst the partitions and it is generally good practice to make sure the ",(0,r.jsx)(s.code,{children:"num_streams"})," value evenly divides into your number of cores. By default ",(0,r.jsx)(s.code,{children:"num_streams"})," is set to multiplex requests across L3 caches."]}),"\n",(0,r.jsx)(s.p,{children:"Here's an example. Consider a machine with 2 sockets, each with 8 cores. In this case, the multi-stream scheduler will create two streams, one per socket by default. The first stream will contain cores 0-7 and the second stream will contain cores 8-15."}),"\n",(0,r.jsxs)(s.p,{children:["Manually increasing ",(0,r.jsx)(s.code,{children:"num_streams"})," to 3 will result in the following stream breakdown: threads 0-5 in the first stream, 6-10 in the second, and 11-15 in the last. This is problematic for our 2-socket system. The second stream (threads 6-10) is straddling both sockets, meaning that each request being serviced by that stream is going to incur a performance penalty each time one of its threads makes a remote memory access. The impact of this penalty will depend on the workload, but it will likely be significant."]}),"\n",(0,r.jsxs)(s.p,{children:["Manually increasing ",(0,r.jsx)(s.code,{children:"num_streams"})," to 4 is interesting. Here's the stream breakdown: threads 0-3 in the first stream, 4-7 in the second, 8-11 in the third, and 12-15 in the fourth. Each stream is only making memory accesses that are local to its socket, which is good. However, the first two and last two streams are sharing the same L3 cache, which can result in worse performance due to cache thrashing. Depending on the workload, though, the performance gain from the increased parallelism may negate this penalty."]}),"\n",(0,r.jsx)(s.p,{children:"The most common use cases for the multi-stream scheduler are where parallelism is low with respect to core count, and where requests need to be made asynchronously without time to batch them. Implementing a model server may fit such a scenario and be ideal for using multi-stream scheduling."}),"\n",(0,r.jsx)(s.h2,{id:"enabling-a-scheduler",children:"Enabling a Scheduler"}),"\n",(0,r.jsx)(s.p,{children:"Depending on your engine execution strategy, enable one of these options by running:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'engine = compile_model(model_path, scheduler="single_stream")\n'})}),"\n",(0,r.jsx)(s.p,{children:"or:"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'engine = compile_model(model_path, scheduler="multi_stream", num_streams=None) # None is the default\n'})}),"\n",(0,r.jsxs)(s.p,{children:["or pass in the enum value directly, since",(0,r.jsx)(s.code,{children:' "multi_stream" == Scheduler.multi_stream'}),"."]}),"\n",(0,r.jsx)(s.p,{children:"By default, the scheduler will map to a single stream."})]})}function h(e={}){const{wrapper:s}={...(0,n.R)(),...e.components};return s?(0,r.jsx)(s,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,s,t)=>{t.d(s,{R:()=>i,x:()=>l});var r=t(6540);const n={},a=r.createContext(n);function i(e){const s=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function l(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),r.createElement(a.Provider,{value:s},e.children)}}}]);