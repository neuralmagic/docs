"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[3038],{2902:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>m,frontMatter:()=>t,metadata:()=>l,toc:()=>d});var a=s(4848),i=s(8453);const t={tags:["sparse fine-tuning","Llama2 7B","llm","text generation","sparseml","optimization","neural magic","GSM8K dataset"],keywords:["sparse fine-tuning","Llama2 7B","SparseML","GSM8K dataset"],description:"Guide on sparse fine-tuning Llama2 7B model on GSM8K dataset, including steps, commands, and recipes for optimization.",sidebar_label:"Sparse Fine-Tuning LLMs on GSM8k",sidebar_position:4},r="Sparse Fine-Tuning Llama2 7B on GSM8k",l={id:"llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml",title:"Sparse Fine-Tuning Llama2 7B on GSM8k",description:"Guide on sparse fine-tuning Llama2 7B model on GSM8K dataset, including steps, commands, and recipes for optimization.",source:"@site/versioned_docs/version-1.7.0/llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml.md",sourceDirName:"llms/guides",slug:"/llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml",permalink:"/llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml.md",tags:[{label:"sparse fine-tuning",permalink:"/tags/sparse-fine-tuning"},{label:"Llama2 7B",permalink:"/tags/llama-2-7-b"},{label:"llm",permalink:"/tags/llm"},{label:"text generation",permalink:"/tags/text-generation"},{label:"sparseml",permalink:"/tags/sparseml"},{label:"optimization",permalink:"/tags/optimization"},{label:"neural magic",permalink:"/tags/neural-magic"},{label:"GSM8K dataset",permalink:"/tags/gsm-8-k-dataset"}],version:"1.7.0",sidebarPosition:4,frontMatter:{tags:["sparse fine-tuning","Llama2 7B","llm","text generation","sparseml","optimization","neural magic","GSM8K dataset"],keywords:["sparse fine-tuning","Llama2 7B","SparseML","GSM8K dataset"],description:"Guide on sparse fine-tuning Llama2 7B model on GSM8K dataset, including steps, commands, and recipes for optimization.",sidebar_label:"Sparse Fine-Tuning LLMs on GSM8k",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Compress LLMs With SparseGPT",permalink:"/llms/guides/one-shot-llms-with-sparseml"},next:{title:"LLM Serving on Windows",permalink:"/llms/guides/llm-serving-on-windows"}},o={},d=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Base Model",id:"base-model",level:2},{value:"Dense fine-tuning",id:"dense-fine-tuning",level:2},{value:"Dense fine-tuned model accuracy",id:"dense-fine-tuned-model-accuracy",level:3},{value:"Oneshot Sparsification",id:"oneshot-sparsification",level:2},{value:"Oneshot 50% sparse model accuracy",id:"oneshot-50-sparse-model-accuracy",level:3},{value:"Sparse fine-tuning",id:"sparse-fine-tuning",level:2},{value:"Fine-tuned 50% sparse model accuracy",id:"fine-tuned-50-sparse-model-accuracy",level:3},{value:"Evaluation Setup",id:"evaluation-setup",level:2}];function c(e){const n={a:"a",b:"b",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"sparse-fine-tuning-llama2-7b-on-gsm8k",children:"Sparse Fine-Tuning Llama2 7B on GSM8k"}),"\n",(0,a.jsx)(n.p,{children:"This guide details the steps for going from a pre-trained, unoptimized Llama2 7B model to a 50% sparse Llama2 7B model that has been fine-tuned on the GSM8K dataset and recovers fully and goes beyond the dense baseline accuracy."}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#dense-fine-tuning",children:"Dense fine-tuning"}),": Finetune the pre-trained, unoptimized Llama2 7B model on the ",(0,a.jsx)(n.a,{href:"https://huggingface.co/datasets/gsm8k",children:"GSM8K dataset"})," from HuggingFace."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#oneshot-sparsification",children:"Oneshot Sparsification"}),": Oneshot sparsify the dense fine-tuned model to 50% sparsity."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"#sparse-fine-tuning",children:"Sparse finetuning"}),": Further fine-tune the oneshot 50% sparse model on the GSM8K dataset to recover some of the accuracy that is lost during the oneshot sparsification step."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.b,{children:"Training Environment:"})," A system that meets the minimum hardware and software requirements as outlined in the ",(0,a.jsx)(n.a,{href:"/get-started/install/sparseml#prerequisites",children:"Install Guide"}),". To replicate the setup used for fine-tuning in this guide, use 4 NVIDIA A100 GPUs for both dense and sparse fine-tuning steps."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.b,{children:"SparseML LLM Installation:"})," An environment with SparseML for LLMs installed as outlined in the ",(0,a.jsx)(n.a,{href:"/get-started/install/sparseml#generative-ai-hugging-face",children:"Install Guide"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.b,{children:"Background:"})," Familiarity with Generative AI and working with large language models is recommended."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"base-model",children:"Base Model"}),"\n",(0,a.jsx)(n.p,{children:"To obtain an optimized sparse model trained on the GSM8K dataset, we first start with the pre-trained, unoptimized Llama2 7B model. You can obtain this model using the following SparseZoo stub:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"zoo:llama2-7b-llama2_pretrain-base\n"})}),"\n",(0,a.jsx)(n.h2,{id:"dense-fine-tuning",children:"Dense fine-tuning"}),"\n",(0,a.jsx)(n.p,{children:"We then fine-tune the above pre-trained dense model on the GSM8K dataset to obtain a model that we can later optimize using sparsification."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'accelerate launch \\\n    --config_file example_fsdp_config.yaml \\\n    --no_python sparseml.transformers.text_generation.finetune \\\n    --model PATH_TO_MODEL or ZOO_STUB \\\n    --dataset "gsm8k" \\\n    --dataset_config_name "main" \\\n    --output_dir PATH_TO_OUTPUT \\\n    --splits "train" \\\n    --num_train_epochs 2 \\\n    --precision "bfloat16" \\\n    --gradient_checkpointing True \\\n    --bf16 True \\\n    --learning_rate 0.00005 \\\n    --lr_scheduler_type "linear" \\\n    --max_seq_length 1024 \\\n    --per_device_train_batch_size 32 \\\n    --max_grad_norm 2 \\\n    --warmup_steps 20\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Note: ",(0,a.jsxs)(n.em,{children:["Some of these hyper-parameters may need further tuning to enhance the overall accuracy of the fine-tuned model. The values mentioned above were obtained through a quick hyper-parameter search. Parameters that could have a significant impact and are worth considering for tuning include: ",(0,a.jsx)(n.code,{children:"learning_rate"}),", ",(0,a.jsx)(n.code,{children:"max_grad_norm"}),", ",(0,a.jsx)(n.code,{children:"warmup_steps"}),", ",(0,a.jsx)(n.code,{children:"max_seq_length"}),"."]})]}),"\n",(0,a.jsxs)(n.p,{children:["The example_fsdp_config.yaml used above contains the following setup for FSDP. Set the ",(0,a.jsx)(n.code,{children:"num_processes"})," to the number of GPUs available. For our setup, we used 4 NVIDIA A100 GPUs so we set ",(0,a.jsx)(n.code,{children:"num_processes"})," to ",(0,a.jsx)(n.code,{children:"4"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"compute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: false\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: false\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_use_orig_params: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n"})}),"\n",(0,a.jsx)(n.h3,{id:"dense-fine-tuned-model-accuracy",children:"Dense fine-tuned model accuracy"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"#evaluation-setup",children:"Evaluating"})," the dense fine-tuned model on the ",(0,a.jsx)(n.code,{children:"gsm8k 0-shot"})," task, results in a baseline accuracy of ",(0,a.jsx)(n.code,{children:"37.52%"}),". We'll consider this accuracy as our baseline for calculating recovery for the oneshot sparse and sparse fine-tuned models we'll get later. Detailed results are provided below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "results": {\n    "gsm8k": {\n      "acc": 0.3752843062926459,\n      "acc_stderr": 0.013337170545742932\n    }\n  },\n  "versions": {\n    "gsm8k": 0\n  },\n  "config": {\n    "model": "sparseml",\n    "model_args": "pretrained=/cache/shubhra/gsm8k_tutorial/scripts/models/llama7b_dense_gsm8k_linear_e2_gc2_lr5e-5_gpus4/,trust_remote_code=True",\n    "num_fewshot": 0,\n    "batch_size": "48",\n    "batch_sizes": [],\n    "device": "cuda:0",\n    "no_cache": true,\n    "limit": null,\n    "bootstrap_iters": 100000,\n    "description_dict": {}\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"oneshot-sparsification",children:"Oneshot Sparsification"}),"\n",(0,a.jsx)(n.p,{children:"Use the dense fine-tuned model obtained above and sparsify it to 50% in a oneshot manner using the command and recipe specified below."}),"\n",(0,a.jsx)(n.p,{children:"Command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'accelerate launch \\\n    --config_file example_fsdp_config.yaml \\\n    --no_python sparseml.transformers.text_generation.oneshot \\\n    --model PATH_TO_MODEL \\\n    --dataset "gsm8k" \\\n    --dataset_config_name "main" \\\n    --concatenate_data OPTIONAL \\\n    --recipe PATH_TO_RECIPE \\\n    --output_dir PATH_TO_OUTPUT \\\n    --splits "train" \\\n    --pad_to_max_length False \\\n    --oneshot_device DEVICE \\\n    --num_calibration_samples 1024 \\\n    --max_seq_len 4096\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Note: ",(0,a.jsxs)(n.em,{children:["You may wish to tweak the ",(0,a.jsx)(n.code,{children:"num_calibration_samples"})," above to obtain better accuracy."]})]}),"\n",(0,a.jsx)(n.p,{children:"Recipe:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'pruning_stage:\n  obcq_modifiers:\n    SparseGPTModifier:\n      sparsity: 0.5\n      block_size: 128\n      sequential_update: False\n      quantize: False\n      targets: [\n        "re:model.layers.\\\\d+$"\n      ]\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Note: ",(0,a.jsxs)(n.em,{children:["The recipe above uses SparseGPT to oneshot sparsify the model to a uniform sparsity of 50% as specified by the ",(0,a.jsx)(n.code,{children:"sparsity"})," param."]})]}),"\n",(0,a.jsx)(n.p,{children:"Alternatively, you could use non-uniform sparsity distribution centered around 50% to oneshot sparsify your model by using the modified recipe below."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'pruning_stage:\n  obcq_modifiers:\n    SparseGPTModifier:\n      sparsity: 0.5\n      sparsity_profile: "OWL"\n      owl_m: 5\n      owl_lmbda: 0.08\n      block_size: 128\n      sequential_update: False\n      quantize: False\n      targets: [\n        "re:model.layers.\\\\d+$"\n      ]\n'})}),"\n",(0,a.jsxs)(n.p,{children:["To learn more about the OWL non-uniform sparsity profile method, visit ",(0,a.jsx)(n.a,{href:"https://github.com/luuyin/OWL/tree/main?tab=readme-ov-file#script-example-of-pruning-llama-7b-using-owl-sparsegpt",children:"this link"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"oneshot-50-sparse-model-accuracy",children:"Oneshot 50% sparse model accuracy"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"#evaluation-setup",children:"Evaluating"})," the oneshot 50% sparse model on the ",(0,a.jsx)(n.code,{children:"gsm8k 0-shot"})," task, results in an accuracy of ",(0,a.jsx)(n.code,{children:"33.81%"})," and translates to a ",(0,a.jsx)(n.code,{children:"90.11%"})," recovery over our [dense baseline](#Dense fine-tuned model accuracy). In the next step we'll see how to improve the recovery of this model using sparse fine-tuning. Detailed results for the oneshot 50% sparse model are provided below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "results": {\n    "gsm8k": {\n      "acc": 0.33813495072024263,\n      "acc_stderr": 0.0130308291451722\n    }\n  },\n  "versions": {\n    "gsm8k": 0\n  },\n  "config": {\n    "model": "sparseml",\n    "model_args": "pretrained=/cache/shubhra/gsm8k_tutorial/scripts/models/llama7b_oneshot_sparse_oneshot_linear_e2_gc2_lr5e-5,trust_remote_code=True",\n    "num_fewshot": 0,\n    "batch_size": "48",\n    "batch_sizes": [],\n    "device": "cuda:0",\n    "no_cache": true,\n    "limit": null,\n    "bootstrap_iters": 100000,\n    "description_dict": {}\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sparse-fine-tuning",children:"Sparse fine-tuning"}),"\n",(0,a.jsx)(n.p,{children:"The one-shot sparse model generated previously can undergo further sparse fine-tuning to enhance its overall accuracy. This process involves distilling information from the previously obtained dense fine-tuned model, which serves as the teacher model, to the one-shot sparse model, acting as the student. This can be achieved using the following command and recipe."}),"\n",(0,a.jsx)(n.p,{children:"Command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'accelerate launch \\\n    --config_file example_fsdp_config.yaml \\\n    --no_python sparseml.transformers.text_generation.finetune \\\n    --model PATH_TO_MODEL \\\n    --dataset "gsm8k" \\\n    --dataset_config_name "main" \\\n    --output_dir PATH_TO_OUTPUT \\\n    --splits "train" \\\n    --num_train_epochs 2 \\\n    --precision "bfloat16" \\\n    --gradient_checkpointing True \\\n    --bf16 True \\\n    --learning_rate 0.00005 \\\n    --lr_scheduler_type "linear" \\\n    --max_seq_length 1024 \\\n    --per_device_train_batch_size 32 \\\n    --max_grad_norm None \\\n    --warmup_steps 20 \\\n    --distill_teacher PATH_TO_TEACHER \\\n    --recipe PATH_TO_RECIPE \n'})}),"\n",(0,a.jsx)(n.p,{children:"Recipe:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'test_stage:\n  pruning_modifiers:\n    ConstantPruningModifier:\n      targets: [\n        "re:.*self_attn.q_proj",\n        "re:.*self_attn.k_proj",\n        "re:.*self_attn.v_proj",\n        "re:.*self_attn.o_proj",\n        "re:.*mlp.gate_proj",\n        "re:.*mlp.up_proj"\n      ]\n      start: 0\n  distillation_modifiers:\n    OutputDistillationModifier:\n      targets: [\n        "model.embed_tokens",\n        "model.layers.0",\n        "model.layers.1",\n        "model.layers.2",\n        "model.layers.3",\n        "model.layers.4",\n        "model.layers.5",\n        "model.layers.6",\n        "model.layers.7",\n        "model.layers.8",\n        "model.layers.9",\n        "model.layers.10",\n        "model.layers.11",\n        "model.layers.12",\n        "model.layers.13",\n        "model.layers.14",\n        "model.layers.15",\n        "model.layers.16",\n        "model.layers.17",\n        "model.layers.18",\n        "model.layers.19",\n        "model.layers.20",\n        "model.layers.21",\n        "model.layers.22",\n        "model.layers.23",\n        "model.layers.24",\n        "model.layers.25",\n        "model.layers.26",\n        "model.layers.27",\n        "model.layers.28",\n        "model.layers.29",\n        "model.layers.30",\n        "model"\n      ]\n      comparison: "square_head"\n      start: 0\n      orig_scale: 1.0\n      distill_scale: 1.0\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Note: ",(0,a.jsxs)(n.em,{children:["Some of these hyper-parameters may need further tuning to enhance the overall accuracy of the fine-tuned model. The values mentioned above were obtained through a quick hyper-parameter search. Parameters that could have a significant impact and are worth considering for tuning include: ",(0,a.jsx)(n.code,{children:"learning_rate"}),", ",(0,a.jsx)(n.code,{children:"max_grad_norm"}),", ",(0,a.jsx)(n.code,{children:"warmup_steps"}),", ",(0,a.jsx)(n.code,{children:"max_seq_length"}),"."]})]}),"\n",(0,a.jsx)(n.h3,{id:"fine-tuned-50-sparse-model-accuracy",children:"Fine-tuned 50% sparse model accuracy"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"#evaluation-setup",children:"Evaluating"})," the fine-tuned 50% sparse model on the ",(0,a.jsx)(n.code,{children:"gsm8k 0-shot"})," task, results in an accuracy of ",(0,a.jsx)(n.code,{children:"38.59%"})," and shows clear improvement over the [oneshot accuracy](#Oneshot 50% sparse model accuracy). The sparse fine-tuning step not only helped improve over the oneshot accuracy but even surpassed the dense baseline model. Detailed results for the oneshot 50% sparse model are provided below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "results": {\n    "gsm8k": {\n      "acc": 0.3858984078847612,\n      "acc_stderr": 0.01340907747131918\n    }\n  },\n  "versions": {\n    "gsm8k": 0\n  },\n  "config": {\n    "model": "sparseml",\n    "model_args": "pretrained=/cache/shubhra/gsm8k_tutorial/scripts/models/llama7b_sparse_gsm8k_linear_e2_gc0_lr5e-5,trust_remote_code=True",\n    "num_fewshot": 0,\n    "batch_size": "48",\n    "batch_sizes": [],\n    "device": "cuda:0",\n    "no_cache": true,\n    "limit": null,\n    "bootstrap_iters": 100000,\n    "description_dict": {}\n  }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"evaluation-setup",children:"Evaluation Setup"}),"\n",(0,a.jsxs)(n.p,{children:["To evaluate model performance we use ",(0,a.jsx)(n.a,{href:"https://github.com/neuralmagic/lm-evaluation-harness",children:"lm-evaluation-harness framework"}),".\nClone the forked repository with SparseML support and install it:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/neuralmagic/lm-evaluation-harness.git\ncd lm-evaluation-harness\npip install -e .\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Evaluate on the ",(0,a.jsx)(n.code,{children:"gsm8k 0-shot"})," task:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'MODEL_PATH=<MODEL_PATH>\nTASK=gsm8k\npython main.py \\\n --model sparseml \\\n --model_args pretrained=MODEL_PATH,trust_remote_code=True \\\n --tasks $TASK \\\n --batch_size 48 \\\n --no_cache \\\n --write_out \\\n --output_path "${MODEL_PATH}/${TASK}.json" \\\n --device "cuda:0" \\\n --num_fewshot 0\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var a=s(6540);const i={},t=a.createContext(i);function r(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);