"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[8904],{7222:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>c,frontMatter:()=>i,metadata:()=>a,toc:()=>l});var r=o(4848),t=o(8453);const i={tags:["onnx","model export","cpu inference"],keywords:["onnx model format","neural network interchange","cross-platform compatibility","deepsparse"],description:"Overview of ONNX, its role in DeepSparse for CPU inference, and guidance on exporting models to the ONNX format.",sidebar_label:"ONNX",sidebar_position:4},s="ONNX: Model Definitions for DeepSparse",a={id:"guides/onnx/index",title:"ONNX: Model Definitions for DeepSparse",description:"Overview of ONNX, its role in DeepSparse for CPU inference, and guidance on exporting models to the ONNX format.",source:"@site/versioned_docs/version-1.7.0/guides/onnx/index.mdx",sourceDirName:"guides/onnx",slug:"/guides/onnx/",permalink:"/guides/onnx/",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/onnx/index.mdx",tags:[{label:"onnx",permalink:"/tags/onnx"},{label:"model export",permalink:"/tags/model-export"},{label:"cpu inference",permalink:"/tags/cpu-inference"}],version:"1.7.0",sidebarPosition:4,frontMatter:{tags:["onnx","model export","cpu inference"],keywords:["onnx model format","neural network interchange","cross-platform compatibility","deepsparse"],description:"Overview of ONNX, its role in DeepSparse for CPU inference, and guidance on exporting models to the ONNX format.",sidebar_label:"ONNX",sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Google Cloud Run",permalink:"/guides/deploying-deepsparse/google-cloud-run"},next:{title:"Products",permalink:"/products/"}},d={},l=[{value:"Why ONNX Matters for DeepSparse",id:"why-onnx-matters-for-deepsparse",level:2},{value:"Exporting Models to ONNX",id:"exporting-models-to-onnx",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"onnx-model-definitions-for-deepsparse",children:"ONNX: Model Definitions for DeepSparse"}),"\n",(0,r.jsx)(n.p,{children:"ONNX (Open Neural Network Exchange) is an open-source format representing machine learning models, including deep neural networks.\nIt provides a standardized way to exchange models between different frameworks and tools, promoting cross-platform compatibility."}),"\n",(0,r.jsx)(n.h2,{id:"why-onnx-matters-for-deepsparse",children:"Why ONNX Matters for DeepSparse"}),"\n",(0,r.jsx)(n.p,{children:"DeepSparse leverages ONNX for optimized CPU inference pathways. Here's why it's important:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)("b",{children:"Framework Flexibility:"})," Exporting models to ONNX allows you to deploy sparsified and optimized neural networks created in various training frameworks (e.g., PyTorch, TensorFlow) to DeepSparse's CPU inference engine."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)("b",{children:"Hardware Portability:"})," ONNX, combined with DeepSparse, can run on various CPU architectures (x86, ARM, etc.), ensuring your optimized models work across diverse hardware environments."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)("b",{children:"Performance Optimization:"})," DeepSparse's ONNX runtime is specifically tuned to deliver efficient inference performance on CPUs, taking advantage of platform-specific optimizations."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"exporting-models-to-onnx",children:"Exporting Models to ONNX"}),"\n",(0,r.jsx)(n.p,{children:"Converting your trained model to the ONNX format depends on the original training framework.\nSparseML is the default pathway for exporting models to ONNX within the Neural Magic ecosystem.\nIt supports exporting standard PyTorch models to ONNX, including unoptimized and sparsified models."}),"\n",(0,r.jsx)(n.p,{children:"Here's a basic example of exporting a PyTorch GPT2 from HuggingFace to ONNX using SparseML:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from sparseml import export\n\n# Load your PyTorch model\nfrom sparseml.transformers import SparseAutoModelForCausalLM, SparseAutoTokenizer\nmodel = SparseAutoModelForCausalLM.from_pretrained("gpt2")\ntokenizer = SparseAutoTokenizer.from_pretrained("gpt2")\n\n# Export the model to ONNX\nexport(\n    model=model,\n    tokenizer=tokenizer,\n    target_path="./onnx-export",\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:"For unsupported frameworks or the above doesn't work for your custom models, you can export models to ONNX using the native APIs of the training framework or supported third-party pathways:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://pytorch.org/docs/stable/onnx.html",children:"PyTorch"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/onnx/tensorflow-onnx",children:"TensorFlow"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/onnx/keras-onnx",children:"Keras"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/google/jaxonnxruntime",children:"JAX"})}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var r=o(6540);const t={},i=r.createContext(t);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);