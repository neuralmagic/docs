"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[1232],{1598:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var a=n(4848),t=n(8453);const r={tags:["sparsification","model optimization","model compression k"],keywords:["model compression","model acceleration","neural network optimization","efficiency","pruning","quantization","distillation"],description:"A comprehensive overview of sparsification techniques used to create smaller, faster, and more energy-efficient neural networks while maintaining accuracy.",sidebar_label:"Sparsification",sidebar_position:1},s="Sparsification: Compressing Neural Networks",o={id:"guides/sparsification/index",title:"Sparsification: Compressing Neural Networks",description:"A comprehensive overview of sparsification techniques used to create smaller, faster, and more energy-efficient neural networks while maintaining accuracy.",source:"@site/versioned_docs/version-1.7.0/guides/sparsification/index.mdx",sourceDirName:"guides/sparsification",slug:"/guides/sparsification/",permalink:"/guides/sparsification/",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/sparsification/index.mdx",tags:[{label:"sparsification",permalink:"/tags/sparsification"},{label:"model optimization",permalink:"/tags/model-optimization"},{label:"model compression k",permalink:"/tags/model-compression-k"}],version:"1.7.0",sidebarPosition:1,frontMatter:{tags:["sparsification","model optimization","model compression k"],keywords:["model compression","model acceleration","neural network optimization","efficiency","pruning","quantization","distillation"],description:"A comprehensive overview of sparsification techniques used to create smaller, faster, and more energy-efficient neural networks while maintaining accuracy.",sidebar_label:"Sparsification",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Guides",permalink:"/guides/"},next:{title:"DeepSparse Features",permalink:"/guides/deepsparse-engine/"}},c={},l=[{value:"Techniques",id:"techniques",level:2},{value:"Quantization",id:"quantization",level:3},{value:"Pruning",id:"pruning",level:3},{value:"Knowledge Distillation",id:"knowledge-distillation",level:3},{value:"Low Rank Approximation",id:"low-rank-approximation",level:3},{value:"Conditional Computation",id:"conditional-computation",level:3},{value:"Regularization",id:"regularization",level:3},{value:"Weight Sharing",id:"weight-sharing",level:3},{value:"Architecture Search",id:"architecture-search",level:3},{value:"Compound Sparsification",id:"compound-sparsification",level:3},{value:"Application",id:"application",level:2},{value:"Post-Training / One-Shot",id:"post-training--one-shot",level:3},{value:"Training Aware",id:"training-aware",level:3},{value:"Transfer Learning",id:"transfer-learning",level:3},{value:"Recipes",id:"recipes",level:2}];function d(e){const i={h1:"h1",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h1,{id:"sparsification-compressing-neural-networks",children:"Sparsification: Compressing Neural Networks"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification encompasses a range of powerful techniques used to compress and optimize neural networks.\nBy strategically removing or reducing the significance of less important connections and information within a model, sparsification leads to retaining accuracy while resulting in:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Smaller Model Sizes:"})," Reduced storage requirements and memory footprint, simplifying deployment."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Faster Inference:"})," Significant boosts in computational speed, especially on resource-constrained hardware, promoting real-time applications."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Reduced Energy Consumption:"})," Enable efficient execution for servers, edge environments, and mobile devices, lowering costs and broadening usage."]}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:"This guide delves into the core concepts of sparsification, and in it, you'll learn:"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"The Purpose of Sparsification:"})," Discover the benefits and motivations behind optimizing neural networks."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Essential Techniques:"})," Explore the key methods used to achieve sparsification."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Application Strategies:"})," Understand how to implement sparsification at different stages of the model's lifecycle."]}),"\n",(0,a.jsxs)(i.li,{children:[(0,a.jsx)("b",{children:"Practical Recipes:"})," Get guidance on applying sparsification techniques to everyday use cases."]}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"techniques",children:"Techniques"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification techniques can be broadly categorized into several key methods, each with its unique approach to compressing and optimizing neural networks:"}),"\n",(0,a.jsx)(i.h3,{id:"quantization",children:"Quantization"}),"\n",(0,a.jsx)(i.p,{children:"Quantization reduces the precision of weights and activations in a neural network, for example, from 32-bit floating-point numbers to 8-bit integers.\nQuantization can be applied to weights, activations, or both and can be done statically (before deployment) or dynamically (at runtime).\nIt decreases model size and memory usage, often leading to faster inference, particularly with specialized hardware support for low-precision arithmetic."}),"\n",(0,a.jsx)(i.h3,{id:"pruning",children:"Pruning"}),"\n",(0,a.jsx)(i.p,{children:"Pruning eliminates redundant or less important connections within a model.\nPruning can be done in either a structured or unstructured manner, where structured pruning changes the model's shape, and unstructured pruning keeps the shape intact while introducing zeros in the weights (sparsity).\nThis results in a smaller model and faster inference due to reduced compute provided the engine/hardware supports sparse computation."}),"\n",(0,a.jsx)(i.h3,{id:"knowledge-distillation",children:"Knowledge Distillation"}),"\n",(0,a.jsx)(i.p,{children:'Distillation generally trains a smaller or more compressed "student" model to mimic the behavior of a larger, unoptimized "teacher" model.\nIt enables the creation of more compressed models that are easier to deploy and execute while leveraging the knowledge and performance of the larger model to maintain accuracy.\nDistillation is further broken down into granularity levels, such as model-level, layer-level, and instance-level distillation.'}),"\n",(0,a.jsx)(i.h3,{id:"low-rank-approximation",children:"Low Rank Approximation"}),"\n",(0,a.jsx)(i.p,{children:"Low-rank approximations (LoRA), also known as matrix factorization, matrix decomposition, or tensor decomposition, reduce the rank of the weight matrices in a neural network, effectively compressing the model.\nThis technique is based on the observation that the weight matrices of neural networks are often low-rank, meaning they can be approximated by a product of two smaller matrices.\nIt can be particularly effective for compressing a model's large, fully connected layers.\nIt's also known to be used in conjunction with other compression techniques, such as quantization (QLoRA), to enable faster fine-tuning."}),"\n",(0,a.jsx)(i.h3,{id:"conditional-computation",children:"Conditional Computation"}),"\n",(0,a.jsx)(i.p,{children:"Conditional computation selectively activates only parts of a model based on the input data, leading to dynamic sparsity.\nThis can be achieved through techniques such as gating, where a gating network decides which parts of the model to execute, or through adaptive computation, where the model learns to skip or reduce computation based on the input, such as Mixture of Experts (MoE) techniques.\nConditional computation can significantly speed up inference time, especially for models with large, redundant, or unnecessary computations."}),"\n",(0,a.jsx)(i.h3,{id:"regularization",children:"Regularization"}),"\n",(0,a.jsx)(i.p,{children:"Regularization methods such as L1 and L2 can be used to encourage sparsity in a neural network's weights.\nAdding a regularization term to the loss function incentivizes the model to reduce overfitting and learn simpler representations, which can lead to sparser models.\nRegularization can be used with other techniques, such as pruning, to further enhance the sparsity of a model."}),"\n",(0,a.jsx)(i.h3,{id:"weight-sharing",children:"Weight Sharing"}),"\n",(0,a.jsx)(i.p,{children:"Weight sharing involves sharing the weights of a neural network across different parts of the model, effectively reducing the number of unique weights and thereby reducing the model size.\nThis can be done by clustering similar weights and sharing the same weight value across multiple connections.\nWeight sharing can be particularly effective for reducing a model's memory footprint, especially when combined with other compression techniques."}),"\n",(0,a.jsx)(i.h3,{id:"architecture-search",children:"Architecture Search"}),"\n",(0,a.jsx)(i.p,{children:"Techniques such as neural architecture search (NAS) can automatically discover more efficient and compact neural network architectures.\nBy searching over a large space of possible architectures, NAS can identify smaller, faster, and more accurate models than hand-designed architectures.\nNAS can be used to optimize existing models or discover entirely new architectures tailored to specific tasks or constraints."}),"\n",(0,a.jsx)(i.h3,{id:"compound-sparsification",children:"Compound Sparsification"}),"\n",(0,a.jsx)(i.p,{children:"Compound sparsification combines multiple techniques to achieve even more significant compression and optimization.\nBy leveraging the strengths of different methods, compound sparsification can create smaller, faster, and more energy-efficient models than those produced by individual techniques.\nFor example, pruning can be combined with quantization and distillation to create highly compressed models that retain high accuracy."}),"\n",(0,a.jsx)(i.h2,{id:"application",children:"Application"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification techniques can be applied at different stages of a model's lifecycle with varying degrees of complexity and effectiveness:"}),"\n",(0,a.jsx)(i.h3,{id:"post-training--one-shot",children:"Post-Training / One-Shot"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification can be applied post-training, where a pre-trained model is compressed using pruning, quantization, or distillation techniques.\nPost-training is often the most straightforward approach to sparsification, as it does not require changes to the training process or hyperparameters.\nHowever, post-training sparsification may have the same level of compression or performance as techniques applied during training.\nIt is particularly practical for quantization but less effective for pruning."}),"\n",(0,a.jsx)(i.h3,{id:"training-aware",children:"Training Aware"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification can also be applied during training, where the model is trained with sparsification techniques such as pruning, quantization, and distillation.\nThis approach can lead to more effective compression and optimization as the model adapts to the sparsity constraints during training.\nTraining-aware sparsification can be more complex and computationally intensive than post-training sparsification, but it can often achieve better results regarding model size, speed, and accuracy."}),"\n",(0,a.jsx)(i.h3,{id:"transfer-learning",children:"Transfer Learning"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification can be combined with transfer learning, where a sparsified, pre-trained model is fine-tuned on a new task or dataset.\nThis approach can leverage the knowledge and compression of the pre-trained model without the complexity of sparsification hyperparameters or training from scratch.\nTransfer learning with sparsification can be particularly effective for quickly adapting compressed models to new tasks or domains with fewer resources and complexity while closely matching the performance of training-aware techniques."}),"\n",(0,a.jsx)(i.h2,{id:"recipes",children:"Recipes"}),"\n",(0,a.jsx)(i.p,{children:"Sparsification recipes provide a structured and reusable way to define the steps and parameters for optimizing neural networks.\nThey encapsulate the specific sparsification techniques, hyperparameters, and necessary training adjustments into a single configuration file.\nSparsification recipes can be shared, reused, and adapted across different models, tasks, and domains, making experimenting with and deploying compressed models easier."}),"\n",(0,a.jsx)(i.p,{children:"Recipes are core to the sparsification process through SparseML, a comprehensive framework for sparsification and model optimization.\nAdditionally, models generally available in the SparseZoo or our HuggingFace model hub include the recipes used to train them, making it easy to reproduce and adapt the training process."}),"\n",(0,a.jsx)(i.p,{children:"Throughout the sparsification guides, you'll find example recipes for different techniques and applications, providing a hands-on approach to implementing and experimenting with sparsification."}),"\n",(0,a.jsx)(i.p,{children:"A general workflow for sparsification using SparseML is as follows:"}),"\n",(0,a.jsxs)(i.ol,{children:["\n",(0,a.jsx)(i.li,{children:"Define a sparsification recipe for the desired technique and application."}),"\n",(0,a.jsx)(i.li,{children:"Integrate SparseML into your experimentation pipelines or utilize the pre-built pipelines in SparseML."}),"\n",(0,a.jsx)(i.li,{children:"Apply the sparsification recipe to your model through one-shot, training-aware, or transfer learning methods."}),"\n",(0,a.jsx)(i.li,{children:"Evaluate the compressed model on your desired metrics and tasks."}),"\n"]}),"\n",(0,a.jsx)(i.hr,{}),"\n",(0,a.jsx)(i.p,{children:"Dive into the guides in this section to learn more about the core sparsification techniques, applications, and recipes for compressing and optimizing neural networks."})]})}function p(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>s,x:()=>o});var a=n(6540);const t={},r=a.createContext(t);function s(e){const i=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(r.Provider,{value:i},e.children)}}}]);