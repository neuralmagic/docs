"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[6211],{8166:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>d});var r=s(4848),t=s(8453);const o={sidebar_position:2,title:"Convert LLMs From Hugging Face"},l="I have a Hugging Face model; how do I use it with DeepSparse?",a={id:"llms/guides/hf-llm-to-deepsparse",title:"Convert LLMs From Hugging Face",description:"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse.",source:"@site/docs/llms/guides/hf-llm-to-deepsparse.mdx",sourceDirName:"llms/guides",slug:"/llms/guides/hf-llm-to-deepsparse",permalink:"/docs-v2/next/llms/guides/hf-llm-to-deepsparse",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/llms/guides/hf-llm-to-deepsparse.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Convert LLMs From Hugging Face"},sidebar:"autogenerated_docs",previous:{title:"Why is Sparsity Important for LLMs?",permalink:"/docs-v2/next/llms/guides/why-weight-sparsity"},next:{title:"Compress LLMs With SparseGPT",permalink:"/docs-v2/next/llms/guides/one-shot-llms-with-sparseml"}},i={},d=[{value:"Step 1: Install SparseML With Hugging Face Transformers Support",id:"step-1-install-sparseml-with-hugging-face-transformers-support",level:3},{value:"Note on system requirements",id:"note-on-system-requirements",level:4},{value:"Step 2: Prepare Your Model",id:"step-2-prepare-your-model",level:3},{value:"Step 3: Export the Model to ONNX",id:"step-3-export-the-model-to-onnx",level:3},{value:"Step 4: Run Inference With DeepSparse",id:"step-4-run-inference-with-deepsparse",level:3},{value:"Benchmark Example",id:"benchmark-example",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"i-have-a-hugging-face-model-how-do-i-use-it-with-deepsparse",children:"I have a Hugging Face model; how do I use it with DeepSparse?"}),"\n",(0,r.jsx)(n.p,{children:"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse."}),"\n",(0,r.jsx)(n.h3,{id:"step-1-install-sparseml-with-hugging-face-transformers-support",children:"Step 1: Install SparseML With Hugging Face Transformers Support"}),"\n",(0,r.jsx)(n.p,{children:"SparseML provides tools for optimizing machine learning models for deployment. To install it along with the necessary support for Hugging Face Transformers, open your terminal and run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install sparseml-nightly[transformers]\n"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.h4,{id:"note-on-system-requirements",children:"Note on system requirements"}),"\n",(0,r.jsxs)(n.p,{children:["Due to inefficiencies in PyTorch ONNX export, a lot of system memory is required to export the models for inference. There are ",(0,r.jsx)(n.a,{href:"https://github.com/pytorch/pytorch/commit/b4a49124c8165a374a3ef49e14807ac05b3fc030",children:"improvements coming in torch>=2.2 so use the latest version possible"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-prepare-your-model",children:"Step 2: Prepare Your Model"}),"\n",(0,r.jsx)(n.p,{children:"Before exporting your model, you need to download its weights from the Hugging Face Hub."}),"\n",(0,r.jsx)(n.p,{children:"Use the Hugging Face CLI tool to download the model of your choice. For example, to download the TinyLlama model, run:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0 --local-dir original_model --local-dir-use-symlinks False\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This command saves the model to a directory named ",(0,r.jsx)(n.code,{children:"original_model"})," in your current working directory."]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-export-the-model-to-onnx",children:"Step 3: Export the Model to ONNX"}),"\n",(0,r.jsx)(n.p,{children:"Now, you'll export the model to the ONNX format, which is compatible with DeepSparse."}),"\n",(0,r.jsx)(n.p,{children:"Run the following command to export your model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sparseml.export --task text-generation original_model/\n"})}),"\n",(0,r.jsxs)(n.p,{children:["This process creates a new folder named ",(0,r.jsx)(n.code,{children:"deployment/"})," containing the exported model in ONNX format."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optional: Uploading the Model back to Hugging Face Hub"}),":\nIf desired, you can upload the exported model back to the Hugging Face Hub for easy access or sharing. Replace ",(0,r.jsx)(n.code,{children:"username/your-model-id"})," with your Hugging Face username and a new model ID."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"huggingface-cli upload --repo-type model username/your-model-id-ds original_model/deployment/\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Make sure to add the ",(0,r.jsx)(n.code,{children:"-ds"})," ending and use the ",(0,r.jsx)(n.code,{children:"deepsparse"})," in your README to make it easy to find compatible models!"]}),"\n",(0,r.jsx)(n.h3,{id:"step-4-run-inference-with-deepsparse",children:"Step 4: Run Inference With DeepSparse"}),"\n",(0,r.jsx)(n.p,{children:"After exporting your model, you can run inference using DeepSparse."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Install DeepSparse LLM"}),":\nInstall the DeepSparse library, which is specifically designed for running inference on large language models (LLMs) efficiently."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install deepsparse-nightly[llm]\n"})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Load Your Model and Run Inference"}),":\nUse the following Python code to load your model and generate a response to a prompt:"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from deepsparse import TextGeneration\n\n# Replace the model_path with the path to your exported model or use a model from the DeepSparse Zoo\nmodel = TextGeneration(model_path="path_to_your_exported_model")\nprompt = "Your prompt here"\nprint(model(prompt).generations[0].text)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Replace ",(0,r.jsx)(n.code,{children:'"path_to_your_exported_model"'})," with the actual path to your ONNX model file. If you uploaded your model to the Hugging Face Hub, you can use the model's Hugging Face Hub path instead."]}),"\n",(0,r.jsx)(n.h2,{id:"benchmark-example",children:"Benchmark Example"}),"\n",(0,r.jsxs)(n.p,{children:["Benchmarking was performed on an AWS m7i.4xlarge instance using ",(0,r.jsx)(n.code,{children:"deepsparse-nightly[llm]==1.7.0.20240103"})," with FP32 dense and sparse Llama models finetuned on GSM8k - full details of those models can be found in the ",(0,r.jsx)(n.a,{href:"https://neuralmagic.com/blog/fast-llama-2-on-cpus-with-sparse-fine-tuning-and-deepsparse/",children:"release blog"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["These benchmarks used ",(0,r.jsx)(n.a,{href:"https://sparsezoo.neuralmagic.com/?architectures=llama2&datasets=gsm8k&ungrouped=true",children:"models from SparseZoo"}),", as seen from the prepended ",(0,r.jsx)(n.code,{children:"zoo:"}),", but you can also use exported models hosted on Hugging Face by prepending with ",(0,r.jsx)(n.code,{children:"hf:"})," such as ",(0,r.jsx)(n.a,{href:"https://huggingface.co/neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds",children:(0,r.jsx)(n.code,{children:"hf:neuralmagic/TinyLlama-1.1B-Chat-v0.4-pruned50-quant-ds"})}),"."]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Sparsity"}),(0,r.jsx)(n.th,{children:"Decode tokens/s"}),(0,r.jsx)(n.th,{children:"Decode Speedup"}),(0,r.jsx)(n.th,{children:"Prefill tokens/s (128 token input)"}),(0,r.jsx)(n.th,{children:"Prefill Speedup"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"0%"}),(0,r.jsx)(n.td,{children:"3.63"}),(0,r.jsx)(n.td,{children:"1.00"}),(0,r.jsx)(n.td,{children:"66.06"}),(0,r.jsx)(n.td,{children:"1.00"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"50%"}),(0,r.jsx)(n.td,{children:"6.44"}),(0,r.jsx)(n.td,{children:"1.77"}),(0,r.jsx)(n.td,{children:"91.53"}),(0,r.jsx)(n.td,{children:"1.39"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"60%"}),(0,r.jsx)(n.td,{children:"7.79"}),(0,r.jsx)(n.td,{children:"2.14"}),(0,r.jsx)(n.td,{children:"101.71"}),(0,r.jsx)(n.td,{children:"1.54"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"70%"}),(0,r.jsx)(n.td,{children:"9.82"}),(0,r.jsx)(n.td,{children:"2.70"}),(0,r.jsx)(n.td,{children:"115.87"}),(0,r.jsx)(n.td,{children:"1.75"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"80%"}),(0,r.jsx)(n.td,{children:"13.17"}),(0,r.jsx)(n.td,{children:"3.62"}),(0,r.jsx)(n.td,{children:"140.62"}),(0,r.jsx)(n.td,{children:"2.13"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Benchmarking commands:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"export NM_BENCHMARK_KV_TOKENS=1\n\n# Decode benchmarking: Time to generate a token aka generated token/s\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-base\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned50\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned70\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 1 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned80\n\n# Prefill benchmarking: Time to process 128 tokens aka prefill token/s\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-base\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned50\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned60\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned70\ndeepsparse.benchmark --sequence_length 2048 --input_ids_length 128 --num_cores 8 zoo:llama2-7b-gsm8k_llama2_pretrain-pruned80\n"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var r=s(6540);const t={},o=r.createContext(t);function l(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);