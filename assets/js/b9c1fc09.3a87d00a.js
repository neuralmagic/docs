"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[8476],{6897:e=>{e.exports=JSON.parse('{"label":"optimization","permalink":"/next/tags/optimization","allTagsPath":"/next/tags","count":6,"items":[{"id":"get-started/index","title":"Getting Started","description":"Launch your Neural Magic journey with essential setup, installation guides, and foundational concepts.","permalink":"/next/get-started/"},{"id":"get-started/install/sparseml","title":"Installing SparseML","description":"Install SparseML, Neural Magic\'s toolkit for optimizing deep learning models through state-of-the-art sparsification techniques.","permalink":"/next/get-started/install/sparseml"},{"id":"get-started/optimize","title":"Optimizing LLMs","description":"Optimize large language models (LLMs) for efficient inference using one-shot pruning and quantization. Learn how to improve model performance and reduce costs without sacrificing accuracy.","permalink":"/next/get-started/optimize"},{"id":"llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml","title":"Sparse Fine-Tuning Llama2 7B on GSM8k","description":"Guide on sparse fine-tuning Llama2 7B model on GSM8K dataset, including steps, commands, and recipes for optimization.","permalink":"/next/llms/guides/sparse-finetuning-llm-gsm8k-with-sparseml"},{"id":"get-started/finetune","title":"Sparse Fine-Tuning With LLMs","description":"Improve the performance of your large language models (LLMs) through fine-tuning with Neural Magic\'s SparseML. Optimize LLMs for specific tasks while maintaining accuracy.","permalink":"/next/get-started/finetune"},{"id":"get-started/transfer","title":"Sparse Transferring LLMs","description":"Adapt large language models (LLMs) to new domains and tasks using sparse transfer learning with Neural Magic\'s SparseML. Maintain accuracy while optimizing for efficiency.","permalink":"/next/get-started/transfer"}],"unlisted":false}')}}]);