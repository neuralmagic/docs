"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[8581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"nightly","banner":"unreleased","badge":true,"noIndex":false,"className":"docs-version-current","isLast":false,"docsSidebars":{"autogenerated_docs":[{"type":"link","label":"Home","href":"/docs-v2/next/","docId":"home","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":false,"items":[{"type":"category","label":"Install","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"DeepSparse","href":"/docs-v2/next/get-started/install/deepsparse","docId":"get-started/install/deepsparse","unlisted":false},{"type":"link","label":"SparseML","href":"/docs-v2/next/get-started/install/sparseml","docId":"get-started/install/sparseml","unlisted":false},{"type":"link","label":"SparseZoo","href":"/docs-v2/next/get-started/install/sparsezoo","docId":"get-started/install/sparsezoo","unlisted":false}],"href":"/docs-v2/next/get-started/install/"},{"type":"link","label":"Deploy","href":"/docs-v2/next/get-started/deploy","docId":"get-started/deploy","unlisted":false},{"type":"link","label":"Optimize","href":"/docs-v2/next/get-started/optimize","docId":"get-started/optimize","unlisted":false},{"type":"link","label":"Sparse Fine-Tuning","href":"/docs-v2/next/get-started/finetune","docId":"get-started/finetune","unlisted":false},{"type":"link","label":"Sparse Transfer","href":"/docs-v2/next/get-started/transfer","docId":"get-started/transfer","unlisted":false}],"href":"/docs-v2/next/get-started/"},{"type":"category","label":"LLMs - Causal Language Modeling","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Serving LLMs","href":"/docs-v2/next/llms/serving-llms","docId":"llms/serving-llms","unlisted":false},{"type":"category","label":"Models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama 2","href":"/docs-v2/next/llms/models/sparse-foundational-llama-2","docId":"llms/models/sparse-foundational-llama-2","unlisted":false}],"href":"/docs-v2/next/llms/models/"},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Why is Sparsity Important for LLMs?","href":"/docs-v2/next/llms/guides/why-weight-sparsity","docId":"llms/guides/why-weight-sparsity","unlisted":false},{"type":"link","label":"Convert LLMs From Hugging Face","href":"/docs-v2/next/llms/guides/hf-llm-to-deepsparse","docId":"llms/guides/hf-llm-to-deepsparse","unlisted":false},{"type":"link","label":"Compress LLMs With SparseGPT","href":"/docs-v2/next/llms/guides/one-shot-llms-with-sparseml","docId":"llms/guides/one-shot-llms-with-sparseml","unlisted":false},{"type":"link","label":"LLM Serving on Windows","href":"/docs-v2/next/llms/guides/llm-serving-on-windows","docId":"llms/guides/llm-serving-on-windows","unlisted":false}],"href":"/docs-v2/next/llms/guides/"}],"href":"/docs-v2/next/llms/"},{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Detection","href":"/docs-v2/next/computer-vision/object-detection/","docId":"computer-vision/object-detection/index","unlisted":false},{"type":"link","label":"Image Segmentation","href":"/docs-v2/next/computer-vision/image-segmentation/","docId":"computer-vision/image-segmentation/index","unlisted":false},{"type":"link","label":"Image Classification","href":"/docs-v2/next/computer-vision/image-classification/","docId":"computer-vision/image-classification/index","unlisted":false}],"href":"/docs-v2/next/computer-vision/"},{"type":"link","label":"Natural Language Processing","href":"/docs-v2/next/nlp/","docId":"nlp/index","unlisted":false},{"type":"link","label":"Guides","href":"/docs-v2/next/guides/","docId":"guides/index","unlisted":false},{"type":"category","label":"Products","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"DeepSparse","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/docs-v2/next/products/deepsparse/releases","docId":"products/deepsparse/releases","unlisted":false}],"href":"/docs-v2/next/products/deepsparse/"},{"type":"category","label":"SparseML","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/docs-v2/next/products/sparseml/releases","docId":"products/sparseml/releases","unlisted":false}],"href":"/docs-v2/next/products/sparseml/"},{"type":"category","label":"SparseZoo","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/docs-v2/next/products/sparsezoo/releases","docId":"products/sparsezoo/releases","unlisted":false}],"href":"/docs-v2/next/products/sparsezoo/"}],"href":"/docs-v2/next/products/"},{"type":"category","label":"Details","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"FAQs","href":"/docs-v2/next/details/faqs","docId":"details/faqs","unlisted":false},{"type":"link","label":"Glossary","href":"/docs-v2/next/details/glossary","docId":"details/glossary","unlisted":false},{"type":"link","label":"Research Papers","href":"/docs-v2/next/details/research-papers","docId":"details/research-papers","unlisted":false}],"href":"/docs-v2/next/details/"}]},"docs":{"computer-vision/image-classification/index":{"id":"computer-vision/image-classification/index","title":"Image Classification","description":"Optimize and deploy cutting-edge models for image classification.","sidebar":"autogenerated_docs"},"computer-vision/image-segmentation/index":{"id":"computer-vision/image-segmentation/index","title":"Image Segmentation","description":"Optimize and deploy cutting-edge models for image segmentation.","sidebar":"autogenerated_docs"},"computer-vision/index":{"id":"computer-vision/index","title":"Computer Vision","description":"Optimize and deploy cutting-edge computer vision models for image classification, object detection, and complex image segmentation tasks.","sidebar":"autogenerated_docs"},"computer-vision/object-detection/index":{"id":"computer-vision/object-detection/index","title":"Object Detection","description":"Optimize and deploy cutting-edge models for object detection.","sidebar":"autogenerated_docs"},"details/faqs":{"id":"details/faqs","title":"FAQs","description":"General Product FAQs","sidebar":"autogenerated_docs"},"details/glossary":{"id":"details/glossary","title":"Glossary","description":"Terms and Definitions","sidebar":"autogenerated_docs"},"details/index":{"id":"details/index","title":"Details","description":"Explore in-depth tutorials and walkthroughs showcasing best practices for model optimization, deployment, and use-case-specific applications.","sidebar":"autogenerated_docs"},"details/research-papers":{"id":"details/research-papers","title":"Research Papers","description":"Research papers and technical documentation across Neural Magic\'s products and solutions.","sidebar":"autogenerated_docs"},"get-started/deploy":{"id":"get-started/deploy","title":"Deploying LLMs","description":"Deploy large language models (LLMs) for text generation using Neural Magic\'s DeepSparse. This doc includes code examples, performance benchmarking, and server setup.","sidebar":"autogenerated_docs"},"get-started/finetune":{"id":"get-started/finetune","title":"Sparse Fine-Tuning With LLMs","description":"Improve the performance of your large language models (LLMs) through fine-tuning with Neural Magic\'s SparseML. Optimize LLMs for specific tasks while maintaining accuracy.","sidebar":"autogenerated_docs"},"get-started/index":{"id":"get-started/index","title":"Getting Started","description":"Launch your Neural Magic journey with essential setup, installation guides, and foundational concepts.","sidebar":"autogenerated_docs"},"get-started/install/deepsparse":{"id":"get-started/install/deepsparse","title":"Installing DeepSparse","description":"Install DeepSparse, Neural Magic\'s high-performance inference engine, for optimized deep learning model deployment on CPUs.","sidebar":"autogenerated_docs"},"get-started/install/index":{"id":"get-started/install/index","title":"Installation","description":"Step-by-step guides for installing NeuralMagic Products such as DeepSparse, SparseML, and SparseZoo.","sidebar":"autogenerated_docs"},"get-started/install/sparseml":{"id":"get-started/install/sparseml","title":"Installing SparseML","description":"Install SparseML, Neural Magic\'s toolkit for optimizing deep learning models through state-of-the-art sparsification techniques.","sidebar":"autogenerated_docs"},"get-started/install/sparsezoo":{"id":"get-started/install/sparsezoo","title":"Installing SparseZoo","description":"Install SparseZoo, Neural Magic\'s repository of pre-sparsified models, or learn how to access it through SparseML and DeepSparse.","sidebar":"autogenerated_docs"},"get-started/optimize":{"id":"get-started/optimize","title":"Optimizing LLMs","description":"Optimize large language models (LLMs) for efficient inference using one-shot pruning and quantization. Learn how to improve model performance and reduce costs without sacrificing accuracy.","sidebar":"autogenerated_docs"},"get-started/transfer":{"id":"get-started/transfer","title":"Sparse Transferring LLMs","description":"Adapt large language models (LLMs) to new domains and tasks using sparse transfer learning with Neural Magic\'s SparseML. Maintain accuracy while optimizing for efficiency.","sidebar":"autogenerated_docs"},"guides/index":{"id":"guides/index","title":"Guides","description":"Access FAQs, a glossary of terms, and insightful research papers to deepen your knowledge of Neural Magic\'s technology.","sidebar":"autogenerated_docs"},"home":{"id":"home","title":"What is Neural Magic?","description":"Neural Magic empowers you to optimize and deploy deep learning models on CPUs with GPU-class performance. Unlock efficiency, accessibility, and cost savings with our software solutions.","sidebar":"autogenerated_docs"},"llms/guides/hf-llm-to-deepsparse":{"id":"llms/guides/hf-llm-to-deepsparse","title":"Convert LLMs From Hugging Face","description":"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse.","sidebar":"autogenerated_docs"},"llms/guides/index":{"id":"llms/guides/index","title":"Guides","description":"Explore best practices and step-by-step tutorials for specific LLM use cases.","sidebar":"autogenerated_docs"},"llms/guides/llm-serving-on-windows":{"id":"llms/guides/llm-serving-on-windows","title":"LLM Serving on Windows","description":"Here is a guide for running a large language model (LLM) for text generation on Windows using Windows Subsystem for Linux (WSL) and DeepSparse Server","sidebar":"autogenerated_docs"},"llms/guides/one-shot-llms-with-sparseml":{"id":"llms/guides/one-shot-llms-with-sparseml","title":"Compress LLMs With SparseGPT","description":"This page describes how to perform one-shot quantization of large language models using SparseML. This workflow requires a GPU with at least 16GB VRAM and 64GB of system RAM.","sidebar":"autogenerated_docs"},"llms/guides/why-weight-sparsity":{"id":"llms/guides/why-weight-sparsity","title":"Why is Sparsity Important for LLMs?","description":"Large Language Models (LLMs) have a large size that often poses challenges in terms of computational efficiency and memory usage. Weight sparsity is a technique that can significantly alleviate these issues, enhancing the practicality and scalability of LLMs. Here we outline the key benefits of weight sparsity in LLMs, focusing on three main aspects:","sidebar":"autogenerated_docs"},"llms/index":{"id":"llms/index","title":"LLMs - Causal Language Modeling","description":"Harness the power of causal language models for creative text generation tasks, including creative writing, dialogue simulation, and code writing assistance.","sidebar":"autogenerated_docs"},"llms/models/index":{"id":"llms/models/index","title":"Sparse LLMs","description":"Discover and utilize optimized LLM models from SparseZoo and Hugging Face Hub for efficient DeepSparse deployment.","sidebar":"autogenerated_docs"},"llms/models/sparse-foundational-llama-2":{"id":"llms/models/sparse-foundational-llama-2","title":"Sparse Foundational Llama 2 Models","description":"Neural Magic offers a range of expertly optimized Llama 2-based Large Language Models (LLMs) that have been sparsified for superior performance and reduced footprint.","sidebar":"autogenerated_docs"},"llms/serving-llms":{"id":"llms/serving-llms","title":"Serving LLMs","description":"DeepSparse is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with SparseML, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.","sidebar":"autogenerated_docs"},"nlp/index":{"id":"nlp/index","title":"Natural Language Processing","description":"Optimize and deploy cutting-edge models for natural language processing.","sidebar":"autogenerated_docs"},"products/deepsparse/index":{"id":"products/deepsparse/index","title":"DeepSparse","description":"GitHub","sidebar":"autogenerated_docs"},"products/deepsparse/releases":{"id":"products/deepsparse/releases","title":"DeepSparse Releases","description":"Versions","sidebar":"autogenerated_docs"},"products/index":{"id":"products/index","title":"Products","description":"Gain a comprehensive understanding of Neural Magic\'s core products (SparseML, DeepSparse, SparseZoo) and their key features.","sidebar":"autogenerated_docs"},"products/sparseml/index":{"id":"products/sparseml/index","title":"SparseML","description":"GitHub","sidebar":"autogenerated_docs"},"products/sparseml/releases":{"id":"products/sparseml/releases","title":"SparseML Releases","description":"Versions","sidebar":"autogenerated_docs"},"products/sparsezoo/index":{"id":"products/sparsezoo/index","title":"SparseZoo","description":"GitHub","sidebar":"autogenerated_docs"},"products/sparsezoo/releases":{"id":"products/sparsezoo/releases","title":"SparseZoo Releases","description":"Versions","sidebar":"autogenerated_docs"}}}')}}]);