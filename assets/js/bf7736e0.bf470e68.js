"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[1977],{9005:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>t,toc:()=>d});var r=s(4848),o=s(8453);const a={tags:["to-do"],keywords:["to-do"],description:"Deploying With DeepSparse Server for faster and cheaper model deployments behind an HTTP API.",sidebar_label:"DeepSparse Server",sidebar_position:1},i="Deploying With DeepSparse Server",t={id:"guides/deploying-deepsparse/deepsparse-server",title:"Deploying With DeepSparse Server",description:"Deploying With DeepSparse Server for faster and cheaper model deployments behind an HTTP API.",source:"@site/docs/guides/deploying-deepsparse/deepsparse-server.mdx",sourceDirName:"guides/deploying-deepsparse",slug:"/guides/deploying-deepsparse/deepsparse-server",permalink:"/next/guides/deploying-deepsparse/deepsparse-server",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/deploying-deepsparse/deepsparse-server.mdx",tags:[{label:"to-do",permalink:"/next/tags/to-do"}],version:"current",sidebarPosition:1,frontMatter:{tags:["to-do"],keywords:["to-do"],description:"Deploying With DeepSparse Server for faster and cheaper model deployments behind an HTTP API.",sidebar_label:"DeepSparse Server",sidebar_position:1},sidebar:"autogenerated_docs",previous:{title:"DeepSparse Features",permalink:"/next/guides/deploying-deepsparse/"},next:{title:"Amazon SageMaker",permalink:"/next/guides/deploying-deepsparse/amazon-sagemaker"}},l={},d=[{value:"Installation Requirements",id:"installation-requirements",level:2},{value:"Usage",id:"usage",level:2},{value:"Single Model Inference",id:"single-model-inference",level:2},{value:"Multiple Model Inference",id:"multiple-model-inference",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"deploying-with-deepsparse-server",children:"Deploying With DeepSparse Server"}),"\n",(0,r.jsx)(n.p,{children:"This section explains how to deploy with DeepSparse Server."}),"\n",(0,r.jsx)(n.h2,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,r.jsxs)(n.p,{children:["This use case requires the installation of ",(0,r.jsx)(n.a,{href:"/get-started/install/deepsparse",children:"DeepSparse Server"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsxs)(n.p,{children:["DeepSparse Server allows you to serve models and ",(0,r.jsx)(n.code,{children:"Pipelines"})," for deployment in HTTP. The server runs on top of the popular FastAPI web framework and Uvicorn web server.\nThe server supports any task from DeepSparse, such as ",(0,r.jsx)(n.code,{children:"Pipelines"})," including NLP, image classification, and object detection tasks.\nAn updated list of available tasks can be found\n",(0,r.jsx)(n.a,{href:"https://github.com/neuralmagic/deepsparse/blob/main/src/deepsparse/PIPELINES.md",children:"in the DeepSparse Pipelines Introduction"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Run the help CLI to look up the available arguments."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'$ deepsparse.server --help\n\n> Usage: deepsparse.server [OPTIONS] COMMAND [ARGS]...\n>\n>   Start a DeepSparse inference server for serving the models and pipelines.\n>\n>       1. `deepsparse.server config [OPTIONS] <config path>`\n>\n>       2. `deepsparse.server task [OPTIONS] <task>\n>\n>   Examples for using the server:\n>\n>       `deepsparse.server config server-config.yaml`\n>\n>       `deepsparse.server task question_answering --batch-size 2`\n>\n>       `deepsparse.server task question_answering --host "0.0.0.0"`\n>\n>   Example config.yaml for serving:\n>\n>   \\```yaml\n>   num_cores: 2\n>   num_workers: 2\n>   endpoints:\n>     - task: question_answering\n>       route: /unpruned/predict\n>       model: zoo:some/zoo/stub\n>     - task: question_answering\n>       route: /pruned/predict\n>       model: /path/to/local/model\n>   \\```\n>\n> Options:\n>   --help  Show this message and exit.\n>\n> Commands:\n>   config  Run the server using configuration from a .yaml file.\n>   task    Run the server using configuration with CLI options, which can...\n'})}),"\n",(0,r.jsx)(n.h2,{id:"single-model-inference",children:"Single Model Inference"}),"\n",(0,r.jsx)(n.p,{children:"Here is an example CLI command for serving a single model for the question answering task:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'deepsparse.server \\\n    task question_answering \\\n    --model_path "zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni"\n'})}),"\n",(0,r.jsxs)(n.p,{children:["To make a request to your server, use the ",(0,r.jsx)(n.code,{children:"requests"})," library and pass the request URL:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\n\nurl = "http://localhost:5543/predict"\n\nobj = {\n    "question": "Who is Mark?",\n    "context": "Mark is batman."\n}\n\nresponse = requests.post(url, json=obj)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In addition, you can make a request with a ",(0,r.jsx)(n.code,{children:"curl"})," command from the terminal:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"curl -X POST \\\n  'http://localhost:5543/predict' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"question\": \"Who is Mark?\",\n  \"context\": \"Mark is batman.\"\n}'\n"})}),"\n",(0,r.jsx)(n.h2,{id:"multiple-model-inference",children:"Multiple Model Inference"}),"\n",(0,r.jsxs)(n.p,{children:["To serve multiple models, you can build a ",(0,r.jsx)(n.code,{children:"config.yaml"})," file.\nIn the sample YAML file below, we are defining two BERT models to be served by the ",(0,r.jsx)(n.code,{children:"deepsparse.server"})," for the question answering task:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"num_cores: 2\nnum_workers: 2\nendpoints:\n    - task: question_answering\n      route: /unpruned/predict\n      model: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/base-none\n      batch_size: 1\n    - task: question_answering\n      route: /pruned/predict\n      model: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/12layer_pruned80_quant-none-vnni\n      batch_size: 1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You can now run the server with the configuration file path using the ",(0,r.jsx)(n.code,{children:"config"})," subcommand:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"deepsparse.server config config.yaml\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You can send requests to a specific model by appending the model's ",(0,r.jsx)(n.code,{children:"alias"})," from the ",(0,r.jsx)(n.code,{children:"config.yaml"})," to the end of the request url. For example, to call the second model, you can send a request to its configured route:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import requests\n\nurl = "http://localhost:5543/pruned/predict"\n\nobj = {\n    "question": "Who is Mark?",\n    "context": "Mark is batman."\n}\n\nresponse = requests.post(url, json=obj)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PRO TIP:"})," While your server is running, you can always use the awesome swagger UI that's built into FastAPI to view your model's pipeline ",(0,r.jsx)(n.code,{children:"POST"})," routes.\nThe UI also enables you to easily make sample requests to your server.\nAll you need is to add ",(0,r.jsx)(n.code,{children:"/docs"})," at the end of your host URL:"]}),"\n",(0,r.jsx)(n.p,{children:"localhost:5543/docs"}),"\n",(0,r.jsx)("img",{src:"https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/server/img/swagger_ui.png",alt:"Swagger UI For Viewing Model Pipeline",width:"1200",height:"524"})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>t});var r=s(6540);const o={},a=r.createContext(o);function i(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);