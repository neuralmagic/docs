"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[7264],{3504:(e,s,a)=>{a.r(s),a.d(s,{assets:()=>i,contentTitle:()=>r,default:()=>m,frontMatter:()=>n,metadata:()=>t,toc:()=>d});var o=a(4848),l=a(8453);const n={tags:["TODO"],keywords:["TODO"],sidebar_position:1,sidebar_label:"Llama 2"},r="Sparse Foundational Llama 2 Models",t={id:"llms/models/sparse-foundational-llama-2",title:"Sparse Foundational Llama 2 Models",description:"Neural Magic offers a range of expertly optimized Llama 2-based Large Language Models (LLMs) that have been sparsified for superior performance and reduced footprint.",source:"@site/versioned_docs/version-1.7.0/llms/models/sparse-foundational-llama-2.mdx",sourceDirName:"llms/models",slug:"/llms/models/sparse-foundational-llama-2",permalink:"/docs-v2/llms/models/sparse-foundational-llama-2",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/llms/models/sparse-foundational-llama-2.mdx",tags:[{label:"TODO",permalink:"/docs-v2/tags/todo"}],version:"1.7.0",sidebarPosition:1,frontMatter:{tags:["TODO"],keywords:["TODO"],sidebar_position:1,sidebar_label:"Llama 2"},sidebar:"tutorialSidebar",previous:{title:"Models",permalink:"/docs-v2/llms/models/"},next:{title:"Hugging Face Hub Models",permalink:"/docs-v2/llms/models/huggingface-models"}},i={},d=[{value:"Why Choose Sparse Llama 2 Models?",id:"why-choose-sparse-llama-2-models",level:2}];function c(e){const s={h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(s.h1,{id:"sparse-foundational-llama-2-models",children:"Sparse Foundational Llama 2 Models"}),"\n",(0,o.jsx)(s.p,{children:"Neural Magic offers a range of expertly optimized Llama 2-based Large Language Models (LLMs) that have been sparsified for superior performance and reduced footprint.\nThese models are carefully selected and rigorously tested, ensuring exceptional quality and seamless deployment."}),"\n",(0,o.jsx)(s.h2,{id:"why-choose-sparse-llama-2-models",children:"Why Choose Sparse Llama 2 Models?"}),"\n",(0,o.jsxs)(s.ul,{children:["\n",(0,o.jsx)(s.li,{children:"Accelerated Inference: Sparse Llama 2 models offer significant speed improvements, enabling faster responses and real-time applications."}),"\n",(0,o.jsx)(s.li,{children:"Reduced Resource Requirements: Sparsification decreases the model's size, allowing deployment on edge devices or in environments with limited compute power."}),"\n",(0,o.jsx)(s.li,{children:"Cost-Effectiveness: Lower compute requirements translate to reduced operational costs for your LLM-based applications."}),"\n"]})]})}function m(e={}){const{wrapper:s}={...(0,l.R)(),...e.components};return s?(0,o.jsx)(s,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,s,a)=>{a.d(s,{R:()=>r,x:()=>t});var o=a(6540);const l={},n=o.createContext(l);function r(e){const s=o.useContext(n);return o.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function t(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),o.createElement(n.Provider,{value:s},e.children)}}}]);