"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[6831],{2344:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"1.7.0","label":"1.7.0","banner":null,"badge":true,"noIndex":false,"className":"docs-version-1.7.0","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Home","href":"/","docId":"home","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":false,"items":[{"type":"category","label":"Install","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"DeepSparse","href":"/get-started/install/deepsparse","docId":"get-started/install/deepsparse","unlisted":false},{"type":"link","label":"SparseML","href":"/get-started/install/sparseml","docId":"get-started/install/sparseml","unlisted":false},{"type":"link","label":"SparseZoo","href":"/get-started/install/sparsezoo","docId":"get-started/install/sparsezoo","unlisted":false}],"href":"/get-started/install/"},{"type":"link","label":"Deploy","href":"/get-started/deploy","docId":"get-started/deploy","unlisted":false},{"type":"link","label":"Optimize","href":"/get-started/optimize","docId":"get-started/optimize","unlisted":false},{"type":"link","label":"Sparse Fine-Tuning","href":"/get-started/finetune","docId":"get-started/finetune","unlisted":false},{"type":"link","label":"Sparse Transfer","href":"/get-started/transfer","docId":"get-started/transfer","unlisted":false}],"href":"/get-started/"},{"type":"category","label":"LLMs - Causal Language Modeling","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Serving LLMs","href":"/llms/serving-llms","docId":"llms/serving-llms","unlisted":false},{"type":"category","label":"Models","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Llama 2","href":"/llms/models/sparse-foundational-llama-2","docId":"llms/models/sparse-foundational-llama-2","unlisted":false}],"href":"/llms/models/"},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Why is Sparsity Important for LLMs?","href":"/llms/guides/why-weight-sparsity","docId":"llms/guides/why-weight-sparsity","unlisted":false},{"type":"link","label":"Convert LLMs From Hugging Face","href":"/llms/guides/hf-llm-to-deepsparse","docId":"llms/guides/hf-llm-to-deepsparse","unlisted":false},{"type":"link","label":"Compress LLMs With SparseGPT","href":"/llms/guides/one-shot-llms-with-sparseml","docId":"llms/guides/one-shot-llms-with-sparseml","unlisted":false},{"type":"link","label":"LLM Serving on Windows","href":"/llms/guides/llm-serving-on-windows","docId":"llms/guides/llm-serving-on-windows","unlisted":false}],"href":"/llms/guides/"}],"href":"/llms/"},{"type":"category","label":"Computer Vision","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Detection","href":"/computer-vision/object-detection/","docId":"computer-vision/object-detection/index","unlisted":false},{"type":"link","label":"Image Segmentation","href":"/computer-vision/image-segmentation/","docId":"computer-vision/image-segmentation/index","unlisted":false},{"type":"link","label":"Image Classification","href":"/computer-vision/image-classification/","docId":"computer-vision/image-classification/index","unlisted":false}],"href":"/computer-vision/"},{"type":"link","label":"Natural Language Processing","href":"/nlp/","docId":"nlp/index","unlisted":false},{"type":"category","label":"Guides","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"DeepSparse Features","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Supported Hardware","href":"/guides/deepsparse-engine/hardware-support","docId":"guides/deepsparse-engine/hardware-support","unlisted":false},{"type":"link","label":"Inference Types","href":"/guides/deepsparse-engine/scheduler","docId":"guides/deepsparse-engine/scheduler","unlisted":false},{"type":"link","label":"Benchmarking","href":"/guides/deepsparse-engine/benchmarking","docId":"guides/deepsparse-engine/benchmarking","unlisted":false},{"type":"link","label":"Diagnostics/Debugging","href":"/guides/deepsparse-engine/diagnostics-debugging","docId":"guides/deepsparse-engine/diagnostics-debugging","unlisted":false},{"type":"link","label":"numactl Utility","href":"/guides/deepsparse-engine/numactl-utility","docId":"guides/deepsparse-engine/numactl-utility","unlisted":false},{"type":"link","label":"DeepSparse Logging","href":"/guides/deepsparse-engine/logging","docId":"guides/deepsparse-engine/logging","unlisted":false}],"href":"/guides/deepsparse-engine/"},{"type":"category","label":"Deployment Options","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"DeepSparse Server","href":"/guides/deploying-deepsparse/deepsparse-server","docId":"guides/deploying-deepsparse/deepsparse-server","unlisted":false},{"type":"link","label":"Amazon SageMaker","href":"/guides/deploying-deepsparse/amazon-sagemaker","docId":"guides/deploying-deepsparse/amazon-sagemaker","unlisted":false},{"type":"link","label":"AWS Lambda","href":"/guides/deploying-deepsparse/aws-lambda","docId":"guides/deploying-deepsparse/aws-lambda","unlisted":false},{"type":"link","label":"Google Cloud Run","href":"/guides/deploying-deepsparse/google-cloud-run","docId":"guides/deploying-deepsparse/google-cloud-run","unlisted":false}],"href":"/guides/deploying-deepsparse/"}],"href":"/guides/"},{"type":"category","label":"Products","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"DeepSparse","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/products/deepsparse/releases","docId":"products/deepsparse/releases","unlisted":false}],"href":"/products/deepsparse/"},{"type":"category","label":"SparseML","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/products/sparseml/releases","docId":"products/sparseml/releases","unlisted":false}],"href":"/products/sparseml/"},{"type":"category","label":"SparseZoo","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Releases","href":"/products/sparsezoo/releases","docId":"products/sparsezoo/releases","unlisted":false}],"href":"/products/sparsezoo/"}],"href":"/products/"},{"type":"category","label":"Details","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"FAQs","href":"/details/faqs","docId":"details/faqs","unlisted":false},{"type":"link","label":"Glossary","href":"/details/glossary","docId":"details/glossary","unlisted":false},{"type":"link","label":"Research Papers","href":"/details/research-papers","docId":"details/research-papers","unlisted":false}],"href":"/details/"}]},"docs":{"computer-vision/image-classification/index":{"id":"computer-vision/image-classification/index","title":"Image Classification","description":"Optimize and deploy cutting-edge models for image classification.","sidebar":"tutorialSidebar"},"computer-vision/image-segmentation/index":{"id":"computer-vision/image-segmentation/index","title":"Image Segmentation","description":"Optimize and deploy cutting-edge models for image segmentation.","sidebar":"tutorialSidebar"},"computer-vision/index":{"id":"computer-vision/index","title":"Computer Vision","description":"Optimize and deploy cutting-edge computer vision models for image classification, object detection, and complex image segmentation tasks.","sidebar":"tutorialSidebar"},"computer-vision/object-detection/index":{"id":"computer-vision/object-detection/index","title":"Object Detection","description":"Optimize and deploy cutting-edge models for object detection.","sidebar":"tutorialSidebar"},"details/faqs":{"id":"details/faqs","title":"FAQs","description":"General Product FAQs","sidebar":"tutorialSidebar"},"details/glossary":{"id":"details/glossary","title":"Glossary","description":"Terms and Definitions","sidebar":"tutorialSidebar"},"details/index":{"id":"details/index","title":"Details","description":"Explore in-depth tutorials and walkthroughs showcasing best practices for model optimization, deployment, and use-case-specific applications.","sidebar":"tutorialSidebar"},"details/research-papers":{"id":"details/research-papers","title":"Research Papers","description":"Research papers and technical documentation across Neural Magic\'s products and solutions.","sidebar":"tutorialSidebar"},"get-started/deploy":{"id":"get-started/deploy","title":"Deploying LLMs","description":"Deploy large language models (LLMs) for text generation using Neural Magic\'s DeepSparse. This doc includes code examples, performance benchmarking, and server setup.","sidebar":"tutorialSidebar"},"get-started/finetune":{"id":"get-started/finetune","title":"Sparse Fine-Tuning With LLMs","description":"Improve the performance of your large language models (LLMs) through fine-tuning with Neural Magic\'s SparseML. Optimize LLMs for specific tasks while maintaining accuracy.","sidebar":"tutorialSidebar"},"get-started/index":{"id":"get-started/index","title":"Getting Started","description":"Launch your Neural Magic journey with essential setup, installation guides, and foundational concepts.","sidebar":"tutorialSidebar"},"get-started/install/deepsparse":{"id":"get-started/install/deepsparse","title":"Installing DeepSparse","description":"Install DeepSparse, Neural Magic\'s high-performance inference engine, for optimized deep learning model deployment on CPUs.","sidebar":"tutorialSidebar"},"get-started/install/index":{"id":"get-started/install/index","title":"Installation","description":"Step-by-step guides for installing NeuralMagic Products such as DeepSparse, SparseML, and SparseZoo.","sidebar":"tutorialSidebar"},"get-started/install/sparseml":{"id":"get-started/install/sparseml","title":"Installing SparseML","description":"Install SparseML, Neural Magic\'s toolkit for optimizing deep learning models through state-of-the-art sparsification techniques.","sidebar":"tutorialSidebar"},"get-started/install/sparsezoo":{"id":"get-started/install/sparsezoo","title":"Installing SparseZoo","description":"Install SparseZoo, Neural Magic\'s repository of pre-sparsified models, or learn how to access it through SparseML and DeepSparse.","sidebar":"tutorialSidebar"},"get-started/optimize":{"id":"get-started/optimize","title":"Optimizing LLMs","description":"Optimize large language models (LLMs) for efficient inference using one-shot pruning and quantization. Learn how to improve model performance and reduce costs without sacrificing accuracy.","sidebar":"tutorialSidebar"},"get-started/transfer":{"id":"get-started/transfer","title":"Sparse Transferring LLMs","description":"Adapt large language models (LLMs) to new domains and tasks using sparse transfer learning with Neural Magic\'s SparseML. Maintain accuracy while optimizing for efficiency.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/benchmarking":{"id":"guides/deepsparse-engine/benchmarking","title":"Benchmarking ONNX Models With DeepSparse","description":"Benchmark ONNX models with DeepSparse.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/diagnostics-debugging":{"id":"guides/deepsparse-engine/diagnostics-debugging","title":"Logging Guidance for Diagnostics and Debugging","description":"Get DeepSparse logging and troubleshooting guidance.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/hardware-support":{"id":"guides/deepsparse-engine/hardware-support","title":"Supported Hardware for DeepSparse","description":"Use supported hardware with these CPU types and instruction sets.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/index":{"id":"guides/deepsparse-engine/index","title":"DeepSparse Features","description":"DeepSparse Feature Overview","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/logging":{"id":"guides/deepsparse-engine/logging","title":"DeepSparse Logging","description":"Monitor deployments and extract the data you need.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/numactl-utility":{"id":"guides/deepsparse-engine/numactl-utility","title":"Using the numactl Utility to Control Resource Utilization With DeepSparse","description":"Learn about numactl for effective process management.","sidebar":"tutorialSidebar"},"guides/deepsparse-engine/scheduler":{"id":"guides/deepsparse-engine/scheduler","title":"Inference Types With DeepSparse Scheduler","description":"Tune the performance to your workload.","sidebar":"tutorialSidebar"},"guides/deploying-deepsparse/amazon-sagemaker":{"id":"guides/deploying-deepsparse/amazon-sagemaker","title":"Deploying With DeepSparse on Amazon SageMaker","description":"Get sparse-CPU acceleration and automatic scaling on SageMaker.","sidebar":"tutorialSidebar"},"guides/deploying-deepsparse/aws-lambda":{"id":"guides/deploying-deepsparse/aws-lambda","title":"Deploying With DeepSparse on AWS Lambda","description":"Use event-driven, serverless computing infrastructure for deployment.","sidebar":"tutorialSidebar"},"guides/deploying-deepsparse/deepsparse-server":{"id":"guides/deploying-deepsparse/deepsparse-server","title":"Deploying With DeepSparse Server","description":"Wrap pipelines with a REST API using FastAPI. Send raw data over HTTP and receive the prediction.","sidebar":"tutorialSidebar"},"guides/deploying-deepsparse/google-cloud-run":{"id":"guides/deploying-deepsparse/google-cloud-run","title":"Deploying With DeepSparse on GCP Cloud Run","description":"Deploy DeepSparse in a Serverless framework with Google Cloud Run.","sidebar":"tutorialSidebar"},"guides/deploying-deepsparse/index":{"id":"guides/deploying-deepsparse/index","title":"DeepSparse Deployment Options","description":"DeepSparse Deployment Options","sidebar":"tutorialSidebar"},"guides/index":{"id":"guides/index","title":"Guides","description":"DeepSparse Features and Deployment Options. More to come.","sidebar":"tutorialSidebar"},"home":{"id":"home","title":"What is Neural Magic?","description":"Neural Magic empowers you to optimize and deploy deep learning models on CPUs with GPU-class performance. Unlock efficiency, accessibility, and cost savings with our software solutions.","sidebar":"tutorialSidebar"},"llms/guides/hf-llm-to-deepsparse":{"id":"llms/guides/hf-llm-to-deepsparse","title":"Convert LLMs From Hugging Face","description":"This guide is for people interested in exporting their Hugging Face-compatible LLMs to work in DeepSparse.","sidebar":"tutorialSidebar"},"llms/guides/index":{"id":"llms/guides/index","title":"Guides","description":"Explore best practices and step-by-step tutorials for specific LLM use cases.","sidebar":"tutorialSidebar"},"llms/guides/llm-serving-on-windows":{"id":"llms/guides/llm-serving-on-windows","title":"LLM Serving on Windows","description":"Here is a guide for running a large language model (LLM) for text generation on Windows using Windows Subsystem for Linux (WSL) and DeepSparse Server","sidebar":"tutorialSidebar"},"llms/guides/one-shot-llms-with-sparseml":{"id":"llms/guides/one-shot-llms-with-sparseml","title":"Compress LLMs With SparseGPT","description":"This page describes how to perform one-shot quantization of large language models using SparseML. This workflow requires a GPU with at least 16GB VRAM and 64GB of system RAM.","sidebar":"tutorialSidebar"},"llms/guides/why-weight-sparsity":{"id":"llms/guides/why-weight-sparsity","title":"Why is Sparsity Important for LLMs?","description":"Large Language Models (LLMs) have a large size that often poses challenges in terms of computational efficiency and memory usage. Weight sparsity is a technique that can significantly alleviate these issues, enhancing the practicality and scalability of LLMs. Here we outline the key benefits of weight sparsity in LLMs, focusing on three main aspects:","sidebar":"tutorialSidebar"},"llms/index":{"id":"llms/index","title":"LLMs - Causal Language Modeling","description":"Harness the power of causal language models for creative text generation tasks, including creative writing, dialogue simulation, and code writing assistance.","sidebar":"tutorialSidebar"},"llms/models/index":{"id":"llms/models/index","title":"Sparse LLMs","description":"Discover and utilize optimized LLM models from SparseZoo and Hugging Face Hub for efficient DeepSparse deployment.","sidebar":"tutorialSidebar"},"llms/models/sparse-foundational-llama-2":{"id":"llms/models/sparse-foundational-llama-2","title":"Sparse Foundational Llama 2 Models","description":"Discover Neural Magic\'s optimized Llama 2 models. Experience faster LLM performance with sparsification.","sidebar":"tutorialSidebar"},"llms/serving-llms":{"id":"llms/serving-llms","title":"Serving LLMs","description":"DeepSparse is a CPU inference runtime that takes advantage of sparsity to accelerate neural network inference. Coupled with SparseML, our optimization library for pruning and quantizing your models, DeepSparse delivers exceptional inference performance on CPU hardware.","sidebar":"tutorialSidebar"},"nlp/index":{"id":"nlp/index","title":"Natural Language Processing","description":"Optimize and deploy cutting-edge models for natural language processing.","sidebar":"tutorialSidebar"},"products/deepsparse/index":{"id":"products/deepsparse/index","title":"DeepSparse","description":"GitHub","sidebar":"tutorialSidebar"},"products/deepsparse/releases":{"id":"products/deepsparse/releases","title":"DeepSparse Releases","description":"Versions","sidebar":"tutorialSidebar"},"products/index":{"id":"products/index","title":"Products","description":"Gain a comprehensive understanding of Neural Magic\'s core products (SparseML, DeepSparse, SparseZoo) and their key features.","sidebar":"tutorialSidebar"},"products/sparseml/index":{"id":"products/sparseml/index","title":"SparseML","description":"GitHub","sidebar":"tutorialSidebar"},"products/sparseml/releases":{"id":"products/sparseml/releases","title":"SparseML Releases","description":"Versions","sidebar":"tutorialSidebar"},"products/sparsezoo/index":{"id":"products/sparsezoo/index","title":"SparseZoo","description":"GitHub","sidebar":"tutorialSidebar"},"products/sparsezoo/releases":{"id":"products/sparsezoo/releases","title":"SparseZoo Releases","description":"Versions","sidebar":"tutorialSidebar"}}}')}}]);