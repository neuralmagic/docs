"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[7264],{3504:(e,a,s)=>{s.r(a),s.d(a,{assets:()=>t,contentTitle:()=>n,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var i=s(4848),l=s(8453);const r={tags:["llama2","foundational-models","sparsified-models"],keywords:["llama2","sparse-llama","efficient-inference"],description:"Discover Neural Magic's optimized Llama 2 models. Experience faster LLM performance with sparsification.",sidebar_position:1,sidebar_label:"Llama 2"},n="Sparse Foundational Llama 2 Models",o={id:"llms/models/sparse-foundational-llama-2",title:"Sparse Foundational Llama 2 Models",description:"Discover Neural Magic's optimized Llama 2 models. Experience faster LLM performance with sparsification.",source:"@site/versioned_docs/version-1.7.0/llms/models/sparse-foundational-llama-2.mdx",sourceDirName:"llms/models",slug:"/llms/models/sparse-foundational-llama-2",permalink:"/llms/models/sparse-foundational-llama-2",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs-v2/tree/main/docs/llms/models/sparse-foundational-llama-2.mdx",tags:[{label:"llama2",permalink:"/tags/llama-2"},{label:"foundational-models",permalink:"/tags/foundational-models"},{label:"sparsified-models",permalink:"/tags/sparsified-models"}],version:"1.7.0",sidebarPosition:1,frontMatter:{tags:["llama2","foundational-models","sparsified-models"],keywords:["llama2","sparse-llama","efficient-inference"],description:"Discover Neural Magic's optimized Llama 2 models. Experience faster LLM performance with sparsification.",sidebar_position:1,sidebar_label:"Llama 2"},sidebar:"tutorialSidebar",previous:{title:"Models",permalink:"/llms/models/"},next:{title:"Guides",permalink:"/llms/guides/"}},t={},d=[{value:"Why Choose Sparse Llama 2 Models?",id:"why-choose-sparse-llama-2-models",level:2},{value:"Demo",id:"demo",level:2},{value:"Models",id:"models",level:2},{value:"Deploy",id:"deploy",level:2},{value:"Optimize",id:"optimize",level:2}];function c(e){const a={em:"em",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components},{DocCardList:s}=a;return s||function(e,a){throw new Error("Expected "+(a?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("DocCardList",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(a.h1,{id:"sparse-foundational-llama-2-models",children:"Sparse Foundational Llama 2 Models"}),"\n",(0,i.jsx)(a.p,{children:"Neural Magic and Cerebras partnered to offer a range of expertly optimized Llama 2-based Large Language Models (LLMs) that have been sparsified for superior performance and reduced footprint.\nThese models are carefully selected and rigorously tested, ensuring exceptional quality and seamless deployment."}),"\n",(0,i.jsx)(a.h2,{id:"why-choose-sparse-llama-2-models",children:"Why Choose Sparse Llama 2 Models?"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.em,{children:"Accelerated Inference"}),": Sparse Llama 2 models offer significant speed improvements, enabling faster responses and real-time applications."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.em,{children:"Reduced Resource Requirements"}),": Sparsification decreases the model's size, allowing deployment on edge devices or in environments with limited compute power."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.em,{children:"Cost-Effectiveness"}),": Lower compute requirements translate to reduced operational costs for your LLM-based applications."]}),"\n"]}),"\n",(0,i.jsx)(a.h2,{id:"demo",children:"Demo"}),"\n",(0,i.jsx)(s,{children:(0,i.jsxs)("a",{href:"https://huggingface.co/spaces/neuralmagic/llama-2-sparse-transfer-chat-deepsparse",children:[(0,i.jsx)("h3",{children:"Sparse Llama 2 Chat Demo"}),(0,i.jsx)("span",{children:"Experience the capabilities of Sparse Llama 2 models firsthand with our HF Spaces interactive demo highlighting their performance and potential applications."})]})}),"\n",(0,i.jsx)(a.h2,{id:"models",children:"Models"}),"\n",(0,i.jsx)(a.p,{children:"Currently, the following Sparse Llama 2 models are available for immediate use:"}),"\n",(0,i.jsxs)(a.ul,{children:["\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sparse Llama2-7B Pretrained"}),": A versatile and powerful LLM that has been sparsified for finetuning onto your specific use case."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sparse Llama2-7B Chat"}),": A specialized variant of the Sparse Llama2-7B model, optimized for chatbot applications."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sparse Llama2-7B Code Generation"}),": A specialized variant of the Sparse Llama2-7B model, optimized for code generation tasks."]}),"\n",(0,i.jsxs)(a.li,{children:[(0,i.jsx)(a.strong,{children:"Sparse Llama2-7B Instruction Tuning"}),": A specialized variant of the Sparse Llama2-7B model, optimized for instruction tuning tasks."]}),"\n"]}),"\n",(0,i.jsx)(s,{children:(0,i.jsxs)("a",{href:"https://huggingface.co/collections/neuralmagic/sparse-foundational-llama-2-models-65f48cec6396309f02e74d21",children:[(0,i.jsx)("h3",{children:"Sparse Llama 2 Models on Hugging Face"}),(0,i.jsx)("span",{children:"Explore Neural Magic's collection of Sparse Llama 2 models, including the versatile Sparse Llama2-7B and other specialized variants. Select the model that best aligns with your use case."})]})}),"\n",(0,i.jsx)(a.h2,{id:"deploy",children:"Deploy"}),"\n",(0,i.jsx)(s,{children:(0,i.jsxs)("a",{href:"https://github.com/neuralmagic/deepsparse",children:[(0,i.jsx)("h3",{children:"DeepSparse"}),(0,i.jsx)("span",{children:"Deploy Sparse Llama 2 models with DeepSparse, Neural Magic's platform for efficient and seamless model deployment on CPUs."})]})}),"\n",(0,i.jsx)(a.h2,{id:"optimize",children:"Optimize"}),"\n",(0,i.jsxs)(s,{children:[(0,i.jsxs)("a",{href:"https://github.com/neuralmagic/sparseml",children:[(0,i.jsx)("h3",{children:"SparseML"}),(0,i.jsx)("span",{children:"Optimize your Sparse Llama 2 models even further using SparseML. Discover pre-configured recipes and customize optimization strategies to maximize performance for your specific use case."})]}),(0,i.jsxs)("a",{href:"https://www.cerebras.net/blog/sparsity-made-easy-introducing-the-cerebras-pytorch-sparsity-library",children:[(0,i.jsx)("h3",{children:"Cerebras Sparse Training"}),(0,i.jsx)("span",{children:"Train your own Sparse Llama 2 models using Cerebras' advanced sparse training capabilities, ensuring optimal performance and efficiency."})]})]})]})}function m(e={}){const{wrapper:a}={...(0,l.R)(),...e.components};return a?(0,i.jsx)(a,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,a,s)=>{s.d(a,{R:()=>n,x:()=>o});var i=s(6540);const l={},r=i.createContext(l);function n(e){const a=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:n(e.components),i.createElement(r.Provider,{value:a},e.children)}}}]);