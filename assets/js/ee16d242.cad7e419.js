"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[788],{2493:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var o=i(4848),a=i(8453);const s={tags:["DeepSparse","features","debugging"],keywords:["DeepSparse","features","diagnostics","debugging","logging","troubleshooting"],description:"Logging Guidance for Diagnostics and Debugging",sidebar_label:"Diagnostics/Debugging",sidebar_position:4},t="Logging Guidance for Diagnostics and Debugging",r={id:"guides/deepsparse-engine/diagnostics-debugging",title:"Logging Guidance for Diagnostics and Debugging",description:"Logging Guidance for Diagnostics and Debugging",source:"@site/docs/guides/deepsparse-engine/diagnostics-debugging.mdx",sourceDirName:"guides/deepsparse-engine",slug:"/guides/deepsparse-engine/diagnostics-debugging",permalink:"/next/guides/deepsparse-engine/diagnostics-debugging",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/deepsparse-engine/diagnostics-debugging.mdx",tags:[{label:"DeepSparse",permalink:"/next/tags/deep-sparse"},{label:"features",permalink:"/next/tags/features"},{label:"debugging",permalink:"/next/tags/debugging"}],version:"current",sidebarPosition:4,frontMatter:{tags:["DeepSparse","features","debugging"],keywords:["DeepSparse","features","diagnostics","debugging","logging","troubleshooting"],description:"Logging Guidance for Diagnostics and Debugging",sidebar_label:"Diagnostics/Debugging",sidebar_position:4},sidebar:"autogenerated_docs",previous:{title:"Benchmarking",permalink:"/next/guides/deepsparse-engine/benchmarking"},next:{title:"numactl Utility",permalink:"/next/guides/deepsparse-engine/numactl-utility"}},l={},d=[{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Enabling Logs and Controlling the Amount of Logs Produced by DeepSparse",id:"enabling-logs-and-controlling-the-amount-of-logs-produced-by-deepsparse",level:2},{value:"Parsing an Example Log",id:"parsing-an-example-log",level:2},{value:"Viewing the Whole Graph",id:"viewing-the-whole-graph",level:3},{value:"Finding Supported Nodes for Our Optimized Engine",id:"finding-supported-nodes-for-our-optimized-engine",level:3},{value:"Compiling Each Subgraph",id:"compiling-each-subgraph",level:3},{value:"Determining the Number of Cores and Batch Size",id:"determining-the-number-of-cores-and-batch-size",level:3},{value:"Obtaining Subgraph Statistics",id:"obtaining-subgraph-statistics",level:3}];function g(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"logging-guidance-for-diagnostics-and-debugging",children:"Logging Guidance for Diagnostics and Debugging"}),"\n",(0,o.jsx)(n.p,{children:"This page explains the diagnostics and debugging features available in DeepSparse."}),"\n",(0,o.jsx)(n.p,{children:"Unlike traditional software, debugging utilities available to the machine learning community are scarce. Complicated with deployment pipeline design issues, model weights, model architecture, and unoptimized models, debugging performance issues can be very dynamic in your data science ecosystem. Reviewing a log file can be your first line of defense in pinpointing performance issues with optimizing your inference."}),"\n",(0,o.jsx)(n.p,{children:"DeepSparse ships with diagnostic logging so you can capture real-time monitoring information at model runtime and self-diagnose issues. If you are seeking technical support, we recommend capturing log information first, as described below. You can decide what to share, whether certain parts of the log or the entire content."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," Our logs may reveal your inference network\u2019s macro-architecture, including a general list of operators (such as convolution and pooling) and connections between them. Weights, trained parameters, or dataset parameters will not be captured. Consult Neural Magic\u2019s various legal policies at ",(0,o.jsx)(n.a,{href:"https://neuralmagic.com/legal/",children:"https://neuralmagic.com/legal/"})," which include our privacy statement and software agreements. Your use of the software serves as your consent to these practices."]}),"\n",(0,o.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,o.jsx)(n.p,{children:"An initial decision point to make in troubleshooting performance issues before enabling logs is whether to prevent threads from migrating from their cores. The default behavior is to disable thread binding (or pinning), allowing your OS to manage the allocation of threads to cores. There is a performance hit associated with this if DeepSparse is the main process running on your machine. If you want to enable thread binding for the possible performance benefit, set:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    NM_BIND_THREADS_TO_CORES=1\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Note:"})," If DeepSparse is not the only major process running on your machine, binding threads may hurt performance of the other major process(es) by monopolizing system resources. If you use OpenMP or TBB (Thread Building Blocks) in your application, then enabling thread binding may result in severe performance degradation due to conflicts between Neural Magic thread pool and OpenMP/TBB thread pools."]}),"\n",(0,o.jsx)(n.h2,{id:"enabling-logs-and-controlling-the-amount-of-logs-produced-by-deepsparse",children:"Enabling Logs and Controlling the Amount of Logs Produced by DeepSparse"}),"\n",(0,o.jsxs)(n.p,{children:["Logs are controlled by setting the ",(0,o.jsx)(n.code,{children:"NM_LOGGING_LEVEL"})," environment variable."]}),"\n",(0,o.jsx)(n.p,{children:"Specify within your shell one of the following verbosity levels (in increasing order of verbosity:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"fatal, error, warn,"})," and ",(0,o.jsx)(n.code,{children:"diagnose"})," with ",(0,o.jsx)(n.code,{children:"diagnose"})," as a common default for all logs that will output to stderr:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    NM_LOGGING_LEVEL=diagnose\n    export NM_LOGGING_LEVEL\n"})}),"\n",(0,o.jsx)(n.p,{children:"Alternatively, you can output the logging level by"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    NM_LOGGING_LEVEL=diagnose <some command>\n"})}),"\n",(0,o.jsx)(n.p,{children:"To enable diagnostic logs on a per-run basis, specify it manually before each script execution. For example, if you normally run:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    python run_model.py\n"})}),"\n",(0,o.jsx)(n.p,{children:"Then, to enable diagnostic logs, run:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    NM_LOGGING_LEVEL=diagnose python run_model.py\n"})}),"\n",(0,o.jsx)(n.p,{children:"To enable logging for your entire shell instance, execute within your shell:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    export NM_LOGGING_LEVEL=diagnose\n"})}),"\n",(0,o.jsxs)(n.p,{children:["By default, logs will print out to the stderr of your process. If you would like to output to a file, add ",(0,o.jsx)(n.code,{children:"2> <name_of_log>.txt"})," to the end of your command."]}),"\n",(0,o.jsx)(n.h2,{id:"parsing-an-example-log",children:"Parsing an Example Log"}),"\n",(0,o.jsxs)(n.p,{children:["If you want to see an example log with ",(0,o.jsx)(n.code,{children:"NM_LOGGING_LEVEL=diagnose"}),", a truncated sample output is provided at the end of this guide. It will show a super_resolution network, where Neural Magic only supports running 70% of it."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Different portions of the log are explained below."})}),"\n",(0,o.jsx)(n.h3,{id:"viewing-the-whole-graph",children:"Viewing the Whole Graph"}),"\n",(0,o.jsx)(n.p,{children:"Once a model is in our system, it is parsed to determine what operations it contains. Each operation is made a node and assigned a unique number Its operation type is displayed:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    Printing GraphViewer torch-jit-export:\n    Node 0: Conv\n    Node 1: Relu\n    Node 2: Conv\n    Node 3: Relu\n    Node 4: Conv\n    Node 5: Relu\n    Node 6: Conv\n    Node 7: Reshape\n    Node 8: Transpose\n    Node 9: Reshape\n"})}),"\n",(0,o.jsx)(n.h3,{id:"finding-supported-nodes-for-our-optimized-engine",children:"Finding Supported Nodes for Our Optimized Engine"}),"\n",(0,o.jsxs)(n.p,{children:['After the whole graph is loaded in, nodes are analyzed to determine whether they are supported by our optimized runtime engine. Notable "unsupported" operators are indicated by looking for ',(0,o.jsx)(n.code,{children:"Unsupported [type of node]"})," in the log. For example, this is an unsupported Reshape node that produces a 6D tensor:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    Unsupported Reshape , const shape greater than 5D\n"})}),"\n",(0,o.jsx)(n.h3,{id:"compiling-each-subgraph",children:"Compiling Each Subgraph"}),"\n",(0,o.jsxs)(n.p,{children:["Once all the nodes are located that are supported within the optimized engine, the graphs are split into maximal subgraphs and each one is compiled. \u200bTo find the start of each subgraph compilation, look for ",(0,o.jsx)(n.code,{children:"== Beginning new subgraph =="}),". First, the nodes are displayed in the subgraph: \u200b"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"    Printing subgraph:\n    Node 0: Conv\n    Node 1: Relu\n    Node 2: Conv\n    Node 3: Relu\n    Node 4: Conv\n    Node 5: Relu\n    Node 6: Conv\n"})}),"\n",(0,o.jsx)(n.p,{children:"Simplifications are then performed on the graph to get it in an ideal state for complex optimizations, which are logged:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"== Translating subgraph NM_Subgraph_1 to NM intake graph.\n( L1 graph\n    ( values:\n      (10 float [ 1, 64, 224, 224 ])\n      (11 float [ 1, 64, 224, 224 ])\n      (12 float [ 1, 64, 224, 224 ])\n      (13 float [ 1, 32, 224, 224 ])\n      (14 float [ 1, 32, 224, 224 ])\n      (15 float [ 1, 9, 224, 224 ])\n      (9 float [ 1, 64, 224, 224 ])\n      (conv1.bias float [ 64 ])\n      (conv1.weight float [ 64, 1, 5, 5 ])\n      (conv2.bias float [ 64 ])\n      (conv2.weight float [ 64, 64, 3, 3 ])\n      (conv3.bias float [ 32 ])\n      (conv3.weight float [ 32, 64, 3, 3 ])\n      (conv4.bias float [ 9 ])\n      (conv4.weight float [ 9, 32, 3, 3 ])\n      (input float [ 1, 1, 224, 224 ])\n    )\n    ( operations:\n      (Constant conv1.bias (constant float [ 64 ]))\n      (Constant conv1.weight (constant float [ 64, 1, 5, 5 ]))\n      (Constant conv2.bias (constant float [ 64 ]))\n      (Constant conv2.weight (constant float [ 64, 64, 3, 3 ]))\n      (Constant conv3.bias (constant float [ 32 ]))\n      (Constant conv3.weight (constant float [ 32, 64, 3, 3 ]))\n      (Constant conv4.bias (constant float [ 9 ]))\n      (Constant conv4.weight (constant float [ 9, 32, 3, 3 ]))\n      (Input input (io 0))\n      (Conv input -> 9 (conv kernel = [ 64, 1, 5, 5 ] bias = [ 64 ] padding = {{2, 2}, {2, 2}} strides = {1, 1}))\n      (Elementwise 9 -> 10 (calc Relu))\n      (Conv 10 -> 11 (conv kernel = [ 64, 64, 3, 3 ] bias = [ 64 ] padding = {{1, 1}, {1, 1}} strides = {1, 1}))\n      (Elementwise 11 -> 12 (calc Relu))\n      (Conv 12 -> 13 (conv kernel = [ 32, 64, 3, 3 ] bias = [ 32 ] padding = {{1, 1}, {1, 1}} strides = {1, 1}))\n      (Elementwise 13 -> 14 (calc Relu))\n      (Conv 14 -> 15 (conv kernel = [ 9, 32, 3, 3 ] bias = [ 9 ] padding = {{1, 1}, {1, 1}} strides = {1, 1}))\n      (Output 15 (io 0))\n    )\n)\n"})}),"\n",(0,o.jsx)(n.h3,{id:"determining-the-number-of-cores-and-batch-size",children:"Determining the Number of Cores and Batch Size"}),"\n",(0,o.jsxs)(n.p,{children:["This log detail describes the batch size and number of cores that Neural Magic is optimizing against. Look for ",(0,o.jsx)(n.code,{children:"== Compiling NM_Subgraph"})," as in this example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"== Compiling NM_Subgraph_1 with batch size 1 using 18 cores.\n"})}),"\n",(0,o.jsx)(n.h3,{id:"obtaining-subgraph-statistics",children:"Obtaining Subgraph Statistics"}),"\n",(0,o.jsxs)(n.p,{children:["Locating  ",(0,o.jsx)(n.code,{children:"== NM Execution Provider supports"})," shows how many subgraphs we compiled and what percentage of the network we managed to support running:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"Created 1 compiled subgraphs.\n== NM Execution Provider supports 70% of the network\n"})})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(g,{...e})}):g(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>r});var o=i(6540);const a={},s=o.createContext(a);function t(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);