"use strict";(self.webpackChunkdocs_neuralmagic_com=self.webpackChunkdocs_neuralmagic_com||[]).push([[663],{7309:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>t,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>h});var a=s(4848),r=s(8453);const i={tags:["DeepSparse","features","benchmarking"],keywords:["DeepSparse","features","benchmarking"],description:"Benchmarking ONNX models with DeepSparse",sidebar_label:"Benchmarking",sidebar_position:3},t="Benchmarking ONNX Models With DeepSparse",o={id:"guides/deepsparse-engine/benchmarking",title:"Benchmarking ONNX Models With DeepSparse",description:"Benchmarking ONNX models with DeepSparse",source:"@site/versioned_docs/version-1.7.0/guides/deepsparse-engine/benchmarking.mdx",sourceDirName:"guides/deepsparse-engine",slug:"/guides/deepsparse-engine/benchmarking",permalink:"/guides/deepsparse-engine/benchmarking",draft:!1,unlisted:!1,editUrl:"https://github.com/neuralmagic/docs/tree/main/docs/guides/deepsparse-engine/benchmarking.mdx",tags:[{label:"DeepSparse",permalink:"/tags/deep-sparse"},{label:"features",permalink:"/tags/features"},{label:"benchmarking",permalink:"/tags/benchmarking"}],version:"1.7.0",sidebarPosition:3,frontMatter:{tags:["DeepSparse","features","benchmarking"],keywords:["DeepSparse","features","benchmarking"],description:"Benchmarking ONNX models with DeepSparse",sidebar_label:"Benchmarking",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Inference Types",permalink:"/guides/deepsparse-engine/scheduler"},next:{title:"Diagnostics/Debugging",permalink:"/guides/deepsparse-engine/diagnostics-debugging"}},c={},h=[{value:"Installation Requirements",id:"installation-requirements",level:2},{value:"Quickstart",id:"quickstart",level:2},{value:"Usage",id:"usage",level:2},{value:"Sample CLI Argument Configurations",id:"sample-cli-argument-configurations",level:3},{value:"Inference Scenarios",id:"inference-scenarios",level:2},{value:"Synchronous (Single-Stream) Scenario",id:"synchronous-single-stream-scenario",level:3},{value:"Asynchronous (Multi-Stream) Scenario",id:"asynchronous-multi-stream-scenario",level:3},{value:"Example Benchmarking Output of Synchronous vs. Asynchronous",id:"example-benchmarking-output-of-synchronous-vs-asynchronous",level:3}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"benchmarking-onnx-models-with-deepsparse",children:"Benchmarking ONNX Models With DeepSparse"}),"\n",(0,a.jsx)(n.p,{children:"This page explains how to use DeepSparse Benchmarking utilities."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"deepsparse.benchmark"})," is a command-line (CLI) tool for benchmarking DeepSparse with ONNX models.\nThe tool will parse the arguments, download/compile the network into the engine, generate input tensors, and\nexecute the model depending on the chosen scenario. By default, it will choose a multi-stream or asynchronous mode to optimize for throughput."]}),"\n",(0,a.jsx)(n.h2,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,a.jsxs)(n.p,{children:["Use of the DeepSparse Benchmarking utilities requires installation of the ",(0,a.jsx)(n.a,{href:"/get-started/install/deepsparse",children:"DeepSparse"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"quickstart",children:"Quickstart"}),"\n",(0,a.jsx)(n.p,{children:"To benchmark a dense BERT ONNX model fine-tuned on the SST2 dataset (which is identified by its SparseZoo stub), run:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"deepsparse.benchmark zoo:nlp/text_classification/bert-base/pytorch/huggingface/sst2/base-none\n"})}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsxs)(n.p,{children:["In most cases, good performance will be found in the default options so usage can be as simple as running the command with a SparseZoo model stub or your local ONNX model.\nHowever, if you prefer to customize benchmarking for your personal use case, you can run ",(0,a.jsx)(n.code,{children:"deepsparse.benchmark -h"})," or with ",(0,a.jsx)(n.code,{children:"--help"})," to view your usage options:"]}),"\n",(0,a.jsx)(n.p,{children:"CLI Arguments:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ deepsparse.benchmark --help\n\n> positional arguments:\n>\n>         model_path                    Path to an ONNX model file or SparseZoo model stub.\n>\n> optional arguments:\n>\n>         -h, --help                    show this help message and exit.\n>\n>         -b BATCH_SIZE, --batch_size BATCH_SIZE\n>                                         The batch size to run the analysis for. Must be\n>                                         greater than 0.\n>\n>         -shapes INPUT_SHAPES, --input_shapes INPUT_SHAPES\n>                                         Override the shapes of the inputs, i.e. -shapes\n>                                         \"[1,2,3],[4,5,6],[7,8,9]\" results in input0=[1,2,3]\n>                                         input1=[4,5,6] input2=[7,8,9].\n>\n>         -ncores NUM_CORES, --num_cores NUM_CORES\n>                                         The number of physical cores to run the analysis on,\n>                                         defaults to all physical cores available on the system.\n>\n>         -s {async,sync,elastic}, --scenario {async,sync,elastic}\n>                                         Choose between using the async, sync and elastic\n>                                         scenarios. Sync and async are similar to the single-\n>                                         stream/multi-stream scenarios. Elastic is a newer\n>                                         scenario that behaves similarly to the async scenario\n>                                         but uses a different scheduling backend. Default value\n>                                         is async.\n>\n>         -t TIME, --time TIME\n>                                         The number of seconds the benchmark will run. Default\n>                                         is 10 seconds.\n>\n>         -w WARMUP_TIME, --warmup_time WARMUP_TIME\n>                                         The number of seconds the benchmark will warmup before\n>                                         running.Default is 2 seconds.\n>\n>         -nstreams NUM_STREAMS, --num_streams NUM_STREAMS\n>                                         The number of streams that will submit inferences in\n>                                         parallel using async scenario. Default is\n>                                         automatically determined for given hardware and may be\n>                                         sub-optimal.\n>\n>         -pin {none,core,numa}, --thread_pinning {none,core,numa}\n>                                         Enable binding threads to cores ('core' the default),\n>                                         threads to cores on sockets ('numa'), or disable\n>                                         ('none').\n>\n>         -e {deepsparse,onnxruntime}, --engine {deepsparse,onnxruntime}\n>                                         Inference engine backend to run eval on. Choices are\n>                                         'deepsparse', 'onnxruntime'. Default is 'deepsparse'.\n>\n>         -q, --quiet                     Lower logging verbosity.\n>\n>         -x EXPORT_PATH, --export_path EXPORT_PATH\n>                                         Store results into a JSON file.\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"PRO TIP:"})," Save your benchmark results in a convenient JSON file."]}),"\n",(0,a.jsxs)(n.p,{children:["The following is an example CLI command for benchmarking an ONNX model from the SparseZoo and saving the results to a ",(0,a.jsx)(n.code,{children:"benchmark.json"})," file:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"deepsparse.benchmark zoo:nlp/text_classification/bert-base/pytorch/huggingface/sst2/base-none -x benchmark.json\n"})}),"\n",(0,a.jsx)(n.h3,{id:"sample-cli-argument-configurations",children:"Sample CLI Argument Configurations"}),"\n",(0,a.jsx)(n.p,{children:"To run a sparse FP32 MobileNetV1 at batch size 16 for 10 seconds for throughput using 8 streams of requests, use:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"deepsparse.benchmark zoo:cv/classification/mobilenet_v1-1.0/pytorch/sparseml/imagenet/pruned-moderate --batch_size 16 --time 10 --scenario async --num_streams 8\n"})}),"\n",(0,a.jsx)(n.p,{children:"To run a sparse quantized INT8 6-layer BERT at batch size 1 for latency, use:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned_quant_6layers-aggressive_96 --batch_size 1 --scenario sync\n"})}),"\n",(0,a.jsx)(n.h2,{id:"inference-scenarios",children:"Inference Scenarios"}),"\n",(0,a.jsx)(n.h3,{id:"synchronous-single-stream-scenario",children:"Synchronous (Single-Stream) Scenario"}),"\n",(0,a.jsxs)(n.p,{children:["Set by the ",(0,a.jsx)(n.code,{children:"--scenario sync"})," argument, the goal metric is latency per batch (ms/batch). This scenario submits a single inference request at a time to the engine, recording the time taken for a request to return an output. This mimics an edge deployment scenario."]}),"\n",(0,a.jsx)(n.p,{children:"The latency value reported is the mean of all latencies recorded during the execution period for the given batch size."}),"\n",(0,a.jsx)(n.h3,{id:"asynchronous-multi-stream-scenario",children:"Asynchronous (Multi-Stream) Scenario"}),"\n",(0,a.jsxs)(n.p,{children:["Set by the ",(0,a.jsx)(n.code,{children:"--scenario async"})," argument, the goal metric is throughput in items per second (i/s). This scenario submits ",(0,a.jsx)(n.code,{children:"--num_streams"})," concurrent inference requests to the engine, recording the time taken for each request to return an output. This mimics a model server or bulk batch deployment scenario."]}),"\n",(0,a.jsx)(n.p,{children:"The throughput value reported comes from measuring the number of finished inferences within the execution time and the batch size."}),"\n",(0,a.jsx)(n.h3,{id:"example-benchmarking-output-of-synchronous-vs-asynchronous",children:"Example Benchmarking Output of Synchronous vs. Asynchronous"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"BERT 3-layer FP32 Sparse Throughput"})}),"\n",(0,a.jsxs)(n.p,{children:["There is no need to add a ",(0,a.jsx)(n.em,{children:"scenario"})," argument since ",(0,a.jsx)(n.code,{children:"async"})," is the default option:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned_3layers-aggressive_83\n\n> [INFO benchmark_model.py:202 ] Thread pinning to cores enabled\n> DeepSparse Engine, Copyright 2021-present / Neuralmagic, Inc. version: 0.10.0 (9bba6971) (optimized) (system=avx512, binary=avx512)\n> [INFO benchmark_model.py:247 ] deepsparse.engine.Engine:\n>         onnx_file_path: /home/mgoin/.cache/sparsezoo/c89f3128-4b87-41ae-91a3-eae8aa8c5a7c/model.onnx\n>         batch_size: 1\n>         num_cores: 18\n>         scheduler: Scheduler.multi_stream\n>         cpu_avx_type: avx512\n>         cpu_vnni: False\n> [INFO            onnx.py:176 ] Generating input 'input_ids', type = int64, shape = [1, 384]\n> [INFO            onnx.py:176 ] Generating input 'attention_mask', type = int64, shape = [1, 384]\n> [INFO            onnx.py:176 ] Generating input 'token_type_ids', type = int64, shape = [1, 384]\n> [INFO benchmark_model.py:264 ] num_streams default value chosen of 9. This requires tuning and may be sub-optimal\n> [INFO benchmark_model.py:270 ] Starting 'async' performance measurements for 10 seconds\n> Original Model Path: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned_3layers-aggressive_83\n> Batch Size: 1\n> Scenario: multistream\n> Throughput (items/sec): 83.5037\n> Latency Mean (ms/batch): 107.3422\n> Latency Median (ms/batch): 107.0099\n> Latency Std (ms/batch): 12.4016\n> Iterations: 840\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"BERT 3-layer FP32 Sparse Latency"})}),"\n",(0,a.jsxs)(n.p,{children:["To select a ",(0,a.jsx)(n.em,{children:"synchronous inference scenario"}),", add ",(0,a.jsx)(n.code,{children:"-s sync"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"$ deepsparse.benchmark zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned_3layers-aggressive_83 -s sync\n\n> [INFO benchmark_model.py:202 ] Thread pinning to cores enabled\n> DeepSparse Engine, Copyright 2021-present / Neuralmagic, Inc. version: 0.10.0 (9bba6971) (optimized) (system=avx512, binary=avx512)\n> [INFO benchmark_model.py:247 ] deepsparse.engine.Engine:\n>         onnx_file_path: /home/mgoin/.cache/sparsezoo/c89f3128-4b87-41ae-91a3-eae8aa8c5a7c/model.onnx\n>         batch_size: 1\n>         num_cores: 18\n>         scheduler: Scheduler.single_stream\n>         cpu_avx_type: avx512\n>         cpu_vnni: False\n> [INFO            onnx.py:176 ] Generating input 'input_ids', type = int64, shape = [1, 384]\n> [INFO            onnx.py:176 ] Generating input 'attention_mask', type = int64, shape = [1, 384]\n> [INFO            onnx.py:176 ] Generating input 'token_type_ids', type = int64, shape = [1, 384]\n> [INFO benchmark_model.py:270 ] Starting 'sync' performance measurements for 10 seconds\n> Original Model Path: zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned_3layers-aggressive_83\n> Batch Size: 1\n> Scenario: singlestream\n> Throughput (items/sec): 62.1568\n> Latency Mean (ms/batch): 16.0732\n> Latency Median (ms/batch): 15.7850\n> Latency Std (ms/batch): 1.0427\n> Iterations: 622\n"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var a=s(6540);const r={},i=a.createContext(r);function t(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);